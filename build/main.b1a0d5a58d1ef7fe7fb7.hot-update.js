exports.id = "main";
exports.modules = {

/***/ "./node_modules/abbrev/abbrev.js":
/*!***************************************!*\
  !*** ./node_modules/abbrev/abbrev.js ***!
  \***************************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = exports = abbrev.abbrev = abbrev\n\nabbrev.monkeyPatch = monkeyPatch\n\nfunction monkeyPatch () {\n  Object.defineProperty(Array.prototype, 'abbrev', {\n    value: function () { return abbrev(this) },\n    enumerable: false, configurable: true, writable: true\n  })\n\n  Object.defineProperty(Object.prototype, 'abbrev', {\n    value: function () { return abbrev(Object.keys(this)) },\n    enumerable: false, configurable: true, writable: true\n  })\n}\n\nfunction abbrev (list) {\n  if (arguments.length !== 1 || !Array.isArray(list)) {\n    list = Array.prototype.slice.call(arguments, 0)\n  }\n  for (var i = 0, l = list.length, args = [] ; i < l ; i ++) {\n    args[i] = typeof list[i] === \"string\" ? list[i] : String(list[i])\n  }\n\n  // sort them lexicographically, so that they're next to their nearest kin\n  args = args.sort(lexSort)\n\n  // walk through each, seeing how much it has in common with the next and previous\n  var abbrevs = {}\n    , prev = \"\"\n  for (var i = 0, l = args.length ; i < l ; i ++) {\n    var current = args[i]\n      , next = args[i + 1] || \"\"\n      , nextMatches = true\n      , prevMatches = true\n    if (current === next) continue\n    for (var j = 0, cl = current.length ; j < cl ; j ++) {\n      var curChar = current.charAt(j)\n      nextMatches = nextMatches && curChar === next.charAt(j)\n      prevMatches = prevMatches && curChar === prev.charAt(j)\n      if (!nextMatches && !prevMatches) {\n        j ++\n        break\n      }\n    }\n    prev = current\n    if (j === cl) {\n      abbrevs[current] = current\n      continue\n    }\n    for (var a = current.substr(0, j) ; j <= cl ; j ++) {\n      abbrevs[a] = current\n      a += current.charAt(j)\n    }\n  }\n  return abbrevs\n}\n\nfunction lexSort (a, b) {\n  return a === b ? 0 : a > b ? 1 : -1\n}\n\n\n//# sourceURL=webpack:///./node_modules/abbrev/abbrev.js?");

/***/ }),

/***/ "./node_modules/are-we-there-yet/index.js":
/*!************************************************!*\
  !*** ./node_modules/are-we-there-yet/index.js ***!
  \************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nexports.TrackerGroup = __webpack_require__(/*! ./tracker-group.js */ \"./node_modules/are-we-there-yet/tracker-group.js\")\nexports.Tracker = __webpack_require__(/*! ./tracker.js */ \"./node_modules/are-we-there-yet/tracker.js\")\nexports.TrackerStream = __webpack_require__(/*! ./tracker-stream.js */ \"./node_modules/are-we-there-yet/tracker-stream.js\")\n\n\n//# sourceURL=webpack:///./node_modules/are-we-there-yet/index.js?");

/***/ }),

/***/ "./node_modules/are-we-there-yet/tracker-base.js":
/*!*******************************************************!*\
  !*** ./node_modules/are-we-there-yet/tracker-base.js ***!
  \*******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar EventEmitter = __webpack_require__(/*! events */ \"events\").EventEmitter\nvar util = __webpack_require__(/*! util */ \"util\")\n\nvar trackerId = 0\nvar TrackerBase = module.exports = function (name) {\n  EventEmitter.call(this)\n  this.id = ++trackerId\n  this.name = name\n}\nutil.inherits(TrackerBase, EventEmitter)\n\n\n//# sourceURL=webpack:///./node_modules/are-we-there-yet/tracker-base.js?");

/***/ }),

/***/ "./node_modules/are-we-there-yet/tracker-group.js":
/*!********************************************************!*\
  !*** ./node_modules/are-we-there-yet/tracker-group.js ***!
  \********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar util = __webpack_require__(/*! util */ \"util\")\nvar TrackerBase = __webpack_require__(/*! ./tracker-base.js */ \"./node_modules/are-we-there-yet/tracker-base.js\")\nvar Tracker = __webpack_require__(/*! ./tracker.js */ \"./node_modules/are-we-there-yet/tracker.js\")\nvar TrackerStream = __webpack_require__(/*! ./tracker-stream.js */ \"./node_modules/are-we-there-yet/tracker-stream.js\")\n\nvar TrackerGroup = module.exports = function (name) {\n  TrackerBase.call(this, name)\n  this.parentGroup = null\n  this.trackers = []\n  this.completion = {}\n  this.weight = {}\n  this.totalWeight = 0\n  this.finished = false\n  this.bubbleChange = bubbleChange(this)\n}\nutil.inherits(TrackerGroup, TrackerBase)\n\nfunction bubbleChange (trackerGroup) {\n  return function (name, completed, tracker) {\n    trackerGroup.completion[tracker.id] = completed\n    if (trackerGroup.finished) return\n    trackerGroup.emit('change', name || trackerGroup.name, trackerGroup.completed(), trackerGroup)\n  }\n}\n\nTrackerGroup.prototype.nameInTree = function () {\n  var names = []\n  var from = this\n  while (from) {\n    names.unshift(from.name)\n    from = from.parentGroup\n  }\n  return names.join('/')\n}\n\nTrackerGroup.prototype.addUnit = function (unit, weight) {\n  if (unit.addUnit) {\n    var toTest = this\n    while (toTest) {\n      if (unit === toTest) {\n        throw new Error(\n          'Attempted to add tracker group ' +\n          unit.name + ' to tree that already includes it ' +\n          this.nameInTree(this))\n      }\n      toTest = toTest.parentGroup\n    }\n    unit.parentGroup = this\n  }\n  this.weight[unit.id] = weight || 1\n  this.totalWeight += this.weight[unit.id]\n  this.trackers.push(unit)\n  this.completion[unit.id] = unit.completed()\n  unit.on('change', this.bubbleChange)\n  if (!this.finished) this.emit('change', unit.name, this.completion[unit.id], unit)\n  return unit\n}\n\nTrackerGroup.prototype.completed = function () {\n  if (this.trackers.length === 0) return 0\n  var valPerWeight = 1 / this.totalWeight\n  var completed = 0\n  for (var ii = 0; ii < this.trackers.length; ii++) {\n    var trackerId = this.trackers[ii].id\n    completed += valPerWeight * this.weight[trackerId] * this.completion[trackerId]\n  }\n  return completed\n}\n\nTrackerGroup.prototype.newGroup = function (name, weight) {\n  return this.addUnit(new TrackerGroup(name), weight)\n}\n\nTrackerGroup.prototype.newItem = function (name, todo, weight) {\n  return this.addUnit(new Tracker(name, todo), weight)\n}\n\nTrackerGroup.prototype.newStream = function (name, todo, weight) {\n  return this.addUnit(new TrackerStream(name, todo), weight)\n}\n\nTrackerGroup.prototype.finish = function () {\n  this.finished = true\n  if (!this.trackers.length) this.addUnit(new Tracker(), 1, true)\n  for (var ii = 0; ii < this.trackers.length; ii++) {\n    var tracker = this.trackers[ii]\n    tracker.finish()\n    tracker.removeListener('change', this.bubbleChange)\n  }\n  this.emit('change', this.name, 1, this)\n}\n\nvar buffer = '                                  '\nTrackerGroup.prototype.debug = function (depth) {\n  depth = depth || 0\n  var indent = depth ? buffer.substr(0, depth) : ''\n  var output = indent + (this.name || 'top') + ': ' + this.completed() + '\\n'\n  this.trackers.forEach(function (tracker) {\n    if (tracker instanceof TrackerGroup) {\n      output += tracker.debug(depth + 1)\n    } else {\n      output += indent + ' ' + tracker.name + ': ' + tracker.completed() + '\\n'\n    }\n  })\n  return output\n}\n\n\n//# sourceURL=webpack:///./node_modules/are-we-there-yet/tracker-group.js?");

/***/ }),

/***/ "./node_modules/are-we-there-yet/tracker-stream.js":
/*!*********************************************************!*\
  !*** ./node_modules/are-we-there-yet/tracker-stream.js ***!
  \*********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar util = __webpack_require__(/*! util */ \"util\")\nvar stream = __webpack_require__(/*! readable-stream */ \"readable-stream\")\nvar delegate = __webpack_require__(/*! delegates */ \"./node_modules/delegates/index.js\")\nvar Tracker = __webpack_require__(/*! ./tracker.js */ \"./node_modules/are-we-there-yet/tracker.js\")\n\nvar TrackerStream = module.exports = function (name, size, options) {\n  stream.Transform.call(this, options)\n  this.tracker = new Tracker(name, size)\n  this.name = name\n  this.id = this.tracker.id\n  this.tracker.on('change', delegateChange(this))\n}\nutil.inherits(TrackerStream, stream.Transform)\n\nfunction delegateChange (trackerStream) {\n  return function (name, completion, tracker) {\n    trackerStream.emit('change', name, completion, trackerStream)\n  }\n}\n\nTrackerStream.prototype._transform = function (data, encoding, cb) {\n  this.tracker.completeWork(data.length ? data.length : 1)\n  this.push(data)\n  cb()\n}\n\nTrackerStream.prototype._flush = function (cb) {\n  this.tracker.finish()\n  cb()\n}\n\ndelegate(TrackerStream.prototype, 'tracker')\n  .method('completed')\n  .method('addWork')\n  .method('finish')\n\n\n//# sourceURL=webpack:///./node_modules/are-we-there-yet/tracker-stream.js?");

/***/ }),

/***/ "./node_modules/are-we-there-yet/tracker.js":
/*!**************************************************!*\
  !*** ./node_modules/are-we-there-yet/tracker.js ***!
  \**************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar util = __webpack_require__(/*! util */ \"util\")\nvar TrackerBase = __webpack_require__(/*! ./tracker-base.js */ \"./node_modules/are-we-there-yet/tracker-base.js\")\n\nvar Tracker = module.exports = function (name, todo) {\n  TrackerBase.call(this, name)\n  this.workDone = 0\n  this.workTodo = todo || 0\n}\nutil.inherits(Tracker, TrackerBase)\n\nTracker.prototype.completed = function () {\n  return this.workTodo === 0 ? 0 : this.workDone / this.workTodo\n}\n\nTracker.prototype.addWork = function (work) {\n  this.workTodo += work\n  this.emit('change', this.name, this.completed(), this)\n}\n\nTracker.prototype.completeWork = function (work) {\n  this.workDone += work\n  if (this.workDone > this.workTodo) this.workDone = this.workTodo\n  this.emit('change', this.name, this.completed(), this)\n}\n\nTracker.prototype.finish = function () {\n  this.workTodo = this.workDone = 1\n  this.emit('change', this.name, 1, this)\n}\n\n\n//# sourceURL=webpack:///./node_modules/are-we-there-yet/tracker.js?");

/***/ }),

/***/ "./node_modules/bcrypt sync recursive":
/*!**********************************!*\
  !*** ./node_modules/bcrypt sync ***!
  \**********************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("function webpackEmptyContext(req) {\n\tvar e = new Error(\"Cannot find module '\" + req + \"'\");\n\te.code = 'MODULE_NOT_FOUND';\n\tthrow e;\n}\nwebpackEmptyContext.keys = function() { return []; };\nwebpackEmptyContext.resolve = webpackEmptyContext;\nmodule.exports = webpackEmptyContext;\nwebpackEmptyContext.id = \"./node_modules/bcrypt sync recursive\";\n\n//# sourceURL=webpack:///./node_modules/bcrypt_sync?");

/***/ }),

/***/ "./node_modules/bcrypt/bcrypt.js":
/*!***************************************!*\
  !*** ./node_modules/bcrypt/bcrypt.js ***!
  \***************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/* WEBPACK VAR INJECTION */(function(__dirname) {\n\nvar binary = __webpack_require__(/*! node-pre-gyp */ \"./node_modules/node-pre-gyp/lib/node-pre-gyp.js\");\nvar path = __webpack_require__(/*! path */ \"path\");\nvar binding_path = binary.find(path.resolve(path.join(__dirname, './package.json')));\nvar bindings = __webpack_require__(\"./node_modules/bcrypt sync recursive\")(binding_path);\n\nvar crypto = __webpack_require__(/*! crypto */ \"crypto\");\n\nvar promises = __webpack_require__(/*! ./promises */ \"./node_modules/bcrypt/promises.js\");\n\n/// generate a salt (sync)\n/// @param {Number} [rounds] number of rounds (default 10)\n/// @return {String} salt\nmodule.exports.genSaltSync = function genSaltSync(rounds, minor) {\n    // default 10 rounds\n    if (!rounds) {\n        rounds = 10;\n    } else if (typeof rounds !== 'number') {\n        throw new Error('rounds must be a number');\n    }\n\n    if(!minor) {\n        minor = 'b';\n    } else if(minor !== 'b' && minor !== 'a') {\n        throw new Error('minor must be either \"a\" or \"b\"');\n    }\n\n    return bindings.gen_salt_sync(minor, rounds, crypto.randomBytes(16));\n};\n\n/// generate a salt\n/// @param {Number} [rounds] number of rounds (default 10)\n/// @param {Function} cb callback(err, salt)\nmodule.exports.genSalt = function genSalt(rounds, minor, cb) {\n    var error;\n\n    // if callback is first argument, then use defaults for others\n    if (typeof arguments[0] === 'function') {\n        // have to set callback first otherwise arguments are overriden\n        cb = arguments[0];\n        rounds = 10;\n        minor = 'b';\n    // callback is second argument\n    } else if (typeof arguments[1] === 'function') {\n        // have to set callback first otherwise arguments are overriden\n        cb = arguments[1];\n        minor = 'b';\n    }\n\n    if (!cb) {\n        return promises.promise(genSalt, this, [rounds, minor]);\n    }\n\n    // default 10 rounds\n    if (!rounds) {\n        rounds = 10;\n    } else if (typeof rounds !== 'number') {\n        // callback error asynchronously\n        error = new Error('rounds must be a number');\n        return process.nextTick(function() {\n            cb(error);\n        });\n    }\n\n    if(!minor) {\n        minor = 'b'\n    } else if(minor !== 'b' && minor !== 'a') {\n        error = new Error('minor must be either \"a\" or \"b\"');\n        return process.nextTick(function() {\n            cb(error);\n        });\n    }\n\n    crypto.randomBytes(16, function(error, randomBytes) {\n        if (error) {\n            cb(error);\n            return;\n        }\n\n        bindings.gen_salt(minor, rounds, randomBytes, cb);\n    });\n};\n\n/// hash data using a salt\n/// @param {String} data the data to encrypt\n/// @param {String} salt the salt to use when hashing\n/// @return {String} hash\nmodule.exports.hashSync = function hashSync(data, salt) {\n    if (data == null || salt == null) {\n        throw new Error('data and salt arguments required');\n    }\n\n    if (typeof data !== 'string' || (typeof salt !== 'string' && typeof salt !== 'number')) {\n        throw new Error('data must be a string and salt must either be a salt string or a number of rounds');\n    }\n\n    if (typeof salt === 'number') {\n        salt = module.exports.genSaltSync(salt);\n    }\n\n    return bindings.encrypt_sync(data, salt);\n};\n\n/// hash data using a salt\n/// @param {String} data the data to encrypt\n/// @param {String} salt the salt to use when hashing\n/// @param {Function} cb callback(err, hash)\nmodule.exports.hash = function hash(data, salt, cb) {\n    var error;\n\n    if (typeof data === 'function') {\n        error = new Error('data must be a string and salt must either be a salt string or a number of rounds');\n        return process.nextTick(function() {\n            data(error);\n        });\n    }\n\n    if (typeof salt === 'function') {\n        error = new Error('data must be a string and salt must either be a salt string or a number of rounds');\n        return process.nextTick(function() {\n            salt(error);\n        });\n    }\n\n    // cb exists but is not a function\n    // return a rejecting promise\n    if (cb && typeof cb !== 'function') {\n        return promises.reject(new Error('cb must be a function or null to return a Promise'));\n    }\n\n    if (!cb) {\n        return promises.promise(hash, this, [data, salt]);\n    }\n\n    if (data == null || salt == null) {\n        error = new Error('data and salt arguments required');\n        return process.nextTick(function() {\n            cb(error);\n        });\n    }\n\n    if (typeof data !== 'string' || (typeof salt !== 'string' && typeof salt !== 'number')) {\n        error = new Error('data must be a string and salt must either be a salt string or a number of rounds');\n        return process.nextTick(function() {\n            cb(error);\n        });\n    }\n\n\n    if (typeof salt === 'number') {\n        return module.exports.genSalt(salt, function(err, salt) {\n            return bindings.encrypt(data, salt, cb);\n        });\n    }\n\n    return bindings.encrypt(data, salt, cb);\n};\n\n/// compare raw data to hash\n/// @param {String} data the data to hash and compare\n/// @param {String} hash expected hash\n/// @return {bool} true if hashed data matches hash\nmodule.exports.compareSync = function compareSync(data, hash) {\n    if (data == null || hash == null) {\n        throw new Error('data and hash arguments required');\n    }\n\n    if (typeof data !== 'string' || typeof hash !== 'string') {\n        throw new Error('data and hash must be strings');\n    }\n\n    return bindings.compare_sync(data, hash);\n};\n\n/// compare raw data to hash\n/// @param {String} data the data to hash and compare\n/// @param {String} hash expected hash\n/// @param {Function} cb callback(err, matched) - matched is true if hashed data matches hash\nmodule.exports.compare = function compare(data, hash, cb) {\n    var error;\n\n    if (typeof data === 'function') {\n        error = new Error('data and hash arguments required');\n        return process.nextTick(function() {\n            data(error);\n        });\n    }\n\n    if (typeof hash === 'function') {\n        error = new Error('data and hash arguments required');\n        return process.nextTick(function() {\n            hash(error);\n        });\n    }\n\n    // cb exists but is not a function\n    // return a rejecting promise\n    if (cb && typeof cb !== 'function') {\n        return promises.reject(new Error('cb must be a function or null to return a Promise'));\n    }\n\n    if (!cb) {\n        return promises.promise(compare, this, [data, hash]);\n    }\n\n    if (data == null || hash == null) {\n        error = new Error('data and hash arguments required');\n        return process.nextTick(function() {\n            cb(error);\n        });\n    }\n\n    if (typeof data !== 'string' || typeof hash !== 'string') {\n        error = new Error('data and hash must be strings');\n        return process.nextTick(function() {\n            cb(error);\n        });\n    }\n\n    return bindings.compare(data, hash, cb);\n};\n\n/// @param {String} hash extract rounds from this hash\n/// @return {Number} the number of rounds used to encrypt a given hash\nmodule.exports.getRounds = function getRounds(hash) {\n    if (hash == null) {\n        throw new Error('hash argument required');\n    }\n\n    if (typeof hash !== 'string') {\n        throw new Error('hash must be a string');\n    }\n\n    return bindings.get_rounds(hash);\n};\n\n/* WEBPACK VAR INJECTION */}.call(this, \"/\"))\n\n//# sourceURL=webpack:///./node_modules/bcrypt/bcrypt.js?");

/***/ }),

/***/ "./node_modules/bcrypt/promises.js":
/*!*****************************************!*\
  !*** ./node_modules/bcrypt/promises.js ***!
  \*****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nvar Promise = global.Promise;\n\n/// encapsulate a method with a node-style callback in a Promise\n/// @param {object} 'this' of the encapsulated function\n/// @param {function} function to be encapsulated\n/// @param {Array-like} args to be passed to the called function\n/// @return {Promise} a Promise encapsulating the function\nmodule.exports.promise = function (fn, context, args) {\n\n    if (!Array.isArray(args)) {\n        args = Array.prototype.slice.call(args);\n    }\n\n    if (typeof fn !== 'function') {\n        return Promise.reject(new Error('fn must be a function'));\n    }\n\n    return new Promise(function(resolve, reject) {\n        args.push(function(err, data) {\n            if (err) {\n                reject(err);\n            } else {\n                resolve(data);\n            }\n        });\n\n        fn.apply(context, args);\n    });\n};\n\n/// @param {err} the error to be thrown\nmodule.exports.reject = function (err) {\n    return Promise.reject(err);\n};\n\n/// changes the promise implementation that bcrypt uses\n/// @param {Promise} the implementation to use\nmodule.exports.use = function(promise) {\n  Promise = promise;\n};\n\n\n//# sourceURL=webpack:///./node_modules/bcrypt/promises.js?");

/***/ }),

/***/ "./node_modules/console-control-strings/index.js":
/*!*******************************************************!*\
  !*** ./node_modules/console-control-strings/index.js ***!
  \*******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\n// These tables borrowed from `ansi`\n\nvar prefix = '\\x1b['\n\nexports.up = function up (num) {\n  return prefix + (num || '') + 'A'\n}\n\nexports.down = function down (num) {\n  return prefix + (num || '') + 'B'\n}\n\nexports.forward = function forward (num) {\n  return prefix + (num || '') + 'C'\n}\n\nexports.back = function back (num) {\n  return prefix + (num || '') + 'D'\n}\n\nexports.nextLine = function nextLine (num) {\n  return prefix + (num || '') + 'E'\n}\n\nexports.previousLine = function previousLine (num) {\n  return prefix + (num || '') + 'F'\n}\n\nexports.horizontalAbsolute = function horizontalAbsolute (num) {\n  if (num == null) throw new Error('horizontalAboslute requires a column to position to')\n  return prefix + num + 'G'\n}\n\nexports.eraseData = function eraseData () {\n  return prefix + 'J'\n}\n\nexports.eraseLine = function eraseLine () {\n  return prefix + 'K'\n}\n\nexports.goto = function (x, y) {\n  return prefix + y + ';' + x + 'H'\n}\n\nexports.gotoSOL = function () {\n  return '\\r'\n}\n\nexports.beep = function () {\n  return '\\x07'\n}\n\nexports.hideCursor = function hideCursor () {\n  return prefix + '?25l'\n}\n\nexports.showCursor = function showCursor () {\n  return prefix + '?25h'\n}\n\nvar colors = {\n  reset: 0,\n// styles\n  bold: 1,\n  italic: 3,\n  underline: 4,\n  inverse: 7,\n// resets\n  stopBold: 22,\n  stopItalic: 23,\n  stopUnderline: 24,\n  stopInverse: 27,\n// colors\n  white: 37,\n  black: 30,\n  blue: 34,\n  cyan: 36,\n  green: 32,\n  magenta: 35,\n  red: 31,\n  yellow: 33,\n  bgWhite: 47,\n  bgBlack: 40,\n  bgBlue: 44,\n  bgCyan: 46,\n  bgGreen: 42,\n  bgMagenta: 45,\n  bgRed: 41,\n  bgYellow: 43,\n\n  grey: 90,\n  brightBlack: 90,\n  brightRed: 91,\n  brightGreen: 92,\n  brightYellow: 93,\n  brightBlue: 94,\n  brightMagenta: 95,\n  brightCyan: 96,\n  brightWhite: 97,\n\n  bgGrey: 100,\n  bgBrightBlack: 100,\n  bgBrightRed: 101,\n  bgBrightGreen: 102,\n  bgBrightYellow: 103,\n  bgBrightBlue: 104,\n  bgBrightMagenta: 105,\n  bgBrightCyan: 106,\n  bgBrightWhite: 107\n}\n\nexports.color = function color (colorWith) {\n  if (arguments.length !== 1 || !Array.isArray(colorWith)) {\n    colorWith = Array.prototype.slice.call(arguments)\n  }\n  return prefix + colorWith.map(colorNameToCode).join(';') + 'm'\n}\n\nfunction colorNameToCode (color) {\n  if (colors[color] != null) return colors[color]\n  throw new Error('Unknown color or style name: ' + color)\n}\n\n\n//# sourceURL=webpack:///./node_modules/console-control-strings/index.js?");

/***/ }),

/***/ "./node_modules/deep-extend/lib/deep-extend.js":
/*!*****************************************************!*\
  !*** ./node_modules/deep-extend/lib/deep-extend.js ***!
  \*****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/*!\n * @description Recursive object extending\n * @author Viacheslav Lotsmanov <lotsmanov89@gmail.com>\n * @license MIT\n *\n * The MIT License (MIT)\n *\n * Copyright (c) 2013-2018 Viacheslav Lotsmanov\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy of\n * this software and associated documentation files (the \"Software\"), to deal in\n * the Software without restriction, including without limitation the rights to\n * use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n * the Software, and to permit persons to whom the Software is furnished to do so,\n * subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in all\n * copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n * FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n * COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n * IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n */\n\n\n\nfunction isSpecificValue(val) {\n\treturn (\n\t\tval instanceof Buffer\n\t\t|| val instanceof Date\n\t\t|| val instanceof RegExp\n\t) ? true : false;\n}\n\nfunction cloneSpecificValue(val) {\n\tif (val instanceof Buffer) {\n\t\tvar x = Buffer.alloc\n\t\t\t? Buffer.alloc(val.length)\n\t\t\t: new Buffer(val.length);\n\t\tval.copy(x);\n\t\treturn x;\n\t} else if (val instanceof Date) {\n\t\treturn new Date(val.getTime());\n\t} else if (val instanceof RegExp) {\n\t\treturn new RegExp(val);\n\t} else {\n\t\tthrow new Error('Unexpected situation');\n\t}\n}\n\n/**\n * Recursive cloning array.\n */\nfunction deepCloneArray(arr) {\n\tvar clone = [];\n\tarr.forEach(function (item, index) {\n\t\tif (typeof item === 'object' && item !== null) {\n\t\t\tif (Array.isArray(item)) {\n\t\t\t\tclone[index] = deepCloneArray(item);\n\t\t\t} else if (isSpecificValue(item)) {\n\t\t\t\tclone[index] = cloneSpecificValue(item);\n\t\t\t} else {\n\t\t\t\tclone[index] = deepExtend({}, item);\n\t\t\t}\n\t\t} else {\n\t\t\tclone[index] = item;\n\t\t}\n\t});\n\treturn clone;\n}\n\nfunction safeGetProperty(object, property) {\n\treturn property === '__proto__' ? undefined : object[property];\n}\n\n/**\n * Extening object that entered in first argument.\n *\n * Returns extended object or false if have no target object or incorrect type.\n *\n * If you wish to clone source object (without modify it), just use empty new\n * object as first argument, like this:\n *   deepExtend({}, yourObj_1, [yourObj_N]);\n */\nvar deepExtend = module.exports = function (/*obj_1, [obj_2], [obj_N]*/) {\n\tif (arguments.length < 1 || typeof arguments[0] !== 'object') {\n\t\treturn false;\n\t}\n\n\tif (arguments.length < 2) {\n\t\treturn arguments[0];\n\t}\n\n\tvar target = arguments[0];\n\n\t// convert arguments to array and cut off target object\n\tvar args = Array.prototype.slice.call(arguments, 1);\n\n\tvar val, src, clone;\n\n\targs.forEach(function (obj) {\n\t\t// skip argument if isn't an object, is null, or is an array\n\t\tif (typeof obj !== 'object' || obj === null || Array.isArray(obj)) {\n\t\t\treturn;\n\t\t}\n\n\t\tObject.keys(obj).forEach(function (key) {\n\t\t\tsrc = safeGetProperty(target, key); // source value\n\t\t\tval = safeGetProperty(obj, key); // new value\n\n\t\t\t// recursion prevention\n\t\t\tif (val === target) {\n\t\t\t\treturn;\n\n\t\t\t/**\n\t\t\t * if new value isn't object then just overwrite by new value\n\t\t\t * instead of extending.\n\t\t\t */\n\t\t\t} else if (typeof val !== 'object' || val === null) {\n\t\t\t\ttarget[key] = val;\n\t\t\t\treturn;\n\n\t\t\t// just clone arrays (and recursive clone objects inside)\n\t\t\t} else if (Array.isArray(val)) {\n\t\t\t\ttarget[key] = deepCloneArray(val);\n\t\t\t\treturn;\n\n\t\t\t// custom cloning and overwrite for specific objects\n\t\t\t} else if (isSpecificValue(val)) {\n\t\t\t\ttarget[key] = cloneSpecificValue(val);\n\t\t\t\treturn;\n\n\t\t\t// overwrite by new value if source isn't object or array\n\t\t\t} else if (typeof src !== 'object' || src === null || Array.isArray(src)) {\n\t\t\t\ttarget[key] = deepExtend({}, val);\n\t\t\t\treturn;\n\n\t\t\t// source value and new value is objects both, extending...\n\t\t\t} else {\n\t\t\t\ttarget[key] = deepExtend(src, val);\n\t\t\t\treturn;\n\t\t\t}\n\t\t});\n\t});\n\n\treturn target;\n};\n\n\n//# sourceURL=webpack:///./node_modules/deep-extend/lib/deep-extend.js?");

/***/ }),

/***/ "./node_modules/delegates/index.js":
/*!*****************************************!*\
  !*** ./node_modules/delegates/index.js ***!
  \*****************************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("\n/**\n * Expose `Delegator`.\n */\n\nmodule.exports = Delegator;\n\n/**\n * Initialize a delegator.\n *\n * @param {Object} proto\n * @param {String} target\n * @api public\n */\n\nfunction Delegator(proto, target) {\n  if (!(this instanceof Delegator)) return new Delegator(proto, target);\n  this.proto = proto;\n  this.target = target;\n  this.methods = [];\n  this.getters = [];\n  this.setters = [];\n  this.fluents = [];\n}\n\n/**\n * Delegate method `name`.\n *\n * @param {String} name\n * @return {Delegator} self\n * @api public\n */\n\nDelegator.prototype.method = function(name){\n  var proto = this.proto;\n  var target = this.target;\n  this.methods.push(name);\n\n  proto[name] = function(){\n    return this[target][name].apply(this[target], arguments);\n  };\n\n  return this;\n};\n\n/**\n * Delegator accessor `name`.\n *\n * @param {String} name\n * @return {Delegator} self\n * @api public\n */\n\nDelegator.prototype.access = function(name){\n  return this.getter(name).setter(name);\n};\n\n/**\n * Delegator getter `name`.\n *\n * @param {String} name\n * @return {Delegator} self\n * @api public\n */\n\nDelegator.prototype.getter = function(name){\n  var proto = this.proto;\n  var target = this.target;\n  this.getters.push(name);\n\n  proto.__defineGetter__(name, function(){\n    return this[target][name];\n  });\n\n  return this;\n};\n\n/**\n * Delegator setter `name`.\n *\n * @param {String} name\n * @return {Delegator} self\n * @api public\n */\n\nDelegator.prototype.setter = function(name){\n  var proto = this.proto;\n  var target = this.target;\n  this.setters.push(name);\n\n  proto.__defineSetter__(name, function(val){\n    return this[target][name] = val;\n  });\n\n  return this;\n};\n\n/**\n * Delegator fluent accessor\n *\n * @param {String} name\n * @return {Delegator} self\n * @api public\n */\n\nDelegator.prototype.fluent = function (name) {\n  var proto = this.proto;\n  var target = this.target;\n  this.fluents.push(name);\n\n  proto[name] = function(val){\n    if ('undefined' != typeof val) {\n      this[target][name] = val;\n      return this;\n    } else {\n      return this[target][name];\n    }\n  };\n\n  return this;\n};\n\n\n//# sourceURL=webpack:///./node_modules/delegates/index.js?");

/***/ }),

/***/ "./node_modules/detect-libc/lib/detect-libc.js":
/*!*****************************************************!*\
  !*** ./node_modules/detect-libc/lib/detect-libc.js ***!
  \*****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nvar platform = __webpack_require__(/*! os */ \"os\").platform();\nvar spawnSync = __webpack_require__(/*! child_process */ \"child_process\").spawnSync;\nvar readdirSync = __webpack_require__(/*! fs */ \"fs\").readdirSync;\n\nvar GLIBC = 'glibc';\nvar MUSL = 'musl';\n\nvar spawnOptions = {\n  encoding: 'utf8',\n  env: process.env\n};\n\nif (!spawnSync) {\n  spawnSync = function () {\n    return { status: 126, stdout: '', stderr: '' };\n  };\n}\n\nfunction contains (needle) {\n  return function (haystack) {\n    return haystack.indexOf(needle) !== -1;\n  };\n}\n\nfunction versionFromMuslLdd (out) {\n  return out.split(/[\\r\\n]+/)[1].trim().split(/\\s/)[1];\n}\n\nfunction safeReaddirSync (path) {\n  try {\n    return readdirSync(path);\n  } catch (e) {}\n  return [];\n}\n\nvar family = '';\nvar version = '';\nvar method = '';\n\nif (platform === 'linux') {\n  // Try getconf\n  var glibc = spawnSync('getconf', ['GNU_LIBC_VERSION'], spawnOptions);\n  if (glibc.status === 0) {\n    family = GLIBC;\n    version = glibc.stdout.trim().split(' ')[1];\n    method = 'getconf';\n  } else {\n    // Try ldd\n    var ldd = spawnSync('ldd', ['--version'], spawnOptions);\n    if (ldd.status === 0 && ldd.stdout.indexOf(MUSL) !== -1) {\n      family = MUSL;\n      version = versionFromMuslLdd(ldd.stdout);\n      method = 'ldd';\n    } else if (ldd.status === 1 && ldd.stderr.indexOf(MUSL) !== -1) {\n      family = MUSL;\n      version = versionFromMuslLdd(ldd.stderr);\n      method = 'ldd';\n    } else {\n      // Try filesystem (family only)\n      var lib = safeReaddirSync('/lib');\n      if (lib.some(contains('-linux-gnu'))) {\n        family = GLIBC;\n        method = 'filesystem';\n      } else if (lib.some(contains('libc.musl-'))) {\n        family = MUSL;\n        method = 'filesystem';\n      } else if (lib.some(contains('ld-musl-'))) {\n        family = MUSL;\n        method = 'filesystem';\n      } else {\n        var usrSbin = safeReaddirSync('/usr/sbin');\n        if (usrSbin.some(contains('glibc'))) {\n          family = GLIBC;\n          method = 'filesystem';\n        }\n      }\n    }\n  }\n}\n\nvar isNonGlibcLinux = (family !== '' && family !== GLIBC);\n\nmodule.exports = {\n  GLIBC: GLIBC,\n  MUSL: MUSL,\n  family: family,\n  version: version,\n  method: method,\n  isNonGlibcLinux: isNonGlibcLinux\n};\n\n\n//# sourceURL=webpack:///./node_modules/detect-libc/lib/detect-libc.js?");

/***/ }),

/***/ "./node_modules/fs-minipass/index.js":
/*!*******************************************!*\
  !*** ./node_modules/fs-minipass/index.js ***!
  \*******************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nconst MiniPass = __webpack_require__(/*! minipass */ \"./node_modules/minipass/index.js\")\nconst EE = __webpack_require__(/*! events */ \"events\").EventEmitter\nconst fs = __webpack_require__(/*! fs */ \"fs\")\n\n// for writev\nconst binding = process.binding('fs')\nconst writeBuffers = binding.writeBuffers\n/* istanbul ignore next */\nconst FSReqWrap = binding.FSReqWrap || binding.FSReqCallback\n\nconst _autoClose = Symbol('_autoClose')\nconst _close = Symbol('_close')\nconst _ended = Symbol('_ended')\nconst _fd = Symbol('_fd')\nconst _finished = Symbol('_finished')\nconst _flags = Symbol('_flags')\nconst _flush = Symbol('_flush')\nconst _handleChunk = Symbol('_handleChunk')\nconst _makeBuf = Symbol('_makeBuf')\nconst _mode = Symbol('_mode')\nconst _needDrain = Symbol('_needDrain')\nconst _onerror = Symbol('_onerror')\nconst _onopen = Symbol('_onopen')\nconst _onread = Symbol('_onread')\nconst _onwrite = Symbol('_onwrite')\nconst _open = Symbol('_open')\nconst _path = Symbol('_path')\nconst _pos = Symbol('_pos')\nconst _queue = Symbol('_queue')\nconst _read = Symbol('_read')\nconst _readSize = Symbol('_readSize')\nconst _reading = Symbol('_reading')\nconst _remain = Symbol('_remain')\nconst _size = Symbol('_size')\nconst _write = Symbol('_write')\nconst _writing = Symbol('_writing')\nconst _defaultFlag = Symbol('_defaultFlag')\n\nclass ReadStream extends MiniPass {\n  constructor (path, opt) {\n    opt = opt || {}\n    super(opt)\n\n    this.writable = false\n\n    if (typeof path !== 'string')\n      throw new TypeError('path must be a string')\n\n    this[_fd] = typeof opt.fd === 'number' ? opt.fd : null\n    this[_path] = path\n    this[_readSize] = opt.readSize || 16*1024*1024\n    this[_reading] = false\n    this[_size] = typeof opt.size === 'number' ? opt.size : Infinity\n    this[_remain] = this[_size]\n    this[_autoClose] = typeof opt.autoClose === 'boolean' ?\n      opt.autoClose : true\n\n    if (typeof this[_fd] === 'number')\n      this[_read]()\n    else\n      this[_open]()\n  }\n\n  get fd () { return this[_fd] }\n  get path () { return this[_path] }\n\n  write () {\n    throw new TypeError('this is a readable stream')\n  }\n\n  end () {\n    throw new TypeError('this is a readable stream')\n  }\n\n  [_open] () {\n    fs.open(this[_path], 'r', (er, fd) => this[_onopen](er, fd))\n  }\n\n  [_onopen] (er, fd) {\n    if (er)\n      this[_onerror](er)\n    else {\n      this[_fd] = fd\n      this.emit('open', fd)\n      this[_read]()\n    }\n  }\n\n  [_makeBuf] () {\n    return Buffer.allocUnsafe(Math.min(this[_readSize], this[_remain]))\n  }\n\n  [_read] () {\n    if (!this[_reading]) {\n      this[_reading] = true\n      const buf = this[_makeBuf]()\n      /* istanbul ignore if */\n      if (buf.length === 0) return process.nextTick(() => this[_onread](null, 0, buf))\n      fs.read(this[_fd], buf, 0, buf.length, null, (er, br, buf) =>\n        this[_onread](er, br, buf))\n    }\n  }\n\n  [_onread] (er, br, buf) {\n    this[_reading] = false\n    if (er)\n      this[_onerror](er)\n    else if (this[_handleChunk](br, buf))\n      this[_read]()\n  }\n\n  [_close] () {\n    if (this[_autoClose] && typeof this[_fd] === 'number') {\n      fs.close(this[_fd], _ => this.emit('close'))\n      this[_fd] = null\n    }\n  }\n\n  [_onerror] (er) {\n    this[_reading] = true\n    this[_close]()\n    this.emit('error', er)\n  }\n\n  [_handleChunk] (br, buf) {\n    let ret = false\n    // no effect if infinite\n    this[_remain] -= br\n    if (br > 0)\n      ret = super.write(br < buf.length ? buf.slice(0, br) : buf)\n\n    if (br === 0 || this[_remain] <= 0) {\n      ret = false\n      this[_close]()\n      super.end()\n    }\n\n    return ret\n  }\n\n  emit (ev, data) {\n    switch (ev) {\n      case 'prefinish':\n      case 'finish':\n        break\n\n      case 'drain':\n        if (typeof this[_fd] === 'number')\n          this[_read]()\n        break\n\n      default:\n        return super.emit(ev, data)\n    }\n  }\n}\n\nclass ReadStreamSync extends ReadStream {\n  [_open] () {\n    let threw = true\n    try {\n      this[_onopen](null, fs.openSync(this[_path], 'r'))\n      threw = false\n    } finally {\n      if (threw)\n        this[_close]()\n    }\n  }\n\n  [_read] () {\n    let threw = true\n    try {\n      if (!this[_reading]) {\n        this[_reading] = true\n        do {\n          const buf = this[_makeBuf]()\n          /* istanbul ignore next */\n          const br = buf.length === 0 ? 0 : fs.readSync(this[_fd], buf, 0, buf.length, null)\n          if (!this[_handleChunk](br, buf))\n            break\n        } while (true)\n        this[_reading] = false\n      }\n      threw = false\n    } finally {\n      if (threw)\n        this[_close]()\n    }\n  }\n\n  [_close] () {\n    if (this[_autoClose] && typeof this[_fd] === 'number') {\n      try {\n        fs.closeSync(this[_fd])\n      } catch (er) {}\n      this[_fd] = null\n      this.emit('close')\n    }\n  }\n}\n\nclass WriteStream extends EE {\n  constructor (path, opt) {\n    opt = opt || {}\n    super(opt)\n    this.readable = false\n    this[_writing] = false\n    this[_ended] = false\n    this[_needDrain] = false\n    this[_queue] = []\n    this[_path] = path\n    this[_fd] = typeof opt.fd === 'number' ? opt.fd : null\n    this[_mode] = opt.mode === undefined ? 0o666 : opt.mode\n    this[_pos] = typeof opt.start === 'number' ? opt.start : null\n    this[_autoClose] = typeof opt.autoClose === 'boolean' ?\n      opt.autoClose : true\n\n    // truncating makes no sense when writing into the middle\n    const defaultFlag = this[_pos] !== null ? 'r+' : 'w'\n    this[_defaultFlag] = opt.flags === undefined\n    this[_flags] = this[_defaultFlag] ? defaultFlag : opt.flags\n\n    if (this[_fd] === null)\n      this[_open]()\n  }\n\n  get fd () { return this[_fd] }\n  get path () { return this[_path] }\n\n  [_onerror] (er) {\n    this[_close]()\n    this[_writing] = true\n    this.emit('error', er)\n  }\n\n  [_open] () {\n    fs.open(this[_path], this[_flags], this[_mode],\n      (er, fd) => this[_onopen](er, fd))\n  }\n\n  [_onopen] (er, fd) {\n    if (this[_defaultFlag] &&\n        this[_flags] === 'r+' &&\n        er && er.code === 'ENOENT') {\n      this[_flags] = 'w'\n      this[_open]()\n    } else if (er)\n      this[_onerror](er)\n    else {\n      this[_fd] = fd\n      this.emit('open', fd)\n      this[_flush]()\n    }\n  }\n\n  end (buf, enc) {\n    if (buf)\n      this.write(buf, enc)\n\n    this[_ended] = true\n\n    // synthetic after-write logic, where drain/finish live\n    if (!this[_writing] && !this[_queue].length &&\n        typeof this[_fd] === 'number')\n      this[_onwrite](null, 0)\n  }\n\n  write (buf, enc) {\n    if (typeof buf === 'string')\n      buf = new Buffer(buf, enc)\n\n    if (this[_ended]) {\n      this.emit('error', new Error('write() after end()'))\n      return false\n    }\n\n    if (this[_fd] === null || this[_writing] || this[_queue].length) {\n      this[_queue].push(buf)\n      this[_needDrain] = true\n      return false\n    }\n\n    this[_writing] = true\n    this[_write](buf)\n    return true\n  }\n\n  [_write] (buf) {\n    fs.write(this[_fd], buf, 0, buf.length, this[_pos], (er, bw) =>\n      this[_onwrite](er, bw))\n  }\n\n  [_onwrite] (er, bw) {\n    if (er)\n      this[_onerror](er)\n    else {\n      if (this[_pos] !== null)\n        this[_pos] += bw\n      if (this[_queue].length)\n        this[_flush]()\n      else {\n        this[_writing] = false\n\n        if (this[_ended] && !this[_finished]) {\n          this[_finished] = true\n          this[_close]()\n          this.emit('finish')\n        } else if (this[_needDrain]) {\n          this[_needDrain] = false\n          this.emit('drain')\n        }\n      }\n    }\n  }\n\n  [_flush] () {\n    if (this[_queue].length === 0) {\n      if (this[_ended])\n        this[_onwrite](null, 0)\n    } else if (this[_queue].length === 1)\n      this[_write](this[_queue].pop())\n    else {\n      const iovec = this[_queue]\n      this[_queue] = []\n      writev(this[_fd], iovec, this[_pos],\n        (er, bw) => this[_onwrite](er, bw))\n    }\n  }\n\n  [_close] () {\n    if (this[_autoClose] && typeof this[_fd] === 'number') {\n      fs.close(this[_fd], _ => this.emit('close'))\n      this[_fd] = null\n    }\n  }\n}\n\nclass WriteStreamSync extends WriteStream {\n  [_open] () {\n    let fd\n    try {\n      fd = fs.openSync(this[_path], this[_flags], this[_mode])\n    } catch (er) {\n      if (this[_defaultFlag] &&\n          this[_flags] === 'r+' &&\n          er && er.code === 'ENOENT') {\n        this[_flags] = 'w'\n        return this[_open]()\n      } else\n        throw er\n    }\n    this[_onopen](null, fd)\n  }\n\n  [_close] () {\n    if (this[_autoClose] && typeof this[_fd] === 'number') {\n      try {\n        fs.closeSync(this[_fd])\n      } catch (er) {}\n      this[_fd] = null\n      this.emit('close')\n    }\n  }\n\n  [_write] (buf) {\n    try {\n      this[_onwrite](null,\n        fs.writeSync(this[_fd], buf, 0, buf.length, this[_pos]))\n    } catch (er) {\n      this[_onwrite](er, 0)\n    }\n  }\n}\n\nconst writev = (fd, iovec, pos, cb) => {\n  const done = (er, bw) => cb(er, bw, iovec)\n  const req = new FSReqWrap()\n  req.oncomplete = done\n  binding.writeBuffers(fd, iovec, pos, req)\n}\n\nexports.ReadStream = ReadStream\nexports.ReadStreamSync = ReadStreamSync\n\nexports.WriteStream = WriteStream\nexports.WriteStreamSync = WriteStreamSync\n\n\n//# sourceURL=webpack:///./node_modules/fs-minipass/index.js?");

/***/ }),

/***/ "./node_modules/gauge/base-theme.js":
/*!******************************************!*\
  !*** ./node_modules/gauge/base-theme.js ***!
  \******************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar spin = __webpack_require__(/*! ./spin.js */ \"./node_modules/gauge/spin.js\")\nvar progressBar = __webpack_require__(/*! ./progress-bar.js */ \"./node_modules/gauge/progress-bar.js\")\n\nmodule.exports = {\n  activityIndicator: function (values, theme, width) {\n    if (values.spun == null) return\n    return spin(theme, values.spun)\n  },\n  progressbar: function (values, theme, width) {\n    if (values.completed == null) return\n    return progressBar(theme, width, values.completed)\n  }\n}\n\n\n//# sourceURL=webpack:///./node_modules/gauge/base-theme.js?");

/***/ }),

/***/ "./node_modules/gauge/error.js":
/*!*************************************!*\
  !*** ./node_modules/gauge/error.js ***!
  \*************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar util = __webpack_require__(/*! util */ \"util\")\n\nvar User = exports.User = function User (msg) {\n  var err = new Error(msg)\n  Error.captureStackTrace(err, User)\n  err.code = 'EGAUGE'\n  return err\n}\n\nexports.MissingTemplateValue = function MissingTemplateValue (item, values) {\n  var err = new User(util.format('Missing template value \"%s\"', item.type))\n  Error.captureStackTrace(err, MissingTemplateValue)\n  err.template = item\n  err.values = values\n  return err\n}\n\nexports.Internal = function Internal (msg) {\n  var err = new Error(msg)\n  Error.captureStackTrace(err, Internal)\n  err.code = 'EGAUGEINTERNAL'\n  return err\n}\n\n\n//# sourceURL=webpack:///./node_modules/gauge/error.js?");

/***/ }),

/***/ "./node_modules/gauge/has-color.js":
/*!*****************************************!*\
  !*** ./node_modules/gauge/has-color.js ***!
  \*****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = isWin32() || isColorTerm()\n\nfunction isWin32 () {\n  return process.platform === 'win32'\n}\n\nfunction isColorTerm () {\n  var termHasColor = /^screen|^xterm|^vt100|color|ansi|cygwin|linux/i\n  return !!process.env.COLORTERM || termHasColor.test(process.env.TERM)\n}\n\n\n//# sourceURL=webpack:///./node_modules/gauge/has-color.js?");

/***/ }),

/***/ "./node_modules/gauge/index.js":
/*!*************************************!*\
  !*** ./node_modules/gauge/index.js ***!
  \*************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar Plumbing = __webpack_require__(/*! ./plumbing.js */ \"./node_modules/gauge/plumbing.js\")\nvar hasUnicode = __webpack_require__(/*! has-unicode */ \"./node_modules/has-unicode/index.js\")\nvar hasColor = __webpack_require__(/*! ./has-color.js */ \"./node_modules/gauge/has-color.js\")\nvar onExit = __webpack_require__(/*! signal-exit */ \"signal-exit\")\nvar defaultThemes = __webpack_require__(/*! ./themes */ \"./node_modules/gauge/themes.js\")\nvar setInterval = __webpack_require__(/*! ./set-interval.js */ \"./node_modules/gauge/set-interval.js\")\nvar process = __webpack_require__(/*! ./process.js */ \"./node_modules/gauge/process.js\")\nvar setImmediate = __webpack_require__(/*! ./set-immediate */ \"./node_modules/gauge/set-immediate.js\")\n\nmodule.exports = Gauge\n\nfunction callWith (obj, method) {\n  return function () {\n    return method.call(obj)\n  }\n}\n\nfunction Gauge (arg1, arg2) {\n  var options, writeTo\n  if (arg1 && arg1.write) {\n    writeTo = arg1\n    options = arg2 || {}\n  } else if (arg2 && arg2.write) {\n    writeTo = arg2\n    options = arg1 || {}\n  } else {\n    writeTo = process.stderr\n    options = arg1 || arg2 || {}\n  }\n\n  this._status = {\n    spun: 0,\n    section: '',\n    subsection: ''\n  }\n  this._paused = false // are we paused for back pressure?\n  this._disabled = true // are all progress bar updates disabled?\n  this._showing = false // do we WANT the progress bar on screen\n  this._onScreen = false // IS the progress bar on screen\n  this._needsRedraw = false // should we print something at next tick?\n  this._hideCursor = options.hideCursor == null ? true : options.hideCursor\n  this._fixedFramerate = options.fixedFramerate == null\n    ? !(/^v0\\.8\\./.test(process.version))\n    : options.fixedFramerate\n  this._lastUpdateAt = null\n  this._updateInterval = options.updateInterval == null ? 50 : options.updateInterval\n\n  this._themes = options.themes || defaultThemes\n  this._theme = options.theme\n  var theme = this._computeTheme(options.theme)\n  var template = options.template || [\n    {type: 'progressbar', length: 20},\n    {type: 'activityIndicator', kerning: 1, length: 1},\n    {type: 'section', kerning: 1, default: ''},\n    {type: 'subsection', kerning: 1, default: ''}\n  ]\n  this.setWriteTo(writeTo, options.tty)\n  var PlumbingClass = options.Plumbing || Plumbing\n  this._gauge = new PlumbingClass(theme, template, this.getWidth())\n\n  this._$$doRedraw = callWith(this, this._doRedraw)\n  this._$$handleSizeChange = callWith(this, this._handleSizeChange)\n\n  this._cleanupOnExit = options.cleanupOnExit == null || options.cleanupOnExit\n  this._removeOnExit = null\n\n  if (options.enabled || (options.enabled == null && this._tty && this._tty.isTTY)) {\n    this.enable()\n  } else {\n    this.disable()\n  }\n}\nGauge.prototype = {}\n\nGauge.prototype.isEnabled = function () {\n  return !this._disabled\n}\n\nGauge.prototype.setTemplate = function (template) {\n  this._gauge.setTemplate(template)\n  if (this._showing) this._requestRedraw()\n}\n\nGauge.prototype._computeTheme = function (theme) {\n  if (!theme) theme = {}\n  if (typeof theme === 'string') {\n    theme = this._themes.getTheme(theme)\n  } else if (theme && (Object.keys(theme).length === 0 || theme.hasUnicode != null || theme.hasColor != null)) {\n    var useUnicode = theme.hasUnicode == null ? hasUnicode() : theme.hasUnicode\n    var useColor = theme.hasColor == null ? hasColor : theme.hasColor\n    theme = this._themes.getDefault({hasUnicode: useUnicode, hasColor: useColor, platform: theme.platform})\n  }\n  return theme\n}\n\nGauge.prototype.setThemeset = function (themes) {\n  this._themes = themes\n  this.setTheme(this._theme)\n}\n\nGauge.prototype.setTheme = function (theme) {\n  this._gauge.setTheme(this._computeTheme(theme))\n  if (this._showing) this._requestRedraw()\n  this._theme = theme\n}\n\nGauge.prototype._requestRedraw = function () {\n  this._needsRedraw = true\n  if (!this._fixedFramerate) this._doRedraw()\n}\n\nGauge.prototype.getWidth = function () {\n  return ((this._tty && this._tty.columns) || 80) - 1\n}\n\nGauge.prototype.setWriteTo = function (writeTo, tty) {\n  var enabled = !this._disabled\n  if (enabled) this.disable()\n  this._writeTo = writeTo\n  this._tty = tty ||\n    (writeTo === process.stderr && process.stdout.isTTY && process.stdout) ||\n    (writeTo.isTTY && writeTo) ||\n    this._tty\n  if (this._gauge) this._gauge.setWidth(this.getWidth())\n  if (enabled) this.enable()\n}\n\nGauge.prototype.enable = function () {\n  if (!this._disabled) return\n  this._disabled = false\n  if (this._tty) this._enableEvents()\n  if (this._showing) this.show()\n}\n\nGauge.prototype.disable = function () {\n  if (this._disabled) return\n  if (this._showing) {\n    this._lastUpdateAt = null\n    this._showing = false\n    this._doRedraw()\n    this._showing = true\n  }\n  this._disabled = true\n  if (this._tty) this._disableEvents()\n}\n\nGauge.prototype._enableEvents = function () {\n  if (this._cleanupOnExit) {\n    this._removeOnExit = onExit(callWith(this, this.disable))\n  }\n  this._tty.on('resize', this._$$handleSizeChange)\n  if (this._fixedFramerate) {\n    this.redrawTracker = setInterval(this._$$doRedraw, this._updateInterval)\n    if (this.redrawTracker.unref) this.redrawTracker.unref()\n  }\n}\n\nGauge.prototype._disableEvents = function () {\n  this._tty.removeListener('resize', this._$$handleSizeChange)\n  if (this._fixedFramerate) clearInterval(this.redrawTracker)\n  if (this._removeOnExit) this._removeOnExit()\n}\n\nGauge.prototype.hide = function (cb) {\n  if (this._disabled) return cb && process.nextTick(cb)\n  if (!this._showing) return cb && process.nextTick(cb)\n  this._showing = false\n  this._doRedraw()\n  cb && setImmediate(cb)\n}\n\nGauge.prototype.show = function (section, completed) {\n  this._showing = true\n  if (typeof section === 'string') {\n    this._status.section = section\n  } else if (typeof section === 'object') {\n    var sectionKeys = Object.keys(section)\n    for (var ii = 0; ii < sectionKeys.length; ++ii) {\n      var key = sectionKeys[ii]\n      this._status[key] = section[key]\n    }\n  }\n  if (completed != null) this._status.completed = completed\n  if (this._disabled) return\n  this._requestRedraw()\n}\n\nGauge.prototype.pulse = function (subsection) {\n  this._status.subsection = subsection || ''\n  this._status.spun ++\n  if (this._disabled) return\n  if (!this._showing) return\n  this._requestRedraw()\n}\n\nGauge.prototype._handleSizeChange = function () {\n  this._gauge.setWidth(this._tty.columns - 1)\n  this._requestRedraw()\n}\n\nGauge.prototype._doRedraw = function () {\n  if (this._disabled || this._paused) return\n  if (!this._fixedFramerate) {\n    var now = Date.now()\n    if (this._lastUpdateAt && now - this._lastUpdateAt < this._updateInterval) return\n    this._lastUpdateAt = now\n  }\n  if (!this._showing && this._onScreen) {\n    this._onScreen = false\n    var result = this._gauge.hide()\n    if (this._hideCursor) {\n      result += this._gauge.showCursor()\n    }\n    return this._writeTo.write(result)\n  }\n  if (!this._showing && !this._onScreen) return\n  if (this._showing && !this._onScreen) {\n    this._onScreen = true\n    this._needsRedraw = true\n    if (this._hideCursor) {\n      this._writeTo.write(this._gauge.hideCursor())\n    }\n  }\n  if (!this._needsRedraw) return\n  if (!this._writeTo.write(this._gauge.show(this._status))) {\n    this._paused = true\n    this._writeTo.on('drain', callWith(this, function () {\n      this._paused = false\n      this._doRedraw()\n    }))\n  }\n}\n\n\n//# sourceURL=webpack:///./node_modules/gauge/index.js?");

/***/ }),

/***/ "./node_modules/gauge/plumbing.js":
/*!****************************************!*\
  !*** ./node_modules/gauge/plumbing.js ***!
  \****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar consoleControl = __webpack_require__(/*! console-control-strings */ \"./node_modules/console-control-strings/index.js\")\nvar renderTemplate = __webpack_require__(/*! ./render-template.js */ \"./node_modules/gauge/render-template.js\")\nvar validate = __webpack_require__(/*! aproba */ \"aproba\")\n\nvar Plumbing = module.exports = function (theme, template, width) {\n  if (!width) width = 80\n  validate('OAN', [theme, template, width])\n  this.showing = false\n  this.theme = theme\n  this.width = width\n  this.template = template\n}\nPlumbing.prototype = {}\n\nPlumbing.prototype.setTheme = function (theme) {\n  validate('O', [theme])\n  this.theme = theme\n}\n\nPlumbing.prototype.setTemplate = function (template) {\n  validate('A', [template])\n  this.template = template\n}\n\nPlumbing.prototype.setWidth = function (width) {\n  validate('N', [width])\n  this.width = width\n}\n\nPlumbing.prototype.hide = function () {\n  return consoleControl.gotoSOL() + consoleControl.eraseLine()\n}\n\nPlumbing.prototype.hideCursor = consoleControl.hideCursor\n\nPlumbing.prototype.showCursor = consoleControl.showCursor\n\nPlumbing.prototype.show = function (status) {\n  var values = Object.create(this.theme)\n  for (var key in status) {\n    values[key] = status[key]\n  }\n\n  return renderTemplate(this.width, this.template, values).trim() +\n         consoleControl.color('reset') +\n         consoleControl.eraseLine() + consoleControl.gotoSOL()\n}\n\n\n//# sourceURL=webpack:///./node_modules/gauge/plumbing.js?");

/***/ }),

/***/ "./node_modules/gauge/process.js":
/*!***************************************!*\
  !*** ./node_modules/gauge/process.js ***!
  \***************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n// this exists so we can replace it during testing\nmodule.exports = process\n\n\n//# sourceURL=webpack:///./node_modules/gauge/process.js?");

/***/ }),

/***/ "./node_modules/gauge/progress-bar.js":
/*!********************************************!*\
  !*** ./node_modules/gauge/progress-bar.js ***!
  \********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar validate = __webpack_require__(/*! aproba */ \"aproba\")\nvar renderTemplate = __webpack_require__(/*! ./render-template.js */ \"./node_modules/gauge/render-template.js\")\nvar wideTruncate = __webpack_require__(/*! ./wide-truncate */ \"./node_modules/gauge/wide-truncate.js\")\nvar stringWidth = __webpack_require__(/*! string-width */ \"string-width\")\n\nmodule.exports = function (theme, width, completed) {\n  validate('ONN', [theme, width, completed])\n  if (completed < 0) completed = 0\n  if (completed > 1) completed = 1\n  if (width <= 0) return ''\n  var sofar = Math.round(width * completed)\n  var rest = width - sofar\n  var template = [\n    {type: 'complete', value: repeat(theme.complete, sofar), length: sofar},\n    {type: 'remaining', value: repeat(theme.remaining, rest), length: rest}\n  ]\n  return renderTemplate(width, template, theme)\n}\n\n// lodash's way of repeating\nfunction repeat (string, width) {\n  var result = ''\n  var n = width\n  do {\n    if (n % 2) {\n      result += string\n    }\n    n = Math.floor(n / 2)\n    /*eslint no-self-assign: 0*/\n    string += string\n  } while (n && stringWidth(result) < width)\n\n  return wideTruncate(result, width)\n}\n\n\n//# sourceURL=webpack:///./node_modules/gauge/progress-bar.js?");

/***/ }),

/***/ "./node_modules/gauge/render-template.js":
/*!***********************************************!*\
  !*** ./node_modules/gauge/render-template.js ***!
  \***********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar align = __webpack_require__(/*! wide-align */ \"./node_modules/wide-align/align.js\")\nvar validate = __webpack_require__(/*! aproba */ \"aproba\")\nvar objectAssign = __webpack_require__(/*! object-assign */ \"object-assign\")\nvar wideTruncate = __webpack_require__(/*! ./wide-truncate */ \"./node_modules/gauge/wide-truncate.js\")\nvar error = __webpack_require__(/*! ./error */ \"./node_modules/gauge/error.js\")\nvar TemplateItem = __webpack_require__(/*! ./template-item */ \"./node_modules/gauge/template-item.js\")\n\nfunction renderValueWithValues (values) {\n  return function (item) {\n    return renderValue(item, values)\n  }\n}\n\nvar renderTemplate = module.exports = function (width, template, values) {\n  var items = prepareItems(width, template, values)\n  var rendered = items.map(renderValueWithValues(values)).join('')\n  return align.left(wideTruncate(rendered, width), width)\n}\n\nfunction preType (item) {\n  var cappedTypeName = item.type[0].toUpperCase() + item.type.slice(1)\n  return 'pre' + cappedTypeName\n}\n\nfunction postType (item) {\n  var cappedTypeName = item.type[0].toUpperCase() + item.type.slice(1)\n  return 'post' + cappedTypeName\n}\n\nfunction hasPreOrPost (item, values) {\n  if (!item.type) return\n  return values[preType(item)] || values[postType(item)]\n}\n\nfunction generatePreAndPost (baseItem, parentValues) {\n  var item = objectAssign({}, baseItem)\n  var values = Object.create(parentValues)\n  var template = []\n  var pre = preType(item)\n  var post = postType(item)\n  if (values[pre]) {\n    template.push({value: values[pre]})\n    values[pre] = null\n  }\n  item.minLength = null\n  item.length = null\n  item.maxLength = null\n  template.push(item)\n  values[item.type] = values[item.type]\n  if (values[post]) {\n    template.push({value: values[post]})\n    values[post] = null\n  }\n  return function ($1, $2, length) {\n    return renderTemplate(length, template, values)\n  }\n}\n\nfunction prepareItems (width, template, values) {\n  function cloneAndObjectify (item, index, arr) {\n    var cloned = new TemplateItem(item, width)\n    var type = cloned.type\n    if (cloned.value == null) {\n      if (!(type in values)) {\n        if (cloned.default == null) {\n          throw new error.MissingTemplateValue(cloned, values)\n        } else {\n          cloned.value = cloned.default\n        }\n      } else {\n        cloned.value = values[type]\n      }\n    }\n    if (cloned.value == null || cloned.value === '') return null\n    cloned.index = index\n    cloned.first = index === 0\n    cloned.last = index === arr.length - 1\n    if (hasPreOrPost(cloned, values)) cloned.value = generatePreAndPost(cloned, values)\n    return cloned\n  }\n\n  var output = template.map(cloneAndObjectify).filter(function (item) { return item != null })\n\n  var outputLength = 0\n  var remainingSpace = width\n  var variableCount = output.length\n\n  function consumeSpace (length) {\n    if (length > remainingSpace) length = remainingSpace\n    outputLength += length\n    remainingSpace -= length\n  }\n\n  function finishSizing (item, length) {\n    if (item.finished) throw new error.Internal('Tried to finish template item that was already finished')\n    if (length === Infinity) throw new error.Internal('Length of template item cannot be infinity')\n    if (length != null) item.length = length\n    item.minLength = null\n    item.maxLength = null\n    --variableCount\n    item.finished = true\n    if (item.length == null) item.length = item.getBaseLength()\n    if (item.length == null) throw new error.Internal('Finished template items must have a length')\n    consumeSpace(item.getLength())\n  }\n\n  output.forEach(function (item) {\n    if (!item.kerning) return\n    var prevPadRight = item.first ? 0 : output[item.index - 1].padRight\n    if (!item.first && prevPadRight < item.kerning) item.padLeft = item.kerning - prevPadRight\n    if (!item.last) item.padRight = item.kerning\n  })\n\n  // Finish any that have a fixed (literal or intuited) length\n  output.forEach(function (item) {\n    if (item.getBaseLength() == null) return\n    finishSizing(item)\n  })\n\n  var resized = 0\n  var resizing\n  var hunkSize\n  do {\n    resizing = false\n    hunkSize = Math.round(remainingSpace / variableCount)\n    output.forEach(function (item) {\n      if (item.finished) return\n      if (!item.maxLength) return\n      if (item.getMaxLength() < hunkSize) {\n        finishSizing(item, item.maxLength)\n        resizing = true\n      }\n    })\n  } while (resizing && resized++ < output.length)\n  if (resizing) throw new error.Internal('Resize loop iterated too many times while determining maxLength')\n\n  resized = 0\n  do {\n    resizing = false\n    hunkSize = Math.round(remainingSpace / variableCount)\n    output.forEach(function (item) {\n      if (item.finished) return\n      if (!item.minLength) return\n      if (item.getMinLength() >= hunkSize) {\n        finishSizing(item, item.minLength)\n        resizing = true\n      }\n    })\n  } while (resizing && resized++ < output.length)\n  if (resizing) throw new error.Internal('Resize loop iterated too many times while determining minLength')\n\n  hunkSize = Math.round(remainingSpace / variableCount)\n  output.forEach(function (item) {\n    if (item.finished) return\n    finishSizing(item, hunkSize)\n  })\n\n  return output\n}\n\nfunction renderFunction (item, values, length) {\n  validate('OON', arguments)\n  if (item.type) {\n    return item.value(values, values[item.type + 'Theme'] || {}, length)\n  } else {\n    return item.value(values, {}, length)\n  }\n}\n\nfunction renderValue (item, values) {\n  var length = item.getBaseLength()\n  var value = typeof item.value === 'function' ? renderFunction(item, values, length) : item.value\n  if (value == null || value === '') return ''\n  var alignWith = align[item.align] || align.left\n  var leftPadding = item.padLeft ? align.left('', item.padLeft) : ''\n  var rightPadding = item.padRight ? align.right('', item.padRight) : ''\n  var truncated = wideTruncate(String(value), length)\n  var aligned = alignWith(truncated, length)\n  return leftPadding + aligned + rightPadding\n}\n\n\n//# sourceURL=webpack:///./node_modules/gauge/render-template.js?");

/***/ }),

/***/ "./node_modules/gauge/set-immediate.js":
/*!*********************************************!*\
  !*** ./node_modules/gauge/set-immediate.js ***!
  \*********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar process = __webpack_require__(/*! ./process */ \"./node_modules/gauge/process.js\")\ntry {\n  module.exports = setImmediate\n} catch (ex) {\n  module.exports = process.nextTick\n}\n\n\n//# sourceURL=webpack:///./node_modules/gauge/set-immediate.js?");

/***/ }),

/***/ "./node_modules/gauge/set-interval.js":
/*!********************************************!*\
  !*** ./node_modules/gauge/set-interval.js ***!
  \********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n// this exists so we can replace it during testing\nmodule.exports = setInterval\n\n\n//# sourceURL=webpack:///./node_modules/gauge/set-interval.js?");

/***/ }),

/***/ "./node_modules/gauge/spin.js":
/*!************************************!*\
  !*** ./node_modules/gauge/spin.js ***!
  \************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = function spin (spinstr, spun) {\n  return spinstr[spun % spinstr.length]\n}\n\n\n//# sourceURL=webpack:///./node_modules/gauge/spin.js?");

/***/ }),

/***/ "./node_modules/gauge/template-item.js":
/*!*********************************************!*\
  !*** ./node_modules/gauge/template-item.js ***!
  \*********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar stringWidth = __webpack_require__(/*! string-width */ \"string-width\")\n\nmodule.exports = TemplateItem\n\nfunction isPercent (num) {\n  if (typeof num !== 'string') return false\n  return num.slice(-1) === '%'\n}\n\nfunction percent (num) {\n  return Number(num.slice(0, -1)) / 100\n}\n\nfunction TemplateItem (values, outputLength) {\n  this.overallOutputLength = outputLength\n  this.finished = false\n  this.type = null\n  this.value = null\n  this.length = null\n  this.maxLength = null\n  this.minLength = null\n  this.kerning = null\n  this.align = 'left'\n  this.padLeft = 0\n  this.padRight = 0\n  this.index = null\n  this.first = null\n  this.last = null\n  if (typeof values === 'string') {\n    this.value = values\n  } else {\n    for (var prop in values) this[prop] = values[prop]\n  }\n  // Realize percents\n  if (isPercent(this.length)) {\n    this.length = Math.round(this.overallOutputLength * percent(this.length))\n  }\n  if (isPercent(this.minLength)) {\n    this.minLength = Math.round(this.overallOutputLength * percent(this.minLength))\n  }\n  if (isPercent(this.maxLength)) {\n    this.maxLength = Math.round(this.overallOutputLength * percent(this.maxLength))\n  }\n  return this\n}\n\nTemplateItem.prototype = {}\n\nTemplateItem.prototype.getBaseLength = function () {\n  var length = this.length\n  if (length == null && typeof this.value === 'string' && this.maxLength == null && this.minLength == null) {\n    length = stringWidth(this.value)\n  }\n  return length\n}\n\nTemplateItem.prototype.getLength = function () {\n  var length = this.getBaseLength()\n  if (length == null) return null\n  return length + this.padLeft + this.padRight\n}\n\nTemplateItem.prototype.getMaxLength = function () {\n  if (this.maxLength == null) return null\n  return this.maxLength + this.padLeft + this.padRight\n}\n\nTemplateItem.prototype.getMinLength = function () {\n  if (this.minLength == null) return null\n  return this.minLength + this.padLeft + this.padRight\n}\n\n\n\n//# sourceURL=webpack:///./node_modules/gauge/template-item.js?");

/***/ }),

/***/ "./node_modules/gauge/theme-set.js":
/*!*****************************************!*\
  !*** ./node_modules/gauge/theme-set.js ***!
  \*****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar objectAssign = __webpack_require__(/*! object-assign */ \"object-assign\")\n\nmodule.exports = function () {\n  return ThemeSetProto.newThemeSet()\n}\n\nvar ThemeSetProto = {}\n\nThemeSetProto.baseTheme = __webpack_require__(/*! ./base-theme.js */ \"./node_modules/gauge/base-theme.js\")\n\nThemeSetProto.newTheme = function (parent, theme) {\n  if (!theme) {\n    theme = parent\n    parent = this.baseTheme\n  }\n  return objectAssign({}, parent, theme)\n}\n\nThemeSetProto.getThemeNames = function () {\n  return Object.keys(this.themes)\n}\n\nThemeSetProto.addTheme = function (name, parent, theme) {\n  this.themes[name] = this.newTheme(parent, theme)\n}\n\nThemeSetProto.addToAllThemes = function (theme) {\n  var themes = this.themes\n  Object.keys(themes).forEach(function (name) {\n    objectAssign(themes[name], theme)\n  })\n  objectAssign(this.baseTheme, theme)\n}\n\nThemeSetProto.getTheme = function (name) {\n  if (!this.themes[name]) throw this.newMissingThemeError(name)\n  return this.themes[name]\n}\n\nThemeSetProto.setDefault = function (opts, name) {\n  if (name == null) {\n    name = opts\n    opts = {}\n  }\n  var platform = opts.platform == null ? 'fallback' : opts.platform\n  var hasUnicode = !!opts.hasUnicode\n  var hasColor = !!opts.hasColor\n  if (!this.defaults[platform]) this.defaults[platform] = {true: {}, false: {}}\n  this.defaults[platform][hasUnicode][hasColor] = name\n}\n\nThemeSetProto.getDefault = function (opts) {\n  if (!opts) opts = {}\n  var platformName = opts.platform || process.platform\n  var platform = this.defaults[platformName] || this.defaults.fallback\n  var hasUnicode = !!opts.hasUnicode\n  var hasColor = !!opts.hasColor\n  if (!platform) throw this.newMissingDefaultThemeError(platformName, hasUnicode, hasColor)\n  if (!platform[hasUnicode][hasColor]) {\n    if (hasUnicode && hasColor && platform[!hasUnicode][hasColor]) {\n      hasUnicode = false\n    } else if (hasUnicode && hasColor && platform[hasUnicode][!hasColor]) {\n      hasColor = false\n    } else if (hasUnicode && hasColor && platform[!hasUnicode][!hasColor]) {\n      hasUnicode = false\n      hasColor = false\n    } else if (hasUnicode && !hasColor && platform[!hasUnicode][hasColor]) {\n      hasUnicode = false\n    } else if (!hasUnicode && hasColor && platform[hasUnicode][!hasColor]) {\n      hasColor = false\n    } else if (platform === this.defaults.fallback) {\n      throw this.newMissingDefaultThemeError(platformName, hasUnicode, hasColor)\n    }\n  }\n  if (platform[hasUnicode][hasColor]) {\n    return this.getTheme(platform[hasUnicode][hasColor])\n  } else {\n    return this.getDefault(objectAssign({}, opts, {platform: 'fallback'}))\n  }\n}\n\nThemeSetProto.newMissingThemeError = function newMissingThemeError (name) {\n  var err = new Error('Could not find a gauge theme named \"' + name + '\"')\n  Error.captureStackTrace.call(err, newMissingThemeError)\n  err.theme = name\n  err.code = 'EMISSINGTHEME'\n  return err\n}\n\nThemeSetProto.newMissingDefaultThemeError = function newMissingDefaultThemeError (platformName, hasUnicode, hasColor) {\n  var err = new Error(\n    'Could not find a gauge theme for your platform/unicode/color use combo:\\n' +\n    '    platform = ' + platformName + '\\n' +\n    '    hasUnicode = ' + hasUnicode + '\\n' +\n    '    hasColor = ' + hasColor)\n  Error.captureStackTrace.call(err, newMissingDefaultThemeError)\n  err.platform = platformName\n  err.hasUnicode = hasUnicode\n  err.hasColor = hasColor\n  err.code = 'EMISSINGTHEME'\n  return err\n}\n\nThemeSetProto.newThemeSet = function () {\n  var themeset = function (opts) {\n    return themeset.getDefault(opts)\n  }\n  return objectAssign(themeset, ThemeSetProto, {\n    themes: objectAssign({}, this.themes),\n    baseTheme: objectAssign({}, this.baseTheme),\n    defaults: JSON.parse(JSON.stringify(this.defaults || {}))\n  })\n}\n\n\n\n//# sourceURL=webpack:///./node_modules/gauge/theme-set.js?");

/***/ }),

/***/ "./node_modules/gauge/themes.js":
/*!**************************************!*\
  !*** ./node_modules/gauge/themes.js ***!
  \**************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar consoleControl = __webpack_require__(/*! console-control-strings */ \"./node_modules/console-control-strings/index.js\")\nvar ThemeSet = __webpack_require__(/*! ./theme-set.js */ \"./node_modules/gauge/theme-set.js\")\n\nvar themes = module.exports = new ThemeSet()\n\nthemes.addTheme('ASCII', {\n  preProgressbar: '[',\n  postProgressbar: ']',\n  progressbarTheme: {\n    complete: '#',\n    remaining: '.'\n  },\n  activityIndicatorTheme: '-\\\\|/',\n  preSubsection: '>'\n})\n\nthemes.addTheme('colorASCII', themes.getTheme('ASCII'), {\n  progressbarTheme: {\n    preComplete: consoleControl.color('inverse'),\n    complete: ' ',\n    postComplete: consoleControl.color('stopInverse'),\n    preRemaining: consoleControl.color('brightBlack'),\n    remaining: '.',\n    postRemaining: consoleControl.color('reset')\n  }\n})\n\nthemes.addTheme('brailleSpinner', {\n  preProgressbar: '⸨',\n  postProgressbar: '⸩',\n  progressbarTheme: {\n    complete: '░',\n    remaining: '⠂'\n  },\n  activityIndicatorTheme: '⠋⠙⠹⠸⠼⠴⠦⠧⠇⠏',\n  preSubsection: '>'\n})\n\nthemes.addTheme('colorBrailleSpinner', themes.getTheme('brailleSpinner'), {\n  progressbarTheme: {\n    preComplete: consoleControl.color('inverse'),\n    complete: ' ',\n    postComplete: consoleControl.color('stopInverse'),\n    preRemaining: consoleControl.color('brightBlack'),\n    remaining: '░',\n    postRemaining: consoleControl.color('reset')\n  }\n})\n\nthemes.setDefault({}, 'ASCII')\nthemes.setDefault({hasColor: true}, 'colorASCII')\nthemes.setDefault({platform: 'darwin', hasUnicode: true}, 'brailleSpinner')\nthemes.setDefault({platform: 'darwin', hasUnicode: true, hasColor: true}, 'colorBrailleSpinner')\n\n\n//# sourceURL=webpack:///./node_modules/gauge/themes.js?");

/***/ }),

/***/ "./node_modules/gauge/wide-truncate.js":
/*!*********************************************!*\
  !*** ./node_modules/gauge/wide-truncate.js ***!
  \*********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar stringWidth = __webpack_require__(/*! string-width */ \"string-width\")\nvar stripAnsi = __webpack_require__(/*! strip-ansi */ \"strip-ansi\")\n\nmodule.exports = wideTruncate\n\nfunction wideTruncate (str, target) {\n  if (stringWidth(str) === 0) return str\n  if (target <= 0) return ''\n  if (stringWidth(str) <= target) return str\n\n  // We compute the number of bytes of ansi sequences here and add\n  // that to our initial truncation to ensure that we don't slice one\n  // that we want to keep in half.\n  var noAnsi = stripAnsi(str)\n  var ansiSize = str.length + noAnsi.length\n  var truncated = str.slice(0, target + ansiSize)\n\n  // we have to shrink the result to account for our ansi sequence buffer\n  // (if an ansi sequence was truncated) and double width characters.\n  while (stringWidth(truncated) > target) {\n    truncated = truncated.slice(0, -1)\n  }\n  return truncated\n}\n\n\n//# sourceURL=webpack:///./node_modules/gauge/wide-truncate.js?");

/***/ }),

/***/ "./node_modules/has-unicode/index.js":
/*!*******************************************!*\
  !*** ./node_modules/has-unicode/index.js ***!
  \*******************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar os = __webpack_require__(/*! os */ \"os\")\n\nvar hasUnicode = module.exports = function () {\n  // Recent Win32 platforms (>XP) CAN support unicode in the console but\n  // don't have to, and in non-english locales often use traditional local\n  // code pages. There's no way, short of windows system calls or execing\n  // the chcp command line program to figure this out. As such, we default\n  // this to false and encourage your users to override it via config if\n  // appropriate.\n  if (os.type() == \"Windows_NT\") { return false }\n\n  var isUTF8 = /UTF-?8$/i\n  var ctype = process.env.LC_ALL || process.env.LC_CTYPE || process.env.LANG\n  return isUTF8.test(ctype)\n}\n\n\n//# sourceURL=webpack:///./node_modules/has-unicode/index.js?");

/***/ }),

/***/ "./node_modules/ignore-walk/index.js":
/*!*******************************************!*\
  !*** ./node_modules/ignore-walk/index.js ***!
  \*******************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nconst fs = __webpack_require__(/*! fs */ \"fs\")\nconst path = __webpack_require__(/*! path */ \"path\")\nconst EE = __webpack_require__(/*! events */ \"events\").EventEmitter\nconst Minimatch = __webpack_require__(/*! minimatch */ \"minimatch\").Minimatch\n\nclass Walker extends EE {\n  constructor (opts) {\n    opts = opts || {}\n    super(opts)\n    this.path = opts.path || process.cwd()\n    this.basename = path.basename(this.path)\n    this.ignoreFiles = opts.ignoreFiles || [ '.ignore' ]\n    this.ignoreRules = {}\n    this.parent = opts.parent || null\n    this.includeEmpty = !!opts.includeEmpty\n    this.root = this.parent ? this.parent.root : this.path\n    this.follow = !!opts.follow\n    this.result = this.parent ? this.parent.result : new Set()\n    this.entries = null\n    this.sawError = false\n  }\n\n  sort (a, b) {\n    return a.localeCompare(b)\n  }\n\n  emit (ev, data) {\n    let ret = false\n    if (!(this.sawError && ev === 'error')) {\n      if (ev === 'error')\n        this.sawError = true\n      else if (ev === 'done' && !this.parent) {\n        data = Array.from(data)\n          .map(e => /^@/.test(e) ? `./${e}` : e).sort(this.sort)\n        this.result = data\n      }\n\n      if (ev === 'error' && this.parent)\n        ret = this.parent.emit('error', data)\n      else\n        ret = super.emit(ev, data)\n    }\n    return ret\n  }\n\n  start () {\n    fs.readdir(this.path, (er, entries) =>\n      er ? this.emit('error', er) : this.onReaddir(entries))\n    return this\n  }\n\n  isIgnoreFile (e) {\n    return e !== \".\" &&\n      e !== \"..\" &&\n      -1 !== this.ignoreFiles.indexOf(e)\n  }\n\n  onReaddir (entries) {\n    this.entries = entries\n    if (entries.length === 0) {\n      if (this.includeEmpty)\n        this.result.add(this.path.substr(this.root.length + 1))\n      this.emit('done', this.result)\n    } else {\n      const hasIg = this.entries.some(e =>\n        this.isIgnoreFile(e))\n\n      if (hasIg)\n        this.addIgnoreFiles()\n      else\n        this.filterEntries()\n    }\n  }\n\n  addIgnoreFiles () {\n    const newIg = this.entries\n      .filter(e => this.isIgnoreFile(e))\n\n    let igCount = newIg.length\n    const then = _ => {\n      if (--igCount === 0)\n        this.filterEntries()\n    }\n\n    newIg.forEach(e => this.addIgnoreFile(e, then))\n  }\n\n  addIgnoreFile (file, then) {\n    const ig = path.resolve(this.path, file)\n    fs.readFile(ig, 'utf8', (er, data) =>\n      er ? this.emit('error', er) : this.onReadIgnoreFile(file, data, then))\n  }\n\n  onReadIgnoreFile (file, data, then) {\n    const mmopt = {\n      matchBase: true,\n      dot: true,\n      flipNegate: true,\n      nocase: true\n    }\n    const rules = data.split(/\\r?\\n/)\n      .filter(line => !/^#|^$/.test(line.trim()))\n      .map(r => new Minimatch(r, mmopt))\n\n    this.ignoreRules[file] = rules\n\n    then()\n  }\n\n  filterEntries () {\n    // at this point we either have ignore rules, or just inheriting\n    // this exclusion is at the point where we know the list of\n    // entries in the dir, but don't know what they are.  since\n    // some of them *might* be directories, we have to run the\n    // match in dir-mode as well, so that we'll pick up partials\n    // of files that will be included later.  Anything included\n    // at this point will be checked again later once we know\n    // what it is.\n    const filtered = this.entries.map(entry => {\n      // at this point, we don't know if it's a dir or not.\n      const passFile = this.filterEntry(entry)\n      const passDir = this.filterEntry(entry, true)\n      return (passFile || passDir) ? [entry, passFile, passDir] : false\n    }).filter(e => e)\n\n    // now we stat them all\n    // if it's a dir, and passes as a dir, then recurse\n    // if it's not a dir, but passes as a file, add to set\n    let entryCount = filtered.length\n    if (entryCount === 0) {\n      this.emit('done', this.result)\n    } else {\n      const then = _ => {\n        if (-- entryCount === 0)\n          this.emit('done', this.result)\n      }\n      filtered.forEach(filt => {\n        const entry = filt[0]\n        const file = filt[1]\n        const dir = filt[2]\n        this.stat(entry, file, dir, then)\n      })\n    }\n  }\n\n  onstat (st, entry, file, dir, then) {\n    const abs = this.path + '/' + entry\n    if (!st.isDirectory()) {\n      if (file)\n        this.result.add(abs.substr(this.root.length + 1))\n      then()\n    } else {\n      // is a directory\n      if (dir)\n        this.walker(entry, then)\n      else\n        then()\n    }\n  }\n\n  stat (entry, file, dir, then) {\n    const abs = this.path + '/' + entry\n    fs[this.follow ? 'stat' : 'lstat'](abs, (er, st) => {\n      if (er)\n        this.emit('error', er)\n      else\n        this.onstat(st, entry, file, dir, then)\n    })\n  }\n\n  walkerOpt (entry) {\n    return {\n      path: this.path + '/' + entry,\n      parent: this,\n      ignoreFiles: this.ignoreFiles,\n      follow: this.follow,\n      includeEmpty: this.includeEmpty\n    }\n  }\n\n  walker (entry, then) {\n    new Walker(this.walkerOpt(entry)).on('done', then).start()\n  }\n\n  filterEntry (entry, partial) {\n    let included = true\n\n    // this = /a/b/c\n    // entry = d\n    // parent /a/b sees c/d\n    if (this.parent && this.parent.filterEntry) {\n      var pt = this.basename + \"/\" + entry\n      included = this.parent.filterEntry(pt, partial)\n    }\n\n    this.ignoreFiles.forEach(f => {\n      if (this.ignoreRules[f]) {\n        this.ignoreRules[f].forEach(rule => {\n          // negation means inclusion\n          // so if it's negated, and already included, no need to check\n          // likewise if it's neither negated nor included\n          if (rule.negate !== included) {\n            // first, match against /foo/bar\n            // then, against foo/bar\n            // then, in the case of partials, match with a /\n            const match = rule.match('/' + entry) ||\n              rule.match(entry) ||\n              (!!partial && (\n                rule.match('/' + entry + '/') ||\n                rule.match(entry + '/'))) ||\n              (!!partial && rule.negate && (\n                rule.match('/' + entry, true) ||\n                rule.match(entry, true)))\n\n            if (match)\n              included = rule.negate\n          }\n        })\n      }\n    })\n\n    return included\n  }\n}\n\nclass WalkerSync extends Walker {\n  constructor (opt) {\n    super(opt)\n  }\n\n  start () {\n    this.onReaddir(fs.readdirSync(this.path))\n    return this\n  }\n\n  addIgnoreFile (file, then) {\n    const ig = path.resolve(this.path, file)\n    this.onReadIgnoreFile(file, fs.readFileSync(ig, 'utf8'), then)\n  }\n\n  stat (entry, file, dir, then) {\n    const abs = this.path + '/' + entry\n    const st = fs[this.follow ? 'statSync' : 'lstatSync'](abs)\n    this.onstat(st, entry, file, dir, then)\n  }\n\n  walker (entry, then) {\n    new WalkerSync(this.walkerOpt(entry)).start()\n    then()\n  }\n}\n\nconst walk = (options, callback) => {\n  const p = new Promise((resolve, reject) => {\n    new Walker(options).on('done', resolve).on('error', reject).start()\n  })\n  return callback ? p.then(res => callback(null, res), callback) : p\n}\n\nconst walkSync = options => {\n  return new WalkerSync(options).start().result\n}\n\nmodule.exports = walk\nwalk.sync = walkSync\nwalk.Walker = Walker\nwalk.WalkerSync = WalkerSync\n\n\n//# sourceURL=webpack:///./node_modules/ignore-walk/index.js?");

/***/ }),

/***/ "./node_modules/minipass/index.js":
/*!****************************************!*\
  !*** ./node_modules/minipass/index.js ***!
  \****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nconst EE = __webpack_require__(/*! events */ \"events\")\nconst Yallist = __webpack_require__(/*! yallist */ \"yallist\")\nconst SD = __webpack_require__(/*! string_decoder */ \"string_decoder\").StringDecoder\n\nconst EOF = Symbol('EOF')\nconst MAYBE_EMIT_END = Symbol('maybeEmitEnd')\nconst EMITTED_END = Symbol('emittedEnd')\nconst EMITTING_END = Symbol('emittingEnd')\nconst CLOSED = Symbol('closed')\nconst READ = Symbol('read')\nconst FLUSH = Symbol('flush')\nconst FLUSHCHUNK = Symbol('flushChunk')\nconst ENCODING = Symbol('encoding')\nconst DECODER = Symbol('decoder')\nconst FLOWING = Symbol('flowing')\nconst PAUSED = Symbol('paused')\nconst RESUME = Symbol('resume')\nconst BUFFERLENGTH = Symbol('bufferLength')\nconst BUFFERPUSH = Symbol('bufferPush')\nconst BUFFERSHIFT = Symbol('bufferShift')\nconst OBJECTMODE = Symbol('objectMode')\nconst DESTROYED = Symbol('destroyed')\n\n// TODO remove when Node v8 support drops\nconst doIter = global._MP_NO_ITERATOR_SYMBOLS_  !== '1'\nconst ASYNCITERATOR = doIter && Symbol.asyncIterator\n  || Symbol('asyncIterator not implemented')\nconst ITERATOR = doIter && Symbol.iterator\n  || Symbol('iterator not implemented')\n\n// Buffer in node 4.x < 4.5.0 doesn't have working Buffer.from\n// or Buffer.alloc, and Buffer in node 10 deprecated the ctor.\n// .M, this is fine .\\^/M..\nconst B = Buffer.alloc ? Buffer\n  : /* istanbul ignore next */ __webpack_require__(/*! safe-buffer */ \"safe-buffer\").Buffer\n\n// events that mean 'the stream is over'\n// these are treated specially, and re-emitted\n// if they are listened for after emitting.\nconst isEndish = ev =>\n  ev === 'end' ||\n  ev === 'finish' ||\n  ev === 'prefinish'\n\nconst isArrayBuffer = b => b instanceof ArrayBuffer ||\n  typeof b === 'object' &&\n  b.constructor &&\n  b.constructor.name === 'ArrayBuffer' &&\n  b.byteLength >= 0\n\nconst isArrayBufferView = b => !B.isBuffer(b) && ArrayBuffer.isView(b)\n\nmodule.exports = class Minipass extends EE {\n  constructor (options) {\n    super()\n    this[FLOWING] = false\n    // whether we're explicitly paused\n    this[PAUSED] = false\n    this.pipes = new Yallist()\n    this.buffer = new Yallist()\n    this[OBJECTMODE] = options && options.objectMode || false\n    if (this[OBJECTMODE])\n      this[ENCODING] = null\n    else\n      this[ENCODING] = options && options.encoding || null\n    if (this[ENCODING] === 'buffer')\n      this[ENCODING] = null\n    this[DECODER] = this[ENCODING] ? new SD(this[ENCODING]) : null\n    this[EOF] = false\n    this[EMITTED_END] = false\n    this[EMITTING_END] = false\n    this[CLOSED] = false\n    this.writable = true\n    this.readable = true\n    this[BUFFERLENGTH] = 0\n    this[DESTROYED] = false\n  }\n\n  get bufferLength () { return this[BUFFERLENGTH] }\n\n  get encoding () { return this[ENCODING] }\n  set encoding (enc) {\n    if (this[OBJECTMODE])\n      throw new Error('cannot set encoding in objectMode')\n\n    if (this[ENCODING] && enc !== this[ENCODING] &&\n        (this[DECODER] && this[DECODER].lastNeed || this[BUFFERLENGTH]))\n      throw new Error('cannot change encoding')\n\n    if (this[ENCODING] !== enc) {\n      this[DECODER] = enc ? new SD(enc) : null\n      if (this.buffer.length)\n        this.buffer = this.buffer.map(chunk => this[DECODER].write(chunk))\n    }\n\n    this[ENCODING] = enc\n  }\n\n  setEncoding (enc) {\n    this.encoding = enc\n  }\n\n  get objectMode () { return this[OBJECTMODE] }\n  set objectMode (ॐ ) { this[OBJECTMODE] = this[OBJECTMODE] || !!ॐ  }\n\n  write (chunk, encoding, cb) {\n    if (this[EOF])\n      throw new Error('write after end')\n\n    if (this[DESTROYED]) {\n      this.emit('error', Object.assign(\n        new Error('Cannot call write after a stream was destroyed'),\n        { code: 'ERR_STREAM_DESTROYED' }\n      ))\n      return true\n    }\n\n    if (typeof encoding === 'function')\n      cb = encoding, encoding = 'utf8'\n\n    if (!encoding)\n      encoding = 'utf8'\n\n    // convert array buffers and typed array views into buffers\n    // at some point in the future, we may want to do the opposite!\n    // leave strings and buffers as-is\n    // anything else switches us into object mode\n    if (!this[OBJECTMODE] && !B.isBuffer(chunk)) {\n      if (isArrayBufferView(chunk))\n        chunk = B.from(chunk.buffer, chunk.byteOffset, chunk.byteLength)\n      else if (isArrayBuffer(chunk))\n        chunk = B.from(chunk)\n      else if (typeof chunk !== 'string')\n        // use the setter so we throw if we have encoding set\n        this.objectMode = true\n    }\n\n    // this ensures at this point that the chunk is a buffer or string\n    // don't buffer it up or send it to the decoder\n    if (!this.objectMode && !chunk.length) {\n      const ret = this.flowing\n      if (this[BUFFERLENGTH] !== 0)\n        this.emit('readable')\n      if (cb)\n        cb()\n      return ret\n    }\n\n    // fast-path writing strings of same encoding to a stream with\n    // an empty buffer, skipping the buffer/decoder dance\n    if (typeof chunk === 'string' && !this[OBJECTMODE] &&\n        // unless it is a string already ready for us to use\n        !(encoding === this[ENCODING] && !this[DECODER].lastNeed)) {\n      chunk = B.from(chunk, encoding)\n    }\n\n    if (B.isBuffer(chunk) && this[ENCODING])\n      chunk = this[DECODER].write(chunk)\n\n    try {\n      return this.flowing\n        ? (this.emit('data', chunk), this.flowing)\n        : (this[BUFFERPUSH](chunk), false)\n    } finally {\n      if (this[BUFFERLENGTH] !== 0)\n        this.emit('readable')\n      if (cb)\n        cb()\n    }\n  }\n\n  read (n) {\n    if (this[DESTROYED])\n      return null\n\n    try {\n      if (this[BUFFERLENGTH] === 0 || n === 0 || n > this[BUFFERLENGTH])\n        return null\n\n      if (this[OBJECTMODE])\n        n = null\n\n      if (this.buffer.length > 1 && !this[OBJECTMODE]) {\n        if (this.encoding)\n          this.buffer = new Yallist([\n            Array.from(this.buffer).join('')\n          ])\n        else\n          this.buffer = new Yallist([\n            B.concat(Array.from(this.buffer), this[BUFFERLENGTH])\n          ])\n      }\n\n      return this[READ](n || null, this.buffer.head.value)\n    } finally {\n      this[MAYBE_EMIT_END]()\n    }\n  }\n\n  [READ] (n, chunk) {\n    if (n === chunk.length || n === null)\n      this[BUFFERSHIFT]()\n    else {\n      this.buffer.head.value = chunk.slice(n)\n      chunk = chunk.slice(0, n)\n      this[BUFFERLENGTH] -= n\n    }\n\n    this.emit('data', chunk)\n\n    if (!this.buffer.length && !this[EOF])\n      this.emit('drain')\n\n    return chunk\n  }\n\n  end (chunk, encoding, cb) {\n    if (typeof chunk === 'function')\n      cb = chunk, chunk = null\n    if (typeof encoding === 'function')\n      cb = encoding, encoding = 'utf8'\n    if (chunk)\n      this.write(chunk, encoding)\n    if (cb)\n      this.once('end', cb)\n    this[EOF] = true\n    this.writable = false\n\n    // if we haven't written anything, then go ahead and emit,\n    // even if we're not reading.\n    // we'll re-emit if a new 'end' listener is added anyway.\n    // This makes MP more suitable to write-only use cases.\n    if (this.flowing || !this[PAUSED])\n      this[MAYBE_EMIT_END]()\n    return this\n  }\n\n  // don't let the internal resume be overwritten\n  [RESUME] () {\n    if (this[DESTROYED])\n      return\n\n    this[PAUSED] = false\n    this[FLOWING] = true\n    this.emit('resume')\n    if (this.buffer.length)\n      this[FLUSH]()\n    else if (this[EOF])\n      this[MAYBE_EMIT_END]()\n    else\n      this.emit('drain')\n  }\n\n  resume () {\n    return this[RESUME]()\n  }\n\n  pause () {\n    this[FLOWING] = false\n    this[PAUSED] = true\n  }\n\n  get destroyed () {\n    return this[DESTROYED]\n  }\n\n  get flowing () {\n    return this[FLOWING]\n  }\n\n  get paused () {\n    return this[PAUSED]\n  }\n\n  [BUFFERPUSH] (chunk) {\n    if (this[OBJECTMODE])\n      this[BUFFERLENGTH] += 1\n    else\n      this[BUFFERLENGTH] += chunk.length\n    return this.buffer.push(chunk)\n  }\n\n  [BUFFERSHIFT] () {\n    if (this.buffer.length) {\n      if (this[OBJECTMODE])\n        this[BUFFERLENGTH] -= 1\n      else\n        this[BUFFERLENGTH] -= this.buffer.head.value.length\n    }\n    return this.buffer.shift()\n  }\n\n  [FLUSH] () {\n    do {} while (this[FLUSHCHUNK](this[BUFFERSHIFT]()))\n\n    if (!this.buffer.length && !this[EOF])\n      this.emit('drain')\n  }\n\n  [FLUSHCHUNK] (chunk) {\n    return chunk ? (this.emit('data', chunk), this.flowing) : false\n  }\n\n  pipe (dest, opts) {\n    if (this[DESTROYED])\n      return\n\n    const ended = this[EMITTED_END]\n    opts = opts || {}\n    if (dest === process.stdout || dest === process.stderr)\n      opts.end = false\n    else\n      opts.end = opts.end !== false\n\n    const p = { dest: dest, opts: opts, ondrain: _ => this[RESUME]() }\n    this.pipes.push(p)\n\n    dest.on('drain', p.ondrain)\n    this[RESUME]()\n    // piping an ended stream ends immediately\n    if (ended && p.opts.end)\n      p.dest.end()\n    return dest\n  }\n\n  addListener (ev, fn) {\n    return this.on(ev, fn)\n  }\n\n  on (ev, fn) {\n    try {\n      return super.on(ev, fn)\n    } finally {\n      if (ev === 'data' && !this.pipes.length && !this.flowing)\n        this[RESUME]()\n      else if (isEndish(ev) && this[EMITTED_END]) {\n        super.emit(ev)\n        this.removeAllListeners(ev)\n      }\n    }\n  }\n\n  get emittedEnd () {\n    return this[EMITTED_END]\n  }\n\n  [MAYBE_EMIT_END] () {\n    if (!this[EMITTING_END] &&\n        !this[EMITTED_END] &&\n        !this[DESTROYED] &&\n        this.buffer.length === 0 &&\n        this[EOF]) {\n      this[EMITTING_END] = true\n      this.emit('end')\n      this.emit('prefinish')\n      this.emit('finish')\n      if (this[CLOSED])\n        this.emit('close')\n      this[EMITTING_END] = false\n    }\n  }\n\n  emit (ev, data) {\n    // error and close are only events allowed after calling destroy()\n    if (ev !== 'error' && ev !== 'close' && ev !== DESTROYED && this[DESTROYED])\n      return\n    else if (ev === 'data') {\n      if (!data)\n        return\n\n      if (this.pipes.length)\n        this.pipes.forEach(p =>\n          p.dest.write(data) === false && this.pause())\n    } else if (ev === 'end') {\n      // only actual end gets this treatment\n      if (this[EMITTED_END] === true)\n        return\n\n      this[EMITTED_END] = true\n      this.readable = false\n\n      if (this[DECODER]) {\n        data = this[DECODER].end()\n        if (data) {\n          this.pipes.forEach(p => p.dest.write(data))\n          super.emit('data', data)\n        }\n      }\n\n      this.pipes.forEach(p => {\n        p.dest.removeListener('drain', p.ondrain)\n        if (p.opts.end)\n          p.dest.end()\n      })\n    } else if (ev === 'close') {\n      this[CLOSED] = true\n      // don't emit close before 'end' and 'finish'\n      if (!this[EMITTED_END] && !this[DESTROYED])\n        return\n    }\n\n    // TODO: replace with a spread operator when Node v4 support drops\n    const args = new Array(arguments.length)\n    args[0] = ev\n    args[1] = data\n    if (arguments.length > 2) {\n      for (let i = 2; i < arguments.length; i++) {\n        args[i] = arguments[i]\n      }\n    }\n\n    try {\n      return super.emit.apply(this, args)\n    } finally {\n      if (!isEndish(ev))\n        this[MAYBE_EMIT_END]()\n      else\n        this.removeAllListeners(ev)\n    }\n  }\n\n  // const all = await stream.collect()\n  collect () {\n    const buf = []\n    buf.dataLength = 0\n    this.on('data', c => {\n      buf.push(c)\n      buf.dataLength += c.length\n    })\n    return this.promise().then(() => buf)\n  }\n\n  // const data = await stream.concat()\n  concat () {\n    return this[OBJECTMODE]\n      ? Promise.reject(new Error('cannot concat in objectMode'))\n      : this.collect().then(buf =>\n          this[OBJECTMODE]\n            ? Promise.reject(new Error('cannot concat in objectMode'))\n            : this[ENCODING] ? buf.join('') : B.concat(buf, buf.dataLength))\n  }\n\n  // stream.promise().then(() => done, er => emitted error)\n  promise () {\n    return new Promise((resolve, reject) => {\n      this.on(DESTROYED, () => reject(new Error('stream destroyed')))\n      this.on('end', () => resolve())\n      this.on('error', er => reject(er))\n    })\n  }\n\n  // for await (let chunk of stream)\n  [ASYNCITERATOR] () {\n    const next = () => {\n      const res = this.read()\n      if (res !== null)\n        return Promise.resolve({ done: false, value: res })\n\n      if (this[EOF])\n        return Promise.resolve({ done: true })\n\n      let resolve = null\n      let reject = null\n      const onerr = er => {\n        this.removeListener('data', ondata)\n        this.removeListener('end', onend)\n        reject(er)\n      }\n      const ondata = value => {\n        this.removeListener('error', onerr)\n        this.removeListener('end', onend)\n        this.pause()\n        resolve({ value: value, done: !!this[EOF] })\n      }\n      const onend = () => {\n        this.removeListener('error', onerr)\n        this.removeListener('data', ondata)\n        resolve({ done: true })\n      }\n      const ondestroy = () => onerr(new Error('stream destroyed'))\n      return new Promise((res, rej) => {\n        reject = rej\n        resolve = res\n        this.once(DESTROYED, ondestroy)\n        this.once('error', onerr)\n        this.once('end', onend)\n        this.once('data', ondata)\n      })\n    }\n\n    return { next }\n  }\n\n  // for (let chunk of stream)\n  [ITERATOR] () {\n    const next = () => {\n      const value = this.read()\n      const done = value === null\n      return { value, done }\n    }\n    return { next }\n  }\n\n  destroy (er) {\n    if (this[DESTROYED]) {\n      if (er)\n        this.emit('error', er)\n      else\n        this.emit(DESTROYED)\n      return this\n    }\n\n    this[DESTROYED] = true\n\n    // throw away all buffered data, it's never coming out\n    this.buffer = new Yallist()\n    this[BUFFERLENGTH] = 0\n\n    if (typeof this.close === 'function' && !this[CLOSED])\n      this.close()\n\n    if (er)\n      this.emit('error', er)\n    else // if no error to emit, still reject pending promises\n      this.emit(DESTROYED)\n\n    return this\n  }\n\n  static isStream (s) {\n    return !!s && (s instanceof Minipass || s instanceof EE && (\n      typeof s.pipe === 'function' || // readable\n      (typeof s.write === 'function' && typeof s.end === 'function') // writable\n    ))\n  }\n}\n\n\n//# sourceURL=webpack:///./node_modules/minipass/index.js?");

/***/ }),

/***/ "./node_modules/minizlib/constants.js":
/*!********************************************!*\
  !*** ./node_modules/minizlib/constants.js ***!
  \********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("// Update with any zlib constants that are added or changed in the future.\n// Node v6 didn't export this, so we just hard code the version and rely\n// on all the other hard-coded values from zlib v4736.  When node v6\n// support drops, we can just export the realZlibConstants object.\nconst realZlibConstants = __webpack_require__(/*! zlib */ \"zlib\").constants ||\n  /* istanbul ignore next */ { ZLIB_VERNUM: 4736 }\n\nmodule.exports = Object.freeze(Object.assign(Object.create(null), {\n  Z_NO_FLUSH: 0,\n  Z_PARTIAL_FLUSH: 1,\n  Z_SYNC_FLUSH: 2,\n  Z_FULL_FLUSH: 3,\n  Z_FINISH: 4,\n  Z_BLOCK: 5,\n  Z_OK: 0,\n  Z_STREAM_END: 1,\n  Z_NEED_DICT: 2,\n  Z_ERRNO: -1,\n  Z_STREAM_ERROR: -2,\n  Z_DATA_ERROR: -3,\n  Z_MEM_ERROR: -4,\n  Z_BUF_ERROR: -5,\n  Z_VERSION_ERROR: -6,\n  Z_NO_COMPRESSION: 0,\n  Z_BEST_SPEED: 1,\n  Z_BEST_COMPRESSION: 9,\n  Z_DEFAULT_COMPRESSION: -1,\n  Z_FILTERED: 1,\n  Z_HUFFMAN_ONLY: 2,\n  Z_RLE: 3,\n  Z_FIXED: 4,\n  Z_DEFAULT_STRATEGY: 0,\n  DEFLATE: 1,\n  INFLATE: 2,\n  GZIP: 3,\n  GUNZIP: 4,\n  DEFLATERAW: 5,\n  INFLATERAW: 6,\n  UNZIP: 7,\n  BROTLI_DECODE: 8,\n  BROTLI_ENCODE: 9,\n  Z_MIN_WINDOWBITS: 8,\n  Z_MAX_WINDOWBITS: 15,\n  Z_DEFAULT_WINDOWBITS: 15,\n  Z_MIN_CHUNK: 64,\n  Z_MAX_CHUNK: Infinity,\n  Z_DEFAULT_CHUNK: 16384,\n  Z_MIN_MEMLEVEL: 1,\n  Z_MAX_MEMLEVEL: 9,\n  Z_DEFAULT_MEMLEVEL: 8,\n  Z_MIN_LEVEL: -1,\n  Z_MAX_LEVEL: 9,\n  Z_DEFAULT_LEVEL: -1,\n  BROTLI_OPERATION_PROCESS: 0,\n  BROTLI_OPERATION_FLUSH: 1,\n  BROTLI_OPERATION_FINISH: 2,\n  BROTLI_OPERATION_EMIT_METADATA: 3,\n  BROTLI_MODE_GENERIC: 0,\n  BROTLI_MODE_TEXT: 1,\n  BROTLI_MODE_FONT: 2,\n  BROTLI_DEFAULT_MODE: 0,\n  BROTLI_MIN_QUALITY: 0,\n  BROTLI_MAX_QUALITY: 11,\n  BROTLI_DEFAULT_QUALITY: 11,\n  BROTLI_MIN_WINDOW_BITS: 10,\n  BROTLI_MAX_WINDOW_BITS: 24,\n  BROTLI_LARGE_MAX_WINDOW_BITS: 30,\n  BROTLI_DEFAULT_WINDOW: 22,\n  BROTLI_MIN_INPUT_BLOCK_BITS: 16,\n  BROTLI_MAX_INPUT_BLOCK_BITS: 24,\n  BROTLI_PARAM_MODE: 0,\n  BROTLI_PARAM_QUALITY: 1,\n  BROTLI_PARAM_LGWIN: 2,\n  BROTLI_PARAM_LGBLOCK: 3,\n  BROTLI_PARAM_DISABLE_LITERAL_CONTEXT_MODELING: 4,\n  BROTLI_PARAM_SIZE_HINT: 5,\n  BROTLI_PARAM_LARGE_WINDOW: 6,\n  BROTLI_PARAM_NPOSTFIX: 7,\n  BROTLI_PARAM_NDIRECT: 8,\n  BROTLI_DECODER_RESULT_ERROR: 0,\n  BROTLI_DECODER_RESULT_SUCCESS: 1,\n  BROTLI_DECODER_RESULT_NEEDS_MORE_INPUT: 2,\n  BROTLI_DECODER_RESULT_NEEDS_MORE_OUTPUT: 3,\n  BROTLI_DECODER_PARAM_DISABLE_RING_BUFFER_REALLOCATION: 0,\n  BROTLI_DECODER_PARAM_LARGE_WINDOW: 1,\n  BROTLI_DECODER_NO_ERROR: 0,\n  BROTLI_DECODER_SUCCESS: 1,\n  BROTLI_DECODER_NEEDS_MORE_INPUT: 2,\n  BROTLI_DECODER_NEEDS_MORE_OUTPUT: 3,\n  BROTLI_DECODER_ERROR_FORMAT_EXUBERANT_NIBBLE: -1,\n  BROTLI_DECODER_ERROR_FORMAT_RESERVED: -2,\n  BROTLI_DECODER_ERROR_FORMAT_EXUBERANT_META_NIBBLE: -3,\n  BROTLI_DECODER_ERROR_FORMAT_SIMPLE_HUFFMAN_ALPHABET: -4,\n  BROTLI_DECODER_ERROR_FORMAT_SIMPLE_HUFFMAN_SAME: -5,\n  BROTLI_DECODER_ERROR_FORMAT_CL_SPACE: -6,\n  BROTLI_DECODER_ERROR_FORMAT_HUFFMAN_SPACE: -7,\n  BROTLI_DECODER_ERROR_FORMAT_CONTEXT_MAP_REPEAT: -8,\n  BROTLI_DECODER_ERROR_FORMAT_BLOCK_LENGTH_1: -9,\n  BROTLI_DECODER_ERROR_FORMAT_BLOCK_LENGTH_2: -10,\n  BROTLI_DECODER_ERROR_FORMAT_TRANSFORM: -11,\n  BROTLI_DECODER_ERROR_FORMAT_DICTIONARY: -12,\n  BROTLI_DECODER_ERROR_FORMAT_WINDOW_BITS: -13,\n  BROTLI_DECODER_ERROR_FORMAT_PADDING_1: -14,\n  BROTLI_DECODER_ERROR_FORMAT_PADDING_2: -15,\n  BROTLI_DECODER_ERROR_FORMAT_DISTANCE: -16,\n  BROTLI_DECODER_ERROR_DICTIONARY_NOT_SET: -19,\n  BROTLI_DECODER_ERROR_INVALID_ARGUMENTS: -20,\n  BROTLI_DECODER_ERROR_ALLOC_CONTEXT_MODES: -21,\n  BROTLI_DECODER_ERROR_ALLOC_TREE_GROUPS: -22,\n  BROTLI_DECODER_ERROR_ALLOC_CONTEXT_MAP: -25,\n  BROTLI_DECODER_ERROR_ALLOC_RING_BUFFER_1: -26,\n  BROTLI_DECODER_ERROR_ALLOC_RING_BUFFER_2: -27,\n  BROTLI_DECODER_ERROR_ALLOC_BLOCK_TYPE_TREES: -30,\n  BROTLI_DECODER_ERROR_UNREACHABLE: -31,\n}, realZlibConstants))\n\n\n//# sourceURL=webpack:///./node_modules/minizlib/constants.js?");

/***/ }),

/***/ "./node_modules/minizlib/index.js":
/*!****************************************!*\
  !*** ./node_modules/minizlib/index.js ***!
  \****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nconst assert = __webpack_require__(/*! assert */ \"assert\")\nconst Buffer = __webpack_require__(/*! buffer */ \"buffer\").Buffer\nconst realZlib = __webpack_require__(/*! zlib */ \"zlib\")\n\nconst constants = exports.constants = __webpack_require__(/*! ./constants.js */ \"./node_modules/minizlib/constants.js\")\nconst Minipass = __webpack_require__(/*! minipass */ \"./node_modules/minipass/index.js\")\n\nconst OriginalBufferConcat = Buffer.concat\n\nclass ZlibError extends Error {\n  constructor (err) {\n    super('zlib: ' + err.message)\n    this.code = err.code\n    this.errno = err.errno\n    /* istanbul ignore if */\n    if (!this.code)\n      this.code = 'ZLIB_ERROR'\n\n    this.message = 'zlib: ' + err.message\n    Error.captureStackTrace(this, this.constructor)\n  }\n\n  get name () {\n    return 'ZlibError'\n  }\n}\n\n// the Zlib class they all inherit from\n// This thing manages the queue of requests, and returns\n// true or false if there is anything in the queue when\n// you call the .write() method.\nconst _opts = Symbol('opts')\nconst _flushFlag = Symbol('flushFlag')\nconst _finishFlushFlag = Symbol('finishFlushFlag')\nconst _fullFlushFlag = Symbol('fullFlushFlag')\nconst _handle = Symbol('handle')\nconst _onError = Symbol('onError')\nconst _sawError = Symbol('sawError')\nconst _level = Symbol('level')\nconst _strategy = Symbol('strategy')\nconst _ended = Symbol('ended')\nconst _defaultFullFlush = Symbol('_defaultFullFlush')\n\nclass ZlibBase extends Minipass {\n  constructor (opts, mode) {\n    if (!opts || typeof opts !== 'object')\n      throw new TypeError('invalid options for ZlibBase constructor')\n\n    super(opts)\n    this[_ended] = false\n    this[_opts] = opts\n\n    this[_flushFlag] = opts.flush\n    this[_finishFlushFlag] = opts.finishFlush\n    // this will throw if any options are invalid for the class selected\n    try {\n      this[_handle] = new realZlib[mode](opts)\n    } catch (er) {\n      // make sure that all errors get decorated properly\n      throw new ZlibError(er)\n    }\n\n    this[_onError] = (err) => {\n      this[_sawError] = true\n      // there is no way to cleanly recover.\n      // continuing only obscures problems.\n      this.close()\n      this.emit('error', err)\n    }\n\n    this[_handle].on('error', er => this[_onError](new ZlibError(er)))\n    this.once('end', () => this.close)\n  }\n\n  close () {\n    if (this[_handle]) {\n      this[_handle].close()\n      this[_handle] = null\n      this.emit('close')\n    }\n  }\n\n  reset () {\n    if (!this[_sawError]) {\n      assert(this[_handle], 'zlib binding closed')\n      return this[_handle].reset()\n    }\n  }\n\n  flush (flushFlag) {\n    if (this.ended)\n      return\n\n    if (typeof flushFlag !== 'number')\n      flushFlag = this[_fullFlushFlag]\n    this.write(Object.assign(Buffer.alloc(0), { [_flushFlag]: flushFlag }))\n  }\n\n  end (chunk, encoding, cb) {\n    if (chunk)\n      this.write(chunk, encoding)\n    this.flush(this[_finishFlushFlag])\n    this[_ended] = true\n    return super.end(null, null, cb)\n  }\n\n  get ended () {\n    return this[_ended]\n  }\n\n  write (chunk, encoding, cb) {\n    // process the chunk using the sync process\n    // then super.write() all the outputted chunks\n    if (typeof encoding === 'function')\n      cb = encoding, encoding = 'utf8'\n\n    if (typeof chunk === 'string')\n      chunk = Buffer.from(chunk, encoding)\n\n    if (this[_sawError])\n      return\n    assert(this[_handle], 'zlib binding closed')\n\n    // _processChunk tries to .close() the native handle after it's done, so we\n    // intercept that by temporarily making it a no-op.\n    const nativeHandle = this[_handle]._handle\n    const originalNativeClose = nativeHandle.close\n    nativeHandle.close = () => {}\n    const originalClose = this[_handle].close\n    this[_handle].close = () => {}\n    // It also calls `Buffer.concat()` at the end, which may be convenient\n    // for some, but which we are not interested in as it slows us down.\n    Buffer.concat = (args) => args\n    let result\n    try {\n      const flushFlag = typeof chunk[_flushFlag] === 'number'\n        ? chunk[_flushFlag] : this[_flushFlag]\n      result = this[_handle]._processChunk(chunk, flushFlag)\n      // if we don't throw, reset it back how it was\n      Buffer.concat = OriginalBufferConcat\n    } catch (err) {\n      // or if we do, put Buffer.concat() back before we emit error\n      // Error events call into user code, which may call Buffer.concat()\n      Buffer.concat = OriginalBufferConcat\n      this[_onError](new ZlibError(err))\n    } finally {\n      if (this[_handle]) {\n        // Core zlib resets `_handle` to null after attempting to close the\n        // native handle. Our no-op handler prevented actual closure, but we\n        // need to restore the `._handle` property.\n        this[_handle]._handle = nativeHandle\n        nativeHandle.close = originalNativeClose\n        this[_handle].close = originalClose\n        // `_processChunk()` adds an 'error' listener. If we don't remove it\n        // after each call, these handlers start piling up.\n        this[_handle].removeAllListeners('error')\n      }\n    }\n\n    let writeReturn\n    if (result) {\n      if (Array.isArray(result) && result.length > 0) {\n        // The first buffer is always `handle._outBuffer`, which would be\n        // re-used for later invocations; so, we always have to copy that one.\n        writeReturn = super.write(Buffer.from(result[0]))\n        for (let i = 1; i < result.length; i++) {\n          writeReturn = super.write(result[i])\n        }\n      } else {\n        writeReturn = super.write(Buffer.from(result))\n      }\n    }\n\n    if (cb)\n      cb()\n    return writeReturn\n  }\n}\n\nclass Zlib extends ZlibBase {\n  constructor (opts, mode) {\n    opts = opts || {}\n\n    opts.flush = opts.flush || constants.Z_NO_FLUSH\n    opts.finishFlush = opts.finishFlush || constants.Z_FINISH\n    super(opts, mode)\n\n    this[_fullFlushFlag] = constants.Z_FULL_FLUSH\n    this[_level] = opts.level\n    this[_strategy] = opts.strategy\n  }\n\n  params (level, strategy) {\n    if (this[_sawError])\n      return\n\n    if (!this[_handle])\n      throw new Error('cannot switch params when binding is closed')\n\n    // no way to test this without also not supporting params at all\n    /* istanbul ignore if */\n    if (!this[_handle].params)\n      throw new Error('not supported in this implementation')\n\n    if (this[_level] !== level || this[_strategy] !== strategy) {\n      this.flush(constants.Z_SYNC_FLUSH)\n      assert(this[_handle], 'zlib binding closed')\n      // .params() calls .flush(), but the latter is always async in the\n      // core zlib. We override .flush() temporarily to intercept that and\n      // flush synchronously.\n      const origFlush = this[_handle].flush\n      this[_handle].flush = (flushFlag, cb) => {\n        this.flush(flushFlag)\n        cb()\n      }\n      try {\n        this[_handle].params(level, strategy)\n      } finally {\n        this[_handle].flush = origFlush\n      }\n      /* istanbul ignore else */\n      if (this[_handle]) {\n        this[_level] = level\n        this[_strategy] = strategy\n      }\n    }\n  }\n}\n\n// minimal 2-byte header\nclass Deflate extends Zlib {\n  constructor (opts) {\n    super(opts, 'Deflate')\n  }\n}\n\nclass Inflate extends Zlib {\n  constructor (opts) {\n    super(opts, 'Inflate')\n  }\n}\n\n// gzip - bigger header, same deflate compression\nclass Gzip extends Zlib {\n  constructor (opts) {\n    super(opts, 'Gzip')\n  }\n}\n\nclass Gunzip extends Zlib {\n  constructor (opts) {\n    super(opts, 'Gunzip')\n  }\n}\n\n// raw - no header\nclass DeflateRaw extends Zlib {\n  constructor (opts) {\n    super(opts, 'DeflateRaw')\n  }\n}\n\nclass InflateRaw extends Zlib {\n  constructor (opts) {\n    super(opts, 'InflateRaw')\n  }\n}\n\n// auto-detect header.\nclass Unzip extends Zlib {\n  constructor (opts) {\n    super(opts, 'Unzip')\n  }\n}\n\nclass Brotli extends ZlibBase {\n  constructor (opts, mode) {\n    opts = opts || {}\n\n    opts.flush = opts.flush || constants.BROTLI_OPERATION_PROCESS\n    opts.finishFlush = opts.finishFlush || constants.BROTLI_OPERATION_FINISH\n\n    super(opts, mode)\n\n    this[_fullFlushFlag] = constants.BROTLI_OPERATION_FLUSH\n  }\n}\n\nclass BrotliCompress extends Brotli {\n  constructor (opts) {\n    super(opts, 'BrotliCompress')\n  }\n}\n\nclass BrotliDecompress extends Brotli {\n  constructor (opts) {\n    super(opts, 'BrotliDecompress')\n  }\n}\n\nexports.Deflate = Deflate\nexports.Inflate = Inflate\nexports.Gzip = Gzip\nexports.Gunzip = Gunzip\nexports.DeflateRaw = DeflateRaw\nexports.InflateRaw = InflateRaw\nexports.Unzip = Unzip\n/* istanbul ignore else */\nif (typeof realZlib.BrotliCompress === 'function') {\n  exports.BrotliCompress = BrotliCompress\n  exports.BrotliDecompress = BrotliDecompress\n} else {\n  exports.BrotliCompress = exports.BrotliDecompress = class {\n    constructor () {\n      throw new Error('Brotli is not supported in this version of Node.js')\n    }\n  }\n}\n\n\n//# sourceURL=webpack:///./node_modules/minizlib/index.js?");

/***/ }),

/***/ "./node_modules/needle/lib/auth.js":
/*!*****************************************!*\
  !*** ./node_modules/needle/lib/auth.js ***!
  \*****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("var createHash = __webpack_require__(/*! crypto */ \"crypto\").createHash;\n\nfunction get_header(header, credentials, opts) {\n  var type = header.split(' ')[0],\n      user = credentials[0],\n      pass = credentials[1];\n\n  if (type == 'Digest') {\n    return digest.generate(header, user, pass, opts.method, opts.path);\n  } else if (type == 'Basic') {\n    return basic(user, pass);\n  }\n}\n\n////////////////////\n// basic\n\nfunction md5(string) {\n  return createHash('md5').update(string).digest('hex');\n}\n\nfunction basic(user, pass) {\n  var str  = typeof pass == 'undefined' ? user : [user, pass].join(':');\n  return 'Basic ' + Buffer.from(str).toString('base64');\n}\n\n////////////////////\n// digest\n// logic inspired from https://github.com/simme/node-http-digest-client\n\nvar digest = {};\n\ndigest.parse_header = function(header) {\n  var challenge = {},\n      matches   = header.match(/([a-z0-9_-]+)=\"?([a-z0-9=\\/\\.@\\s-\\+)()]+)\"?/gi);\n\n  for (var i = 0, l = matches.length; i < l; i++) {\n    var parts = matches[i].split('='),\n        key   = parts.shift(),\n        val   = parts.join('=').replace(/^\"/, '').replace(/\"$/, '');\n\n    challenge[key] = val;\n  }\n\n  return challenge;\n}\n\ndigest.update_nc = function(nc) {\n  var max = 99999999;\n  nc++;\n\n  if (nc > max)\n    nc = 1;\n\n  var padding = new Array(8).join('0') + '';\n  nc = nc + '';\n  return padding.substr(0, 8 - nc.length) + nc;\n}\n\ndigest.generate = function(header, user, pass, method, path) {\n\n  var nc        = 1,\n      cnonce    = null,\n      challenge = digest.parse_header(header);\n\n  var ha1  = md5(user + ':' + challenge.realm + ':' + pass),\n      ha2  = md5(method.toUpperCase() + ':' + path),\n      resp = [ha1, challenge.nonce];\n\n  if (typeof challenge.qop === 'string') {\n    cnonce = md5(Math.random().toString(36)).substr(0, 8);\n    nc     = digest.update_nc(nc);\n    resp   = resp.concat(nc, cnonce);\n    resp   = resp.concat(challenge.qop, ha2);\n  } else {\n    resp   = resp.concat(ha2);\n  }\n\n\n  var params = {\n    uri      : path,\n    realm    : challenge.realm,\n    nonce    : challenge.nonce,\n    username : user,\n    response : md5(resp.join(':'))\n  }\n\n  if (challenge.qop) {\n    params.qop = challenge.qop;\n  }\n\n  if (challenge.opaque) {\n    params.opaque = challenge.opaque;\n  }\n\n  if (cnonce) {\n    params.nc = nc;\n    params.cnonce = cnonce;\n  }\n\n  header = []\n  for (var k in params)\n    header.push(k + '=\"' + params[k] + '\"')\n\n  return 'Digest ' + header.join(', ');\n}\n\nmodule.exports = {\n  header : get_header,\n  basic  : basic,\n  digest : digest.generate\n}\n\n\n//# sourceURL=webpack:///./node_modules/needle/lib/auth.js?");

/***/ }),

/***/ "./node_modules/needle/lib/cookies.js":
/*!********************************************!*\
  !*** ./node_modules/needle/lib/cookies.js ***!
  \********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("\n//  Simple cookie handling implementation based on the standard RFC 6265.\n//\n//  This module just has two functionalities:\n//    - Parse a set-cookie-header as a key value object\n//    - Write a cookie-string from a key value object\n//\n//  All cookie attributes are ignored.\n\nvar unescape = __webpack_require__(/*! querystring */ \"querystring\").unescape;\n\nvar COOKIE_PAIR        = /^([^=\\s]+)\\s*=\\s*(\"?)\\s*(.*)\\s*\\2\\s*$/;\nvar EXCLUDED_CHARS     = /[\\x00-\\x1F\\x7F\\x3B\\x3B\\s\\\"\\,\\\\\"%]/g;\nvar TRAILING_SEMICOLON = /\\x3B+$/;\nvar SEP_SEMICOLON      = /\\s*\\x3B\\s*/;\n\n// i know these should be 'const', but I'd like to keep\n// supporting earlier node.js versions as long as I can. :)\n\nvar KEY_INDEX   = 1; // index of key from COOKIE_PAIR match\nvar VALUE_INDEX = 3; // index of value from COOKIE_PAIR match\n\n// Returns a copy str trimmed and without trainling semicolon.\nfunction cleanCookieString(str) {\n  return str.trim().replace(/\\x3B+$/, '');\n}\n\nfunction getFirstPair(str) {\n  var index = str.indexOf('\\x3B');\n  return index === -1 ? str : str.substr(0, index);\n}\n\n// Returns a encoded copy of str based on RFC6265 S4.1.1.\nfunction encodeCookieComponent(str) {\n  return str.toString().replace(EXCLUDED_CHARS, encodeURIComponent);\n}\n\n// Parses a set-cookie-string based on the standard defined in RFC6265 S4.1.1.\nfunction parseSetCookieString(str) {\n  str = cleanCookieString(str);\n  str = getFirstPair(str);\n\n  var res = COOKIE_PAIR.exec(str);\n  if (!res || !res[VALUE_INDEX]) return null;\n\n  return {\n    name  : unescape(res[KEY_INDEX]),\n    value : unescape(res[VALUE_INDEX])\n  };\n}\n\n// Parses a set-cookie-header and returns a key/value object.\n// Each key represents the name of a cookie.\nfunction parseSetCookieHeader(header) {\n  if (!header) return {};\n  header = Array.isArray(header) ? header : [header];\n\n  return header.reduce(function(res, str) {\n    var cookie = parseSetCookieString(str);\n    if (cookie) res[cookie.name] = cookie.value;\n    return res;\n  }, {});\n}\n\n// Writes a set-cookie-string based on the standard definded in RFC6265 S4.1.1.\nfunction writeCookieString(obj) {\n  return Object.keys(obj).reduce(function(str, name) {\n    var encodedName  = encodeCookieComponent(name);\n    var encodedValue = encodeCookieComponent(obj[name]);\n    str += (str ? '; ' : '') + encodedName + '=' + encodedValue;\n    return str;\n  }, '');\n}\n\n// returns a key/val object from an array of cookie strings\nexports.read = parseSetCookieHeader;\n\n// writes a cookie string header\nexports.write = writeCookieString;\n\n\n//# sourceURL=webpack:///./node_modules/needle/lib/cookies.js?");

/***/ }),

/***/ "./node_modules/needle/lib/decoder.js":
/*!********************************************!*\
  !*** ./node_modules/needle/lib/decoder.js ***!
  \********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("var iconv,\n    inherits  = __webpack_require__(/*! util */ \"util\").inherits,\n    stream    = __webpack_require__(/*! stream */ \"stream\");\n\nvar regex = /(?:charset|encoding)\\s*=\\s*['\"]? *([\\w\\-]+)/i;\n\ninherits(StreamDecoder, stream.Transform);\n\nfunction StreamDecoder(charset) {\n  if (!(this instanceof StreamDecoder))\n    return new StreamDecoder(charset);\n\n  stream.Transform.call(this, charset);\n  this.charset = charset;\n  this.parsed_chunk = false;\n}\n\nStreamDecoder.prototype._transform = function(chunk, encoding, done) {\n  var res, found;\n\n  // try get charset from chunk, just once\n  if (this.charset == 'utf8' && !this.parsed_chunk) {\n    this.parsed_chunk = true;\n\n    var matches = regex.exec(chunk.toString());\n    if (matches) {\n      found = matches[1].toLowerCase();\n      this.charset = found == 'utf-8' ? 'utf8' : found;\n    }\n  }\n\n  try {\n    res = iconv.decode(chunk, this.charset);\n  } catch(e) { // something went wrong, just return original chunk\n    res = chunk;\n  }\n\n  this.push(res);\n  done();\n}\n\nmodule.exports = function(charset) {\n  try {\n    if (!iconv) iconv = __webpack_require__(/*! iconv-lite */ \"iconv-lite\");\n  } catch(e) {\n    /* iconv not found */\n  }\n\n  if (iconv)\n    return new StreamDecoder(charset);\n  else\n    return new stream.PassThrough;\n}\n\n\n//# sourceURL=webpack:///./node_modules/needle/lib/decoder.js?");

/***/ }),

/***/ "./node_modules/needle/lib/multipart.js":
/*!**********************************************!*\
  !*** ./node_modules/needle/lib/multipart.js ***!
  \**********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("var readFile = __webpack_require__(/*! fs */ \"fs\").readFile,\n    basename = __webpack_require__(/*! path */ \"path\").basename;\n\nexports.build = function(data, boundary, callback) {\n\n  if (typeof data != 'object' || typeof data.pipe == 'function')\n    return callback(new Error('Multipart builder expects data as key/val object.'));\n\n  var body   = '',\n      object = flatten(data),\n      count  = Object.keys(object).length;\n\n  if (count === 0)\n    return callback(new Error('Empty multipart body. Invalid data.'))\n\n  function done(err, section) {\n    if (err) return callback(err);\n    if (section) body += section;\n    --count || callback(null, body + '--' + boundary + '--');\n  };\n\n  for (var key in object) {\n    var value = object[key];\n    if (value === null || typeof value == 'undefined') {\n      done();\n    } else if (Buffer.isBuffer(value)) {\n      var part = { buffer: value, content_type: 'application/octet-stream' };\n      generate_part(key, part, boundary, done);\n    } else {\n      var part = (value.buffer || value.file || value.content_type) ? value : { value: value };\n      generate_part(key, part, boundary, done);\n    }\n  }\n\n}\n\nfunction generate_part(name, part, boundary, callback) {\n\n  var return_part = '--' + boundary + '\\r\\n';\n  return_part += 'Content-Disposition: form-data; name=\"' + name + '\"';\n\n  function append(data, filename) {\n\n    if (data) {\n      var binary = part.content_type.indexOf('text') == -1;\n      return_part += '; filename=\"' + encodeURIComponent(filename) + '\"\\r\\n';\n      if (binary) return_part += 'Content-Transfer-Encoding: binary\\r\\n';\n      return_part += 'Content-Type: ' + part.content_type + '\\r\\n\\r\\n';\n      return_part += binary ? data.toString('binary') : data.toString('utf8');\n    }\n\n    callback(null, return_part + '\\r\\n');\n  };\n\n  if ((part.file || part.buffer) && part.content_type) {\n\n    var filename = part.filename ? part.filename : part.file ? basename(part.file) : name;\n    if (part.buffer) return append(part.buffer, filename);\n\n    readFile(part.file, function(err, data) {\n      if (err) return callback(err);\n      append(data, filename);\n    });\n\n  } else {\n\n    if (typeof part.value == 'object')\n      return callback(new Error('Object received for ' + name + ', expected string.'))\n\n    if (part.content_type) {\n      return_part += '\\r\\n';\n      return_part += 'Content-Type: ' + part.content_type;\n    }\n\n    return_part += '\\r\\n\\r\\n';\n    return_part += Buffer.from(String(part.value), 'utf8').toString('binary');\n    append();\n\n  }\n\n}\n\n// flattens nested objects for multipart body\nfunction flatten(object, into, prefix) {\n  into = into || {};\n\n  for(var key in object) {\n    var prefix_key = prefix ? prefix + '[' + key + ']' : key;\n    var prop = object[key];\n\n    if (prop && typeof prop === 'object' && !(prop.buffer || prop.file || prop.content_type))\n      flatten(prop, into, prefix_key)\n    else\n      into[prefix_key] = prop;\n  }\n\n  return into;\n}\n\n\n//# sourceURL=webpack:///./node_modules/needle/lib/multipart.js?");

/***/ }),

/***/ "./node_modules/needle/lib/needle.js":
/*!*******************************************!*\
  !*** ./node_modules/needle/lib/needle.js ***!
  \*******************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("//////////////////////////////////////////\n// Needle -- HTTP Client for Node.js\n// Written by Tomás Pollak <tomas@forkhq.com>\n// (c) 2012-2020 - Fork Ltd.\n// MIT Licensed\n//////////////////////////////////////////\n\nvar fs          = __webpack_require__(/*! fs */ \"fs\"),\n    http        = __webpack_require__(/*! http */ \"http\"),\n    https       = __webpack_require__(/*! https */ \"https\"),\n    url         = __webpack_require__(/*! url */ \"url\"),\n    stream      = __webpack_require__(/*! stream */ \"stream\"),\n    debug       = __webpack_require__(/*! debug */ \"debug\")('needle'),\n    stringify   = __webpack_require__(/*! ./querystring */ \"./node_modules/needle/lib/querystring.js\").build,\n    multipart   = __webpack_require__(/*! ./multipart */ \"./node_modules/needle/lib/multipart.js\"),\n    auth        = __webpack_require__(/*! ./auth */ \"./node_modules/needle/lib/auth.js\"),\n    cookies     = __webpack_require__(/*! ./cookies */ \"./node_modules/needle/lib/cookies.js\"),\n    parsers     = __webpack_require__(/*! ./parsers */ \"./node_modules/needle/lib/parsers.js\"),\n    decoder     = __webpack_require__(/*! ./decoder */ \"./node_modules/needle/lib/decoder.js\");\n\n//////////////////////////////////////////\n// variabilia\n\nvar version     = __webpack_require__(/*! ../package.json */ \"./node_modules/needle/package.json\").version;\n\nvar user_agent  = 'Needle/' + version;\nuser_agent     += ' (Node.js ' + process.version + '; ' + process.platform + ' ' + process.arch + ')';\n\nvar tls_options = 'agent pfx key passphrase cert ca ciphers rejectUnauthorized secureProtocol checkServerIdentity family';\n\n// older versions of node (< 0.11.4) prevent the runtime from exiting\n// because of connections in keep-alive state. so if this is the case\n// we'll default new requests to set a Connection: close header.\nvar close_by_default = !http.Agent || http.Agent.defaultMaxSockets != Infinity;\n\n// see if we have Object.assign. otherwise fall back to util._extend\nvar extend = Object.assign ? Object.assign : __webpack_require__(/*! util */ \"util\")._extend;\n\n// these are the status codes that Needle interprets as redirects.\nvar redirect_codes = [301, 302, 303, 307, 308];\n\n//////////////////////////////////////////\n// decompressors for gzip/deflate/br bodies\n\nfunction bind_opts(fn, options) {\n  return fn.bind(null, options);\n}\n\nvar decompressors = {};\n\ntry {\n\n  var zlib = __webpack_require__(/*! zlib */ \"zlib\");\n\n  // Enable Z_SYNC_FLUSH to avoid Z_BUF_ERROR errors (Node PR #2595)\n  var zlib_options = {\n    flush: zlib.Z_SYNC_FLUSH,\n    finishFlush: zlib.Z_SYNC_FLUSH\n  };\n\n  var br_options = {\n    flush: zlib.BROTLI_OPERATION_FLUSH,\n    finishFlush: zlib.BROTLI_OPERATION_FLUSH\n  };\n\n  decompressors['x-deflate'] = bind_opts(zlib.Inflate, zlib_options);\n  decompressors['deflate']   = bind_opts(zlib.Inflate, zlib_options);\n  decompressors['x-gzip']    = bind_opts(zlib.Gunzip, zlib_options);\n  decompressors['gzip']      = bind_opts(zlib.Gunzip, zlib_options);\n  if (typeof zlib.BrotliDecompress === 'function') {\n    decompressors['br']      = bind_opts(zlib.BrotliDecompress, br_options);\n  }\n\n} catch(e) { /* zlib not available */ }\n\n//////////////////////////////////////////\n// options and aliases\n\nvar defaults = {\n  // data\n  boundary                : '--------------------NODENEEDLEHTTPCLIENT',\n  encoding                : 'utf8',\n  parse_response          : 'all', // same as true. valid options: 'json', 'xml' or false/null\n  proxy                   : null,\n\n  // headers\n  headers                 : {},\n  accept                  : '*/*',\n  user_agent              : user_agent,\n\n  // numbers\n  open_timeout            : 10000,\n  response_timeout        : 0,\n  read_timeout            : 0,\n  follow_max              : 0,\n  stream_length           : -1,\n\n  // booleans\n  compressed              : false,\n  decode_response         : true,\n  parse_cookies           : true,\n  follow_set_cookies      : false,\n  follow_set_referer      : false,\n  follow_keep_method      : false,\n  follow_if_same_host     : false,\n  follow_if_same_protocol : false,\n  follow_if_same_location : false\n}\n\nvar aliased = {\n  options: {\n    decode  : 'decode_response',\n    parse   : 'parse_response',\n    timeout : 'open_timeout',\n    follow  : 'follow_max'\n  },\n  inverted: {}\n}\n\n// only once, invert aliased keys so we can get passed options.\nObject.keys(aliased.options).map(function(k) {\n  var value = aliased.options[k];\n  aliased.inverted[value] = k;\n});\n\n//////////////////////////////////////////\n// helpers\n\nfunction keys_by_type(type) {\n  return Object.keys(defaults).map(function(el) {\n    if (defaults[el] !== null && defaults[el].constructor == type)\n      return el;\n  }).filter(function(el) { return el })\n}\n\nfunction parse_content_type(header) {\n  if (!header || header === '') return {};\n\n  var found, charset = 'utf8', arr = header.split(';');\n\n  if (arr.length > 1 && (found = arr[1].match(/charset=(.+)/)))\n    charset = found[1];\n\n  return { type: arr[0], charset: charset };\n}\n\nfunction is_stream(obj) {\n  return typeof obj.pipe === 'function';\n}\n\nfunction get_stream_length(stream, given_length, cb) {\n  if (given_length > 0)\n    return cb(given_length);\n\n  if (stream.end !== void 0 && stream.end !== Infinity && stream.start !== void 0)\n    return cb((stream.end + 1) - (stream.start || 0));\n\n  fs.stat(stream.path, function(err, stat) {\n    cb(stat ? stat.size - (stream.start || 0) : null);\n  });\n}\n\n//////////////////////////////////////////\n// the main act\n\nfunction Needle(method, uri, data, options, callback) {\n  // if (!(this instanceof Needle)) {\n  //   return new Needle(method, uri, data, options, callback);\n  // }\n\n  if (typeof uri !== 'string')\n    throw new TypeError('URL must be a string, not ' + uri);\n\n  this.method   = method;\n  this.uri      = uri;\n  this.data     = data;\n\n  if (typeof options == 'function') {\n    this.callback = options;\n    this.options  = {};\n  } else {\n    this.callback = callback;\n    this.options  = options;\n  }\n\n}\n\nNeedle.prototype.setup = function(uri, options) {\n\n  function get_option(key, fallback) {\n    // if original is in options, return that value\n    if (typeof options[key] != 'undefined') return options[key];\n\n    // otherwise, return value from alias or fallback/undefined\n    return typeof options[aliased.inverted[key]] != 'undefined'\n                ? options[aliased.inverted[key]] : fallback;\n  }\n\n  function check_value(expected, key) {\n    var value = get_option(key),\n        type  = typeof value;\n\n    if (type != 'undefined' && type != expected)\n      throw new TypeError(type + ' received for ' + key + ', but expected a ' + expected);\n\n    return (type == expected) ? value : defaults[key];\n  }\n\n  //////////////////////////////////////////////////\n  // the basics\n\n  var config = {\n    http_opts : {\n      localAddress: get_option('localAddress', undefined)\n    }, // passed later to http.request() directly\n    headers   : {},\n    output    : options.output,\n    proxy     : get_option('proxy', defaults.proxy),\n    parser    : get_option('parse_response', defaults.parse_response),\n    encoding  : options.encoding || (options.multipart ? 'binary' : defaults.encoding)\n  }\n\n  keys_by_type(Boolean).forEach(function(key) {\n    config[key] = check_value('boolean', key);\n  })\n\n  keys_by_type(Number).forEach(function(key) {\n    config[key] = check_value('number', key);\n  })\n\n  // populate http_opts with given TLS options\n  tls_options.split(' ').forEach(function(key) {\n    if (typeof options[key] != 'undefined') {\n      config.http_opts[key] = options[key];\n      if (typeof options.agent == 'undefined')\n        config.http_opts.agent = false; // otherwise tls options are skipped\n    }\n  });\n\n  //////////////////////////////////////////////////\n  // headers, cookies\n\n  for (var key in defaults.headers)\n    config.headers[key] = defaults.headers[key];\n\n  config.headers['accept'] = options.accept || defaults.accept;\n  config.headers['user-agent'] = options.user_agent || defaults.user_agent;\n\n  if (options.content_type)\n    config.headers['content-type'] = options.content_type;\n\n  // set connection header if opts.connection was passed, or if node < 0.11.4 (close)\n  if (options.connection || close_by_default)\n    config.headers['connection'] = options.connection || 'close';\n\n  if ((options.compressed || defaults.compressed) && typeof zlib != 'undefined')\n    config.headers['accept-encoding'] = decompressors['br'] ? 'gzip, deflate, br' : 'gzip, deflate';\n\n  if (options.cookies)\n    config.headers['cookie'] = cookies.write(options.cookies);\n\n  //////////////////////////////////////////////////\n  // basic/digest auth\n\n  if (uri.match(/[^\\/]@/)) { // url contains user:pass@host, so parse it.\n    var parts = (url.parse(uri).auth || '').split(':');\n    options.username = parts[0];\n    options.password = parts[1];\n  }\n\n  if (options.username) {\n    if (options.auth && (options.auth == 'auto' || options.auth == 'digest')) {\n      config.credentials = [options.username, options.password];\n    } else {\n      config.headers['authorization'] = auth.basic(options.username, options.password);\n    }\n  }\n\n  // if proxy is present, set auth header from either url or proxy_user option.\n  if (config.proxy) {\n    if (config.proxy.indexOf('http') === -1)\n      config.proxy = 'http://' + config.proxy;\n\n    if (config.proxy.indexOf('@') !== -1) {\n      var proxy = (url.parse(config.proxy).auth || '').split(':');\n      options.proxy_user = proxy[0];\n      options.proxy_pass = proxy[1];\n    }\n\n    if (options.proxy_user)\n      config.headers['proxy-authorization'] = auth.basic(options.proxy_user, options.proxy_pass);\n  }\n\n  // now that all our headers are set, overwrite them if instructed.\n  for (var h in options.headers)\n    config.headers[h.toLowerCase()] = options.headers[h];\n\n  config.uri_modifier = get_option('uri_modifier', null);\n\n  return config;\n}\n\nNeedle.prototype.start = function() {\n\n  var out      = new stream.PassThrough({ objectMode: false }),\n      uri      = this.uri,\n      data     = this.data,\n      method   = this.method,\n      callback = (typeof this.options == 'function') ? this.options : this.callback,\n      options  = this.options || {};\n\n  // if no 'http' is found on URL, prepend it.\n  if (uri.indexOf('http') === -1)\n    uri = uri.replace(/^(\\/\\/)?/, 'http://');\n\n  var self = this, body, waiting = false, config = this.setup(uri, options);\n\n  // unless options.json was set to false, assume boss also wants JSON if content-type matches.\n  var json = options.json || (options.json !== false && config.headers['content-type'] == 'application/json');\n\n  if (data) {\n\n    if (options.multipart) { // boss says we do multipart. so we do it.\n      var boundary = options.boundary || defaults.boundary;\n\n      waiting = true;\n      multipart.build(data, boundary, function(err, parts) {\n        if (err) throw(err);\n\n        config.headers['content-type'] = 'multipart/form-data; boundary=' + boundary;\n        next(parts);\n      });\n\n    } else if (is_stream(data)) {\n\n      if (method.toUpperCase() == 'GET')\n        throw new Error('Refusing to pipe() a stream via GET. Did you mean .post?');\n\n      if (config.stream_length > 0 || (config.stream_length === 0 && data.path)) {\n        // ok, let's get the stream's length and set it as the content-length header.\n        // this prevents some servers from cutting us off before all the data is sent.\n        waiting = true;\n        get_stream_length(data, config.stream_length, function(length) {\n          data.length = length;\n          next(data);\n        })\n\n      } else {\n        // if the boss doesn't want us to get the stream's length, or if it doesn't\n        // have a file descriptor for that purpose, then just head on.\n        body = data;\n      }\n\n    } else if (Buffer.isBuffer(data)) {\n\n      body = data; // use the raw buffer as request body.\n\n    } else if (method.toUpperCase() == 'GET' && !json) {\n\n      // append the data to the URI as a querystring.\n      uri = uri.replace(/\\?.*|$/, '?' + stringify(data));\n\n    } else { // string or object data, no multipart.\n\n      // if string, leave it as it is, otherwise, stringify.\n      body = (typeof(data) === 'string') ? data\n             : json ? JSON.stringify(data) : stringify(data);\n\n      // ensure we have a buffer so bytecount is correct.\n      body = Buffer.from(body, config.encoding);\n    }\n\n  }\n\n  function next(body) {\n    if (body) {\n      if (body.length) config.headers['content-length'] = body.length;\n\n      // if no content-type was passed, determine if json or not.\n      if (!config.headers['content-type']) {\n        config.headers['content-type'] = json\n        ? 'application/json; charset=utf-8'\n        : 'application/x-www-form-urlencoded'; // no charset says W3 spec.\n      }\n    }\n\n    // unless a specific accept header was set, assume json: true wants JSON back.\n    if (options.json && (!options.accept && !(options.headers || {}).accept))\n      config.headers['accept'] = 'application/json';\n\n    self.send_request(1, method, uri, config, body, out, callback);\n  }\n\n  if (!waiting) next(body);\n  return out;\n}\n\nNeedle.prototype.get_request_opts = function(method, uri, config) {\n  var opts      = config.http_opts,\n      proxy     = config.proxy,\n      remote    = proxy ? url.parse(proxy) : url.parse(uri);\n\n  opts.protocol = remote.protocol;\n  opts.host     = remote.hostname;\n  opts.port     = remote.port || (remote.protocol == 'https:' ? 443 : 80);\n  opts.path     = proxy ? uri : remote.pathname + (remote.search || '');\n  opts.method   = method;\n  opts.headers  = config.headers;\n\n  if (!opts.headers['host']) {\n    // if using proxy, make sure the host header shows the final destination\n    var target = proxy ? url.parse(uri) : remote;\n    opts.headers['host'] = target.hostname;\n\n    // and if a non standard port was passed, append it to the port header\n    if (target.port && [80, 443].indexOf(target.port) === -1) {\n      opts.headers['host'] += ':' + target.port;\n    }\n  }\n\n  return opts;\n}\n\nNeedle.prototype.should_follow = function(location, config, original) {\n  if (!location) return false;\n\n  // returns true if location contains matching property (host or protocol)\n  function matches(property) {\n    var property = original[property];\n    return location.indexOf(property) !== -1;\n  }\n\n  // first, check whether the requested location is actually different from the original\n  if (!config.follow_if_same_location && location === original)\n    return false;\n\n  if (config.follow_if_same_host && !matches('host'))\n    return false; // host does not match, so not following\n\n  if (config.follow_if_same_protocol && !matches('protocol'))\n    return false; // procotol does not match, so not following\n\n  return true;\n}\n\nNeedle.prototype.send_request = function(count, method, uri, config, post_data, out, callback) {\n\n  if (typeof config.uri_modifier === 'function') {\n    var modified_uri = config.uri_modifier(uri);\n    debug('Modifying request URI', uri + ' => ' + modified_uri);\n    uri = modified_uri;\n  }\n\n  var timer,\n      returned     = 0,\n      self         = this,\n      request_opts = this.get_request_opts(method, uri, config),\n      protocol     = request_opts.protocol == 'https:' ? https : http;\n\n  function done(err, resp) {\n    if (returned++ > 0)\n      return debug('Already finished, stopping here.');\n\n    if (timer) clearTimeout(timer);\n    request.removeListener('error', had_error);\n\n    if (callback)\n      return callback(err, resp, resp ? resp.body : undefined);\n\n    // NOTE: this event used to be called 'end', but the behaviour was confusing\n    // when errors ocurred, because the stream would still emit an 'end' event.\n    out.emit('done', err);\n  }\n\n  function had_error(err) {\n    debug('Request error', err);\n    out.emit('err', err);\n    done(err || new Error('Unknown error when making request.'));\n  }\n\n  function set_timeout(type, milisecs) {\n    if (timer) clearTimeout(timer);\n    if (milisecs <= 0) return;\n\n    timer = setTimeout(function() {\n      out.emit('timeout', type);\n      request.abort();\n      // also invoke done() to terminate job on read_timeout\n      if (type == 'read') done(new Error(type + ' timeout'));\n    }, milisecs);\n  }\n\n  // handle errors on the underlying socket, that may be closed while writing\n  // for an example case, see test/long_string_spec.js. we make sure this\n  // scenario ocurred by verifying the socket's writable & destroyed states.\n  function on_socket_end() {\n    if (returned && !this.writable && this.destroyed === false) {\n      this.destroy();\n      had_error(new Error('Remote end closed socket abruptly.'))\n    }\n  }\n\n  debug('Making request #' + count, request_opts);\n  var request = protocol.request(request_opts, function(resp) {\n\n    var headers = resp.headers;\n    debug('Got response', resp.statusCode, headers);\n    out.emit('response', resp);\n\n    set_timeout('read', config.read_timeout);\n\n    // if we got cookies, parse them unless we were instructed not to. make sure to include any\n    // cookies that might have been set on previous redirects.\n    if (config.parse_cookies && (headers['set-cookie'] || config.previous_resp_cookies)) {\n      resp.cookies = extend(config.previous_resp_cookies || {}, cookies.read(headers['set-cookie']));\n      debug('Got cookies', resp.cookies);\n    }\n\n    // if redirect code is found, determine if we should follow it according to the given options.\n    if (redirect_codes.indexOf(resp.statusCode) !== -1 && self.should_follow(headers.location, config, uri)) {\n      // clear timer before following redirects to prevent unexpected setTimeout consequence\n      clearTimeout(timer);\n\n      if (count <= config.follow_max) {\n        out.emit('redirect', headers.location);\n\n        // unless 'follow_keep_method' is true, rewrite the request to GET before continuing.\n        if (!config.follow_keep_method) {\n          method    = 'GET';\n          post_data = null;\n          delete config.headers['content-length']; // in case the original was a multipart POST request.\n        }\n\n        // if follow_set_cookies is true, insert cookies in the next request's headers.\n        // we set both the original request cookies plus any response cookies we might have received.\n        if (config.follow_set_cookies) {\n          var request_cookies = cookies.read(config.headers['cookie']);\n          config.previous_resp_cookies = resp.cookies;\n          if (Object.keys(request_cookies).length || Object.keys(resp.cookies || {}).length) {\n            config.headers['cookie'] = cookies.write(extend(request_cookies, resp.cookies));\n          }\n        } else if (config.headers['cookie']) {\n          debug('Clearing original request cookie', config.headers['cookie']);\n          delete config.headers['cookie'];\n        }\n\n        if (config.follow_set_referer)\n          config.headers['referer'] = encodeURI(uri); // the original, not the destination URL.\n\n        config.headers['host'] = null; // clear previous Host header to avoid conflicts.\n\n        debug('Redirecting to ' + url.resolve(uri, headers.location));\n        return self.send_request(++count, method, url.resolve(uri, headers.location), config, post_data, out, callback);\n      } else if (config.follow_max > 0) {\n        return done(new Error('Max redirects reached. Possible loop in: ' + headers.location));\n      }\n    }\n\n    // if auth is requested and credentials were not passed, resend request, provided we have user/pass.\n    if (resp.statusCode == 401 && headers['www-authenticate'] && config.credentials) {\n      if (!config.headers['authorization']) { // only if authentication hasn't been sent\n        var auth_header = auth.header(headers['www-authenticate'], config.credentials, request_opts);\n\n        if (auth_header) {\n          config.headers['authorization'] = auth_header;\n          return self.send_request(count, method, uri, config, post_data, out, callback);\n        }\n      }\n    }\n\n    // ok, so we got a valid (non-redirect & authorized) response. let's notify the stream guys.\n    out.emit('header', resp.statusCode, headers);\n    out.emit('headers', headers);\n\n    var pipeline      = [],\n        mime          = parse_content_type(headers['content-type']),\n        text_response = mime.type && mime.type.indexOf('text/') != -1;\n\n    // To start, if our body is compressed and we're able to inflate it, do it.\n    if (headers['content-encoding'] && decompressors[headers['content-encoding']]) {\n\n      var decompressor = decompressors[headers['content-encoding']]();\n\n      // make sure we catch errors triggered by the decompressor.\n      decompressor.on('error', had_error);\n      pipeline.push(decompressor);\n    }\n\n    // If parse is enabled and we have a parser for it, then go for it.\n    if (config.parser && parsers[mime.type]) {\n\n      // If a specific parser was requested, make sure we don't parse other types.\n      var parser_name = config.parser.toString().toLowerCase();\n      if (['xml', 'json'].indexOf(parser_name) == -1 || parsers[mime.type].name == parser_name) {\n\n        // OK, so either we're parsing all content types or the one requested matches.\n        out.parser = parsers[mime.type].name;\n        pipeline.push(parsers[mime.type].fn());\n\n        // Set objectMode on out stream to improve performance.\n        out._writableState.objectMode = true;\n        out._readableState.objectMode = true;\n      }\n\n    // If we're not parsing, and unless decoding was disabled, we'll try\n    // decoding non UTF-8 bodies to UTF-8, using the iconv-lite library.\n    } else if (text_response && config.decode_response\n      && mime.charset) {\n        pipeline.push(decoder(mime.charset));\n    }\n    // And `out` is the stream we finally push the decoded/parsed output to.\n    pipeline.push(out);\n\n    // Now, release the kraken!\n    var tmp = resp;\n    while (pipeline.length) {\n      tmp = tmp.pipe(pipeline.shift());\n    }\n\n    // If the user has requested and output file, pipe the output stream to it.\n    // In stream mode, we will still get the response stream to play with.\n    if (config.output && resp.statusCode == 200) {\n\n      // for some reason, simply piping resp to the writable stream doesn't\n      // work all the time (stream gets cut in the middle with no warning).\n      // so we'll manually need to do the readable/write(chunk) trick.\n      var file = fs.createWriteStream(config.output);\n      file.on('error', had_error);\n\n      out.on('end', function() {\n        if (file.writable) file.end();\n      });\n\n      file.on('close', function() {\n        delete out.file;\n      })\n\n      out.on('readable', function() {\n        var chunk;\n        while ((chunk = this.read()) !== null) {\n          if (file.writable) file.write(chunk);\n\n          // if callback was requested, also push it to resp.body\n          if (resp.body) resp.body.push(chunk);\n        }\n      })\n\n      out.file = file;\n    }\n\n    // Only aggregate the full body if a callback was requested.\n    if (callback) {\n      resp.raw   = [];\n      resp.body  = [];\n      resp.bytes = 0;\n\n      // Gather and count the amount of (raw) bytes using a PassThrough stream.\n      var clean_pipe = new stream.PassThrough();\n      resp.pipe(clean_pipe);\n\n      clean_pipe.on('readable', function() {\n        var chunk;\n        while ((chunk = this.read()) != null) {\n          resp.bytes += chunk.length;\n          resp.raw.push(chunk);\n        }\n      })\n\n      // Listen on the 'readable' event to aggregate the chunks, but only if\n      // file output wasn't requested. Otherwise we'd have two stream readers.\n      if (!config.output || resp.statusCode != 200) {\n        out.on('readable', function() {\n          var chunk;\n          while ((chunk = this.read()) !== null) {\n            // We're either pushing buffers or objects, never strings.\n            if (typeof chunk == 'string') chunk = Buffer.from(chunk);\n\n            // Push all chunks to resp.body. We'll bind them in resp.end().\n            resp.body.push(chunk);\n          }\n        })\n      }\n    }\n\n    // And set the .body property once all data is in.\n    out.on('end', function() {\n      if (resp.body) { // callback mode\n\n        // we want to be able to access to the raw data later, so keep a reference.\n        resp.raw = Buffer.concat(resp.raw);\n\n        // if parse was successful, we should have an array with one object\n        if (resp.body[0] !== undefined && !Buffer.isBuffer(resp.body[0])) {\n\n          // that's our body right there.\n          resp.body = resp.body[0];\n\n          // set the parser property on our response. we may want to check.\n          if (out.parser) resp.parser = out.parser;\n\n        } else { // we got one or several buffers. string or binary.\n          resp.body = Buffer.concat(resp.body);\n\n          // if we're here and parsed is true, it means we tried to but it didn't work.\n          // so given that we got a text response, let's stringify it.\n          if (text_response || out.parser) {\n            resp.body = resp.body.toString();\n          }\n        }\n      }\n\n      // if an output file is being written to, make sure the callback\n      // is triggered after all data has been written to it.\n      if (out.file) {\n        out.file.on('close', function() {\n          done(null, resp, resp.body);\n        })\n      } else { // elvis has left the building.\n        done(null, resp, resp.body);\n      }\n\n    });\n\n  }); // end request call\n\n  // unless open_timeout was disabled, set a timeout to abort the request.\n  set_timeout('open', config.open_timeout);\n\n  // handle errors on the request object. things might get bumpy.\n  request.on('error', had_error);\n\n  // make sure timer is cleared if request is aborted (issue #257)\n  request.once('abort', function() {\n    if (timer) clearTimeout(timer);\n  })\n\n  // handle socket 'end' event to ensure we don't get delayed EPIPE errors.\n  request.once('socket', function(socket) {\n    if (socket.connecting) {\n      socket.once('connect', function() {\n        set_timeout('response', config.response_timeout);\n      })\n    } else {\n      set_timeout('response', config.response_timeout);\n    }\n\n    // console.log(socket);\n    if (!socket.on_socket_end) {\n      socket.on_socket_end = on_socket_end;\n      socket.once('end', function() { process.nextTick(on_socket_end.bind(socket)) });\n    }\n  })\n\n  if (post_data) {\n    if (is_stream(post_data)) {\n      post_data.pipe(request);\n    } else {\n      request.write(post_data, config.encoding);\n      request.end();\n    }\n  } else {\n    request.end();\n  }\n\n  out.request = request;\n  return out;\n}\n\n//////////////////////////////////////////\n// exports\n\nif (typeof Promise !== 'undefined') {\n  module.exports = function() {\n    var verb, args = [].slice.call(arguments);\n\n    if (args[0].match(/\\.|\\//)) // first argument looks like a URL\n      verb = (args.length > 2) ? 'post' : 'get';\n    else\n      verb = args.shift();\n\n    if (verb.match(/get|head/) && args.length == 2)\n      args.splice(1, 0, null); // assume no data if head/get with two args (url, options)\n\n    return new Promise(function(resolve, reject) {\n      module.exports.request(verb, args[0], args[1], args[2], function(err, resp) {\n        return err ? reject(err) : resolve(resp);\n      });\n    })\n  }\n}\n\nmodule.exports.version = version;\n\nmodule.exports.defaults = function(obj) {\n  for (var key in obj) {\n    var target_key = aliased.options[key] || key;\n\n    if (defaults.hasOwnProperty(target_key) && typeof obj[key] != 'undefined') {\n      if (target_key != 'parse_response' && target_key != 'proxy') {\n        // ensure type matches the original, except for proxy/parse_response that can be null/bool or string\n        var valid_type = defaults[target_key].constructor.name;\n\n        if (obj[key].constructor.name != valid_type)\n          throw new TypeError('Invalid type for ' + key + ', should be ' + valid_type);\n      }\n      defaults[target_key] = obj[key];\n    } else {\n      throw new Error('Invalid property for defaults:' + target_key);\n    }\n  }\n\n  return defaults;\n}\n\n'head get'.split(' ').forEach(function(method) {\n  module.exports[method] = function(uri, options, callback) {\n    return new Needle(method, uri, null, options, callback).start();\n  }\n})\n\n'post put patch delete'.split(' ').forEach(function(method) {\n  module.exports[method] = function(uri, data, options, callback) {\n    return new Needle(method, uri, data, options, callback).start();\n  }\n})\n\nmodule.exports.request = function(method, uri, data, opts, callback) {\n  return new Needle(method, uri, data, opts, callback).start();\n};\n\n\n//# sourceURL=webpack:///./node_modules/needle/lib/needle.js?");

/***/ }),

/***/ "./node_modules/needle/lib/parsers.js":
/*!********************************************!*\
  !*** ./node_modules/needle/lib/parsers.js ***!
  \********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("//////////////////////////////////////////\n// Defines mappings between content-type\n// and the appropriate parsers.\n//////////////////////////////////////////\n\nvar Transform = __webpack_require__(/*! stream */ \"stream\").Transform;\nvar sax = __webpack_require__(/*! sax */ \"./node_modules/sax/lib/sax.js\");\n\nfunction parseXML(str, cb) {\n  var obj, current, parser = sax.parser(true, { trim: true, lowercase: true })\n  parser.onerror = parser.onend = done;\n\n  function done(err) {\n    parser.onerror = parser.onend = function() { }\n    cb(err, obj)\n  }\n\n  function newElement(name, attributes) {\n    return {\n      name: name || '',\n      value: '',\n      attributes: attributes || {},\n      children: []\n    }\n  }\n\n  parser.oncdata = parser.ontext = function(t) {\n    if (current) current.value += t\n  }\n\n  parser.onopentag = function(node) {\n    var element = newElement(node.name, node.attributes)\n    if (current) {\n      element.parent = current\n      current.children.push(element)\n    } else { // root object\n      obj = element\n    }\n\n    current = element\n  };\n\n  parser.onclosetag = function() {\n    if (typeof current.parent !== 'undefined') {\n      var just_closed = current\n      current = current.parent\n      delete just_closed.parent\n    }\n  }\n\n  parser.write(str).close()\n}\n\nfunction parserFactory(name, fn) {\n\n  function parser() {\n    var chunks = [],\n        stream = new Transform({ objectMode: true });\n\n    // Buffer all our data\n    stream._transform = function(chunk, encoding, done) {\n      chunks.push(chunk);\n      done();\n    }\n\n    // And call the parser when all is there.\n    stream._flush = function(done) {\n      var self = this,\n          data = Buffer.concat(chunks);\n\n      try {\n        fn(data, function(err, result) {\n          if (err) throw err;\n          self.push(result);\n        });\n      } catch (err) {\n        self.push(data); // just pass the original data\n      } finally {\n        done();\n      }\n    }\n\n    return stream;\n  }\n\n  return { fn: parser, name: name };\n}\n\nvar parsers = {}\n\nfunction buildParser(name, types, fn) {\n  var parser = parserFactory(name, fn);\n  types.forEach(function(type) {\n    parsers[type] = parser;\n  })\n}\n\nbuildParser('json', [\n  'application/json',\n  'text/javascript'\n], function(buffer, cb) {\n  var err, data;\n  try { data = JSON.parse(buffer); } catch (e) { err = e; }\n  cb(err, data);\n});\n\nbuildParser('xml', [\n  'text/xml',\n  'application/xml',\n  'application/rdf+xml',\n  'application/rss+xml',\n  'application/atom+xml'\n], function(buffer, cb) {\n  parseXML(buffer.toString(), function(err, obj) {\n    cb(err, obj)\n  })\n});\n\nmodule.exports = parsers;\nmodule.exports.use = buildParser;\n\n\n//# sourceURL=webpack:///./node_modules/needle/lib/parsers.js?");

/***/ }),

/***/ "./node_modules/needle/lib/querystring.js":
/*!************************************************!*\
  !*** ./node_modules/needle/lib/querystring.js ***!
  \************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("// based on the qs module, but handles null objects as expected\n// fixes by Tomas Pollak.\n\nvar toString = Object.prototype.toString;\n\nfunction stringify(obj, prefix) {\n  if (prefix && (obj === null || typeof obj == 'undefined')) {\n    return prefix + '=';\n  } else if (toString.call(obj) == '[object Array]') {\n    return stringifyArray(obj, prefix);\n  } else if (toString.call(obj) == '[object Object]') {\n    return stringifyObject(obj, prefix);\n  } else if (toString.call(obj) == '[object Date]') {\n    return obj.toISOString();\n  } else if (prefix) { // string inside array or hash\n    return prefix + '=' + encodeURIComponent(String(obj));\n  } else if (String(obj).indexOf('=') !== -1) { // string with equal sign\n    return String(obj);\n  } else {\n    throw new TypeError('Cannot build a querystring out of: ' + obj);\n  }\n};\n\nfunction stringifyArray(arr, prefix) {\n  var ret = [];\n\n  for (var i = 0, len = arr.length; i < len; i++) {\n    if (prefix)\n      ret.push(stringify(arr[i], prefix + '[]'));\n    else\n      ret.push(stringify(arr[i]));\n  }\n\n  return ret.join('&');\n}\n\nfunction stringifyObject(obj, prefix) {\n  var ret = [];\n\n  Object.keys(obj).forEach(function(key) {\n    ret.push(stringify(obj[key], prefix\n      ? prefix + '[' + encodeURIComponent(key) + ']'\n      : encodeURIComponent(key)));\n  })\n\n  return ret.join('&');\n}\n\nexports.build = stringify;\n\n\n//# sourceURL=webpack:///./node_modules/needle/lib/querystring.js?");

/***/ }),

/***/ "./node_modules/needle/package.json":
/*!******************************************!*\
  !*** ./node_modules/needle/package.json ***!
  \******************************************/
/*! exports provided: _from, _id, _inBundle, _integrity, _location, _phantomChildren, _requested, _requiredBy, _resolved, _shasum, _spec, _where, author, bin, bugs, bundleDependencies, dependencies, deprecated, description, devDependencies, directories, engines, homepage, keywords, license, main, name, repository, scripts, tags, version, default */
/***/ (function(module) {

eval("module.exports = JSON.parse(\"{\\\"_from\\\":\\\"needle@^2.5.0\\\",\\\"_id\\\":\\\"needle@2.6.0\\\",\\\"_inBundle\\\":false,\\\"_integrity\\\":\\\"sha512-KKYdza4heMsEfSWD7VPUIz3zX2XDwOyX2d+geb4vrERZMT5RMU6ujjaD+I5Yr54uZxQ2w6XRTAhHBbSCyovZBg==\\\",\\\"_location\\\":\\\"/needle\\\",\\\"_phantomChildren\\\":{},\\\"_requested\\\":{\\\"type\\\":\\\"range\\\",\\\"registry\\\":true,\\\"raw\\\":\\\"needle@^2.5.0\\\",\\\"name\\\":\\\"needle\\\",\\\"escapedName\\\":\\\"needle\\\",\\\"rawSpec\\\":\\\"^2.5.0\\\",\\\"saveSpec\\\":null,\\\"fetchSpec\\\":\\\"^2.5.0\\\"},\\\"_requiredBy\\\":[\\\"/node-pre-gyp\\\"],\\\"_resolved\\\":\\\"https://registry.npmjs.org/needle/-/needle-2.6.0.tgz\\\",\\\"_shasum\\\":\\\"24dbb55f2509e2324b4a99d61f413982013ccdbe\\\",\\\"_spec\\\":\\\"needle@^2.5.0\\\",\\\"_where\\\":\\\"/media/zuares/Zuares1/Programing/Source Code/Javascript/Typescript/Personal/expressJs/node_modules/node-pre-gyp\\\",\\\"author\\\":{\\\"name\\\":\\\"Tomás Pollak\\\",\\\"email\\\":\\\"tomas@forkhq.com\\\"},\\\"bin\\\":{\\\"needle\\\":\\\"bin/needle\\\"},\\\"bugs\\\":{\\\"url\\\":\\\"https://github.com/tomas/needle/issues\\\"},\\\"bundleDependencies\\\":false,\\\"dependencies\\\":{\\\"debug\\\":\\\"^3.2.6\\\",\\\"iconv-lite\\\":\\\"^0.4.4\\\",\\\"sax\\\":\\\"^1.2.4\\\"},\\\"deprecated\\\":false,\\\"description\\\":\\\"The leanest and most handsome HTTP client in the Nodelands.\\\",\\\"devDependencies\\\":{\\\"JSONStream\\\":\\\"^1.3.5\\\",\\\"jschardet\\\":\\\"^1.6.0\\\",\\\"mocha\\\":\\\"^5.2.0\\\",\\\"q\\\":\\\"^1.5.1\\\",\\\"should\\\":\\\"^13.2.3\\\",\\\"sinon\\\":\\\"^2.3.0\\\",\\\"xml2js\\\":\\\"^0.4.19\\\"},\\\"directories\\\":{\\\"lib\\\":\\\"./lib\\\"},\\\"engines\\\":{\\\"node\\\":\\\">= 4.4.x\\\"},\\\"homepage\\\":\\\"https://github.com/tomas/needle#readme\\\",\\\"keywords\\\":[\\\"http\\\",\\\"https\\\",\\\"simple\\\",\\\"request\\\",\\\"client\\\",\\\"multipart\\\",\\\"upload\\\",\\\"proxy\\\",\\\"deflate\\\",\\\"timeout\\\",\\\"charset\\\",\\\"iconv\\\",\\\"cookie\\\",\\\"redirect\\\"],\\\"license\\\":\\\"MIT\\\",\\\"main\\\":\\\"./lib/needle\\\",\\\"name\\\":\\\"needle\\\",\\\"repository\\\":{\\\"type\\\":\\\"git\\\",\\\"url\\\":\\\"git+https://github.com/tomas/needle.git\\\"},\\\"scripts\\\":{\\\"test\\\":\\\"mocha test\\\"},\\\"tags\\\":[\\\"http\\\",\\\"https\\\",\\\"simple\\\",\\\"request\\\",\\\"client\\\",\\\"multipart\\\",\\\"upload\\\",\\\"proxy\\\",\\\"deflate\\\",\\\"timeout\\\",\\\"charset\\\",\\\"iconv\\\",\\\"cookie\\\",\\\"redirect\\\"],\\\"version\\\":\\\"2.6.0\\\"}\");\n\n//# sourceURL=webpack:///./node_modules/needle/package.json?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib sync recursive":
/*!********************************************!*\
  !*** ./node_modules/node-pre-gyp/lib sync ***!
  \********************************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("function webpackEmptyContext(req) {\n\tvar e = new Error(\"Cannot find module '\" + req + \"'\");\n\te.code = 'MODULE_NOT_FOUND';\n\tthrow e;\n}\nwebpackEmptyContext.keys = function() { return []; };\nwebpackEmptyContext.resolve = webpackEmptyContext;\nmodule.exports = webpackEmptyContext;\nwebpackEmptyContext.id = \"./node_modules/node-pre-gyp/lib sync recursive\";\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib_sync?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib sync recursive ^\\.\\/.*$":
/*!*****************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib sync ^\.\/.*$ ***!
  \*****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("var map = {\n\t\"./build\": \"./node_modules/node-pre-gyp/lib/build.js\",\n\t\"./build.js\": \"./node_modules/node-pre-gyp/lib/build.js\",\n\t\"./clean\": \"./node_modules/node-pre-gyp/lib/clean.js\",\n\t\"./clean.js\": \"./node_modules/node-pre-gyp/lib/clean.js\",\n\t\"./configure\": \"./node_modules/node-pre-gyp/lib/configure.js\",\n\t\"./configure.js\": \"./node_modules/node-pre-gyp/lib/configure.js\",\n\t\"./info\": \"./node_modules/node-pre-gyp/lib/info.js\",\n\t\"./info.js\": \"./node_modules/node-pre-gyp/lib/info.js\",\n\t\"./install\": \"./node_modules/node-pre-gyp/lib/install.js\",\n\t\"./install.js\": \"./node_modules/node-pre-gyp/lib/install.js\",\n\t\"./node-pre-gyp\": \"./node_modules/node-pre-gyp/lib/node-pre-gyp.js\",\n\t\"./node-pre-gyp.js\": \"./node_modules/node-pre-gyp/lib/node-pre-gyp.js\",\n\t\"./package\": \"./node_modules/node-pre-gyp/lib/package.js\",\n\t\"./package.js\": \"./node_modules/node-pre-gyp/lib/package.js\",\n\t\"./pre-binding\": \"./node_modules/node-pre-gyp/lib/pre-binding.js\",\n\t\"./pre-binding.js\": \"./node_modules/node-pre-gyp/lib/pre-binding.js\",\n\t\"./publish\": \"./node_modules/node-pre-gyp/lib/publish.js\",\n\t\"./publish.js\": \"./node_modules/node-pre-gyp/lib/publish.js\",\n\t\"./rebuild\": \"./node_modules/node-pre-gyp/lib/rebuild.js\",\n\t\"./rebuild.js\": \"./node_modules/node-pre-gyp/lib/rebuild.js\",\n\t\"./reinstall\": \"./node_modules/node-pre-gyp/lib/reinstall.js\",\n\t\"./reinstall.js\": \"./node_modules/node-pre-gyp/lib/reinstall.js\",\n\t\"./reveal\": \"./node_modules/node-pre-gyp/lib/reveal.js\",\n\t\"./reveal.js\": \"./node_modules/node-pre-gyp/lib/reveal.js\",\n\t\"./testbinary\": \"./node_modules/node-pre-gyp/lib/testbinary.js\",\n\t\"./testbinary.js\": \"./node_modules/node-pre-gyp/lib/testbinary.js\",\n\t\"./testpackage\": \"./node_modules/node-pre-gyp/lib/testpackage.js\",\n\t\"./testpackage.js\": \"./node_modules/node-pre-gyp/lib/testpackage.js\",\n\t\"./unpublish\": \"./node_modules/node-pre-gyp/lib/unpublish.js\",\n\t\"./unpublish.js\": \"./node_modules/node-pre-gyp/lib/unpublish.js\",\n\t\"./util/abi_crosswalk.json\": \"./node_modules/node-pre-gyp/lib/util/abi_crosswalk.json\",\n\t\"./util/compile\": \"./node_modules/node-pre-gyp/lib/util/compile.js\",\n\t\"./util/compile.js\": \"./node_modules/node-pre-gyp/lib/util/compile.js\",\n\t\"./util/handle_gyp_opts\": \"./node_modules/node-pre-gyp/lib/util/handle_gyp_opts.js\",\n\t\"./util/handle_gyp_opts.js\": \"./node_modules/node-pre-gyp/lib/util/handle_gyp_opts.js\",\n\t\"./util/napi\": \"./node_modules/node-pre-gyp/lib/util/napi.js\",\n\t\"./util/napi.js\": \"./node_modules/node-pre-gyp/lib/util/napi.js\",\n\t\"./util/nw-pre-gyp/index.html\": \"./node_modules/node-pre-gyp/lib/util/nw-pre-gyp/index.html\",\n\t\"./util/nw-pre-gyp/package.json\": \"./node_modules/node-pre-gyp/lib/util/nw-pre-gyp/package.json\",\n\t\"./util/s3_setup\": \"./node_modules/node-pre-gyp/lib/util/s3_setup.js\",\n\t\"./util/s3_setup.js\": \"./node_modules/node-pre-gyp/lib/util/s3_setup.js\",\n\t\"./util/versioning\": \"./node_modules/node-pre-gyp/lib/util/versioning.js\",\n\t\"./util/versioning.js\": \"./node_modules/node-pre-gyp/lib/util/versioning.js\"\n};\n\n\nfunction webpackContext(req) {\n\tvar id = webpackContextResolve(req);\n\treturn __webpack_require__(id);\n}\nfunction webpackContextResolve(req) {\n\tif(!__webpack_require__.o(map, req)) {\n\t\tvar e = new Error(\"Cannot find module '\" + req + \"'\");\n\t\te.code = 'MODULE_NOT_FOUND';\n\t\tthrow e;\n\t}\n\treturn map[req];\n}\nwebpackContext.keys = function webpackContextKeys() {\n\treturn Object.keys(map);\n};\nwebpackContext.resolve = webpackContextResolve;\nmodule.exports = webpackContext;\nwebpackContext.id = \"./node_modules/node-pre-gyp/lib sync recursive ^\\\\.\\\\/.*$\";\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib_sync_^\\.\\/.*$?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/build.js":
/*!************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/build.js ***!
  \************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = exports = build;\n\nexports.usage = 'Attempts to compile the module by dispatching to node-gyp or nw-gyp';\n\nvar napi = __webpack_require__(/*! ./util/napi.js */ \"./node_modules/node-pre-gyp/lib/util/napi.js\");\nvar compile = __webpack_require__(/*! ./util/compile.js */ \"./node_modules/node-pre-gyp/lib/util/compile.js\");\nvar handle_gyp_opts = __webpack_require__(/*! ./util/handle_gyp_opts.js */ \"./node_modules/node-pre-gyp/lib/util/handle_gyp_opts.js\");\nvar configure = __webpack_require__(/*! ./configure.js */ \"./node_modules/node-pre-gyp/lib/configure.js\");\n\nfunction do_build(gyp,argv,callback) {\n    handle_gyp_opts(gyp,argv,function(err,result) {\n        var final_args = ['build'].concat(result.gyp).concat(result.pre);\n        if (result.unparsed.length > 0) {\n            final_args = final_args.\n                          concat(['--']).\n                          concat(result.unparsed);\n        }\n        if (!err && result.opts.napi_build_version) {\n            napi.swap_build_dir_in(result.opts.napi_build_version);\n        }\n        compile.run_gyp(final_args,result.opts,function(err) {\n            if (result.opts.napi_build_version) {\n                napi.swap_build_dir_out(result.opts.napi_build_version);\n            }\n            return callback(err);\n        });\n    });\n}\n\nfunction build(gyp, argv, callback) {\n\n    // Form up commands to pass to node-gyp:\n    // We map `node-pre-gyp build` to `node-gyp configure build` so that we do not\n    // trigger a clean and therefore do not pay the penalty of a full recompile\n    if (argv.length && (argv.indexOf('rebuild') > -1)) {\n        argv.shift(); // remove `rebuild`\n        // here we map `node-pre-gyp rebuild` to `node-gyp rebuild` which internally means\n        // \"clean + configure + build\" and triggers a full recompile\n        compile.run_gyp(['clean'],{},function(err) {\n            if (err) return callback(err);\n            configure(gyp,argv,function(err) {\n                if (err) return callback(err);\n                return do_build(gyp,argv,callback);\n            });\n        });\n    } else {\n        return do_build(gyp,argv,callback);        \n    }\n}\n\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/build.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/clean.js":
/*!************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/clean.js ***!
  \************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = exports = clean;\n\nexports.usage = 'Removes the entire folder containing the compiled .node module';\n\nvar fs = __webpack_require__(/*! fs */ \"fs\");\nvar rm = __webpack_require__(/*! rimraf */ \"rimraf\");\nvar exists = __webpack_require__(/*! fs */ \"fs\").exists || __webpack_require__(/*! path */ \"path\").exists;\nvar versioning = __webpack_require__(/*! ./util/versioning.js */ \"./node_modules/node-pre-gyp/lib/util/versioning.js\");\nvar napi = __webpack_require__(/*! ./util/napi.js */ \"./node_modules/node-pre-gyp/lib/util/napi.js\");\nvar path = __webpack_require__(/*! path */ \"path\");\n\nfunction clean (gyp, argv, callback) {\n    var package_json = JSON.parse(fs.readFileSync('./package.json'));\n    var napi_build_version = napi.get_napi_build_version_from_command_args(argv);\n    var opts = versioning.evaluate(package_json, gyp.opts, napi_build_version);\n    var to_delete = opts.module_path;\n    if (!to_delete) {\n        return callback(new Error(\"module_path is empty, refusing to delete\"));\n    } else if (path.normalize(to_delete) == path.normalize(process.cwd())) {\n        return callback(new Error(\"module_path is not set, refusing to delete\"));\n    } else {\n        exists(to_delete, function(found) {\n            if (found) {\n                if (!gyp.opts.silent_clean) console.log('['+package_json.name+'] Removing \"%s\"', to_delete);\n                return rm(to_delete, callback);\n            }\n            return callback();\n        });\n    }\n}\n\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/clean.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/configure.js":
/*!****************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/configure.js ***!
  \****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = exports = configure;\n\nexports.usage = 'Attempts to configure node-gyp or nw-gyp build';\n\nvar napi = __webpack_require__(/*! ./util/napi.js */ \"./node_modules/node-pre-gyp/lib/util/napi.js\");\nvar compile = __webpack_require__(/*! ./util/compile.js */ \"./node_modules/node-pre-gyp/lib/util/compile.js\");\nvar handle_gyp_opts = __webpack_require__(/*! ./util/handle_gyp_opts.js */ \"./node_modules/node-pre-gyp/lib/util/handle_gyp_opts.js\");\n\nfunction configure(gyp, argv, callback) {\n    handle_gyp_opts(gyp,argv,function(err,result) {\n        var final_args = result.gyp.concat(result.pre);\n        // pull select node-gyp configure options out of the npm environ\n        var known_gyp_args = ['dist-url','python','nodedir','msvs_version'];\n        known_gyp_args.forEach(function(key) {\n            var val = gyp.opts[key] || gyp.opts[key.replace('-','_')];\n            if (val) {\n               final_args.push('--'+key+'='+val);\n            }\n        });\n        // --ensure=false tell node-gyp to re-install node development headers\n        // but it is only respected by node-gyp install, so we have to call install\n        // as a separate step if the user passes it\n        if (gyp.opts.ensure === false) {\n            var install_args = final_args.concat(['install','--ensure=false']);\n            compile.run_gyp(install_args,result.opts,function(err) {\n                if (err) return callback(err);\n                if (result.unparsed.length > 0) {\n                    final_args = final_args.\n                                  concat(['--']).\n                                  concat(result.unparsed);\n                }\n                compile.run_gyp(['configure'].concat(final_args),result.opts,function(err) {\n                    return callback(err);\n                });\n            });\n        } else {\n            if (result.unparsed.length > 0) {\n                final_args = final_args.\n                              concat(['--']).\n                              concat(result.unparsed);\n            }\n            compile.run_gyp(['configure'].concat(final_args),result.opts,function(err) {\n                if (!err && result.opts.napi_build_version) {\n                    napi.swap_build_dir_out(result.opts.napi_build_version);\n                }\n                return callback(err);\n            });\n        }\n    });\n}\n\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/configure.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/info.js":
/*!***********************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/info.js ***!
  \***********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = exports = unpublish;\n\nexports.usage = 'Lists all published binaries (requires aws-sdk)';\n\nvar fs = __webpack_require__(/*! fs */ \"fs\");\nvar log = __webpack_require__(/*! npmlog */ \"./node_modules/npmlog/log.js\");\nvar versioning = __webpack_require__(/*! ./util/versioning.js */ \"./node_modules/node-pre-gyp/lib/util/versioning.js\");\nvar s3_setup = __webpack_require__(/*! ./util/s3_setup.js */ \"./node_modules/node-pre-gyp/lib/util/s3_setup.js\");\nvar config = __webpack_require__(/*! rc */ \"./node_modules/rc/index.js\")(\"node_pre_gyp\",{acl:\"public-read\"});\n\nfunction unpublish(gyp, argv, callback) {\n    var AWS = __webpack_require__(!(function webpackMissingModule() { var e = new Error(\"Cannot find module 'aws-sdk'\"); e.code = 'MODULE_NOT_FOUND'; throw e; }()));\n    var package_json = JSON.parse(fs.readFileSync('./package.json'));\n    var opts = versioning.evaluate(package_json, gyp.opts);\n    s3_setup.detect(opts.hosted_path,config);\n    AWS.config.update(config);\n    var s3 =  new AWS.S3();\n    var s3_opts = {  Bucket: config.bucket,\n                     Prefix: config.prefix\n                  };\n    s3.listObjects(s3_opts, function(err, meta){\n        if (err && err.code == 'NotFound') {\n            return callback(new Error('['+package_json.name+'] Not found: https://' + s3_opts.Bucket + '.s3.amazonaws.com/'+config.prefix));\n        } else if(err) {\n            return callback(err);\n        } else {\n            log.verbose(JSON.stringify(meta,null,1));\n            if (meta && meta.Contents) {\n                meta.Contents.forEach(function(obj) {\n                    console.log(obj.Key);\n                });\n            } else {\n                console.error('['+package_json.name+'] No objects found at https://' + s3_opts.Bucket + '.s3.amazonaws.com/'+config.prefix );\n            }\n            return callback();\n        }\n    });\n}\n\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/info.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/install.js":
/*!**************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/install.js ***!
  \**************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/* WEBPACK VAR INJECTION */(function(__dirname) {\n\nmodule.exports = exports = install;\n\nexports.usage = 'Attempts to install pre-built binary for module';\n\nvar fs = __webpack_require__(/*! fs */ \"fs\");\nvar path = __webpack_require__(/*! path */ \"path\");\nvar log = __webpack_require__(/*! npmlog */ \"./node_modules/npmlog/log.js\");\nvar existsAsync = fs.exists || path.exists;\nvar versioning = __webpack_require__(/*! ./util/versioning.js */ \"./node_modules/node-pre-gyp/lib/util/versioning.js\");\nvar napi = __webpack_require__(/*! ./util/napi.js */ \"./node_modules/node-pre-gyp/lib/util/napi.js\");\nvar mkdirp = __webpack_require__(/*! mkdirp */ \"mkdirp\");\n\nvar npgVersion = 'unknown';\ntry {\n    // Read own package.json to get the current node-pre-pyp version.\n    var ownPackageJSON = fs.readFileSync(path.join(__dirname, '..', 'package.json'), 'utf8');\n    npgVersion = JSON.parse(ownPackageJSON).version;\n} catch (e) {}\n\nvar http_get = {\n    impl: undefined,\n    type: undefined\n};\n\ntry {\n  http_get.impl = __webpack_require__(!(function webpackMissingModule() { var e = new Error(\"Cannot find module 'request'\"); e.code = 'MODULE_NOT_FOUND'; throw e; }()));\n  http_get.type = 'request';\n  log.warn(\"Using request for node-pre-gyp https download\");\n} catch (e) {\n  http_get.impl = __webpack_require__(/*! needle */ \"./node_modules/needle/lib/needle.js\");\n  http_get.type = 'needle';\n  log.warn(\"Using needle for node-pre-gyp https download\");\n}\n\nfunction download(uri,opts,callback) {\n    log.http('GET', uri);\n\n    var req = null;\n\n    // Try getting version info from the currently running npm.\n    var envVersionInfo = process.env.npm_config_user_agent ||\n        'node ' + process.version;\n\n    var requestOpts = {\n        uri: uri.replace('+','%2B'),\n        headers: {\n          'User-Agent': 'node-pre-gyp (v' + npgVersion + ', ' + envVersionInfo + ')'\n        },\n        follow_max: 10,\n    };\n\n    if (opts.cafile) {\n        try {\n            requestOpts.ca = fs.readFileSync(opts.cafile);\n        } catch (e) {\n            return callback(e);\n        }\n    } else if (opts.ca) {\n        requestOpts.ca = opts.ca;\n    }\n\n    var proxyUrl = opts.proxy ||\n                    process.env.http_proxy ||\n                    process.env.HTTP_PROXY ||\n                    process.env.npm_config_proxy;\n    if (proxyUrl) {\n      if (/^https?:\\/\\//i.test(proxyUrl)) {\n        log.verbose('download', 'using proxy url: \"%s\"', proxyUrl);\n        requestOpts.proxy = proxyUrl;\n      } else {\n        log.warn('download', 'ignoring invalid \"proxy\" config setting: \"%s\"', proxyUrl);\n      }\n    }\n    try {\n        req = http_get.impl.get(requestOpts.uri, requestOpts);\n    } catch (e) {\n        return callback(e);\n    }\n    if (req) {\n      req.on('response', function (res) {\n        log.http(res.statusCode, uri);\n      });\n    }\n    return callback(null,req);\n}\n\nfunction place_binary(from,to,opts,callback) {\n    download(from,opts,function(err,req) {\n        if (err) return callback(err);\n        if (!req) return callback(new Error(\"empty req\"));\n        var badDownload = false;\n        var hasResponse = false;\n\n        function afterExtract(err, extractCount) {\n            if (err) return callback(err);\n            if (badDownload) return callback(new Error(\"bad download\"));\n            if (extractCount === 0) {\n                return callback(new Error('There was a fatal problem while downloading/extracting the tarball'));\n            }\n            log.info('tarball', 'done parsing tarball');\n            callback();\n        }\n\n        // for request compatibility\n        req.on('error', function(err) {\n            badDownload = true;\n            if (!hasResponse) {\n                hasResponse = true;\n                return callback(err);\n            }\n        });\n\n        // for needle compatibility\n        req.on('err', function(err) {\n            badDownload = true;\n            if (!hasResponse) {\n                hasResponse = true;\n                return callback(err);\n            }\n        });\n\n        req.on('close', function () {\n            if (!hasResponse) {\n                hasResponse = true;\n                return callback(new Error('Connection closed while downloading tarball file'));\n            }\n        });\n\n      req.on('response', function(res) {\n            // ignore redirects, needle handles these automatically.\n            if (http_get.type === 'needle' && res.headers.hasOwnProperty('location') && res.headers.location !== '') {\n                return;\n            }\n            if (hasResponse) {\n                return;\n            }\n            hasResponse = true;\n            if (res.statusCode !== 200) {\n                badDownload = true;\n                var err = new Error(res.statusCode + ' status code downloading tarball ' + from);\n                err.statusCode = res.statusCode;\n                return callback(err);\n            }\n            // start unzipping and untaring\n            req.pipe(extract(to, afterExtract));\n        });\n    });\n}\n\nfunction extract_from_local(from, to, callback) {\n    if (!fs.existsSync(from)) {\n        return callback(new Error('Cannot find file ' + from));\n    }\n    log.info('Found local file to extract from ' + from);\n    function afterExtract(err, extractCount) {\n        if (err) return callback(err);\n        if (extractCount === 0) {\n            return callback(new Error('There was a fatal problem while extracting the tarball'));\n        }\n        log.info('tarball', 'done parsing tarball');\n        callback();\n    }\n    fs.createReadStream(from).pipe(extract(to, afterExtract));\n}\n\nfunction extract(to, callback) {\n    var extractCount = 0;\n    function filter_func(entry) {\n        log.info('install','unpacking ' + entry.path);\n        extractCount++;\n    }\n\n    function afterTarball(err) {\n        callback(err, extractCount);\n    }\n\n    var tar = __webpack_require__(/*! tar */ \"./node_modules/tar/index.js\");\n    return tar.extract({\n        cwd: to,\n        strip: 1,\n        onentry: filter_func\n    }).on('close', afterTarball).on('error', callback);\n}\n\n\nfunction do_build(gyp,argv,callback) {\n  var args = ['rebuild'].concat(argv);\n  gyp.todo.push( { name: 'build', args: args } );\n  process.nextTick(callback);\n}\n\nfunction print_fallback_error(err,opts,package_json) {\n    var fallback_message = ' (falling back to source compile with node-gyp)';\n    var full_message = '';\n    if (err.statusCode !== undefined) {\n        // If we got a network response it but failed to download\n        // it means remote binaries are not available, so let's try to help\n        // the user/developer with the info to debug why\n        full_message = \"Pre-built binaries not found for \" + package_json.name + \"@\" + package_json.version;\n        full_message += \" and \" + opts.runtime + \"@\" + (opts.target || process.versions.node) + \" (\" + opts.node_abi + \" ABI, \" + opts.libc + \")\";\n        full_message += fallback_message;\n        log.warn(\"Tried to download(\" + err.statusCode + \"): \" + opts.hosted_tarball);\n        log.warn(full_message);\n        log.http(err.message);\n    } else {\n        // If we do not have a statusCode that means an unexpected error\n        // happened and prevented an http response, so we output the exact error\n        full_message = \"Pre-built binaries not installable for \" + package_json.name + \"@\" + package_json.version;\n        full_message += \" and \" + opts.runtime + \"@\" + (opts.target || process.versions.node) + \" (\" + opts.node_abi + \" ABI, \" + opts.libc + \")\";\n        full_message += fallback_message;\n        log.warn(full_message);\n        log.warn(\"Hit error \" + err.message);\n    }\n}\n\nfunction install(gyp, argv, callback) {\n    var package_json = JSON.parse(fs.readFileSync('./package.json'));\n\tvar napi_build_version = napi.get_napi_build_version_from_command_args(argv);\n    var source_build = gyp.opts['build-from-source'] || gyp.opts.build_from_source;\n    var update_binary = gyp.opts['update-binary'] || gyp.opts.update_binary;\n    var should_do_source_build = source_build === package_json.name || (source_build === true || source_build === 'true');\n    if (should_do_source_build) {\n        log.info('build','requesting source compile');\n        return do_build(gyp,argv,callback);\n    } else {\n        var fallback_to_build = gyp.opts['fallback-to-build'] || gyp.opts.fallback_to_build;\n        var should_do_fallback_build = fallback_to_build === package_json.name || (fallback_to_build === true || fallback_to_build === 'true');\n        // but allow override from npm\n        if (process.env.npm_config_argv) {\n            var cooked = JSON.parse(process.env.npm_config_argv).cooked;\n            var match = cooked.indexOf(\"--fallback-to-build\");\n            if (match > -1 && cooked.length > match && cooked[match+1] == \"false\") {\n                should_do_fallback_build = false;\n                log.info('install','Build fallback disabled via npm flag: --fallback-to-build=false');\n            }\n        }\n        var opts;\n        try {\n            opts = versioning.evaluate(package_json, gyp.opts, napi_build_version);\n        } catch (err) {\n            return callback(err);\n        }\n\n        opts.ca = gyp.opts.ca;\n        opts.cafile = gyp.opts.cafile;\n\n        var from = opts.hosted_tarball;\n        var to = opts.module_path;\n        var binary_module = path.join(to,opts.module_name + '.node');\n        existsAsync(binary_module,function(found) {\n            if (found && !update_binary) {\n                console.log('['+package_json.name+'] Success: \"' + binary_module + '\" already installed');\n                console.log('Pass --update-binary to reinstall or --build-from-source to recompile');\n                return callback();\n            } else {\n                if (!update_binary) log.info('check','checked for \"' + binary_module + '\" (not found)');\n                mkdirp(to,function(err) {\n                    if (err) {\n                        after_place(err);\n                    } else {\n                        var fileName = from.startsWith('file://') && from.replace(/^file:\\/\\//, '');\n                        if (fileName) {\n                            extract_from_local(fileName, to, after_place);\n                        } else {\n                            place_binary(from,to,opts,after_place);\n                        }\n                    }\n                });\n            }\n            function after_place(err) {\n                if (err && should_do_fallback_build) {\n                    print_fallback_error(err,opts,package_json);\n                    return do_build(gyp,argv,callback);\n                } else if (err) {\n                    return callback(err);\n                } else {\n                    console.log('['+package_json.name+'] Success: \"' + binary_module + '\" is installed via remote');\n                    return callback();\n                }\n            }\n        });\n    }\n}\n\n/* WEBPACK VAR INJECTION */}.call(this, \"/\"))\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/install.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/node-pre-gyp.js":
/*!*******************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/node-pre-gyp.js ***!
  \*******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/* WEBPACK VAR INJECTION */(function(__dirname) {\n\n/**\n * Module exports.\n */\n\nmodule.exports = exports;\n\n/**\n * Module dependencies.\n */\n\nvar fs = __webpack_require__(/*! fs */ \"fs\");\nvar path = __webpack_require__(/*! path */ \"path\");\nvar nopt = __webpack_require__(/*! nopt */ \"./node_modules/nopt/lib/nopt.js\");\nvar log = __webpack_require__(/*! npmlog */ \"./node_modules/npmlog/log.js\");\nlog.disableProgress();\nvar napi = __webpack_require__(/*! ./util/napi.js */ \"./node_modules/node-pre-gyp/lib/util/napi.js\");\n\nvar EE = __webpack_require__(/*! events */ \"events\").EventEmitter;\nvar inherits = __webpack_require__(/*! util */ \"util\").inherits;\nvar commands = [\n      'clean',\n      'install',\n      'reinstall',\n      'build',\n      'rebuild',\n      'package',\n      'testpackage',\n      'publish',\n      'unpublish',\n      'info',\n      'testbinary',\n      'reveal',\n      'configure'\n    ];\nvar aliases = {};\n\n// differentiate node-pre-gyp's logs from npm's\nlog.heading = 'node-pre-gyp';\n\nexports.find = __webpack_require__(/*! ./pre-binding */ \"./node_modules/node-pre-gyp/lib/pre-binding.js\").find;\n\nfunction Run() {\n  var self = this;\n\n  this.commands = {};\n\n  commands.forEach(function (command) {\n    self.commands[command] = function (argv, callback) {\n      log.verbose('command', command, argv);\n      return __webpack_require__(\"./node_modules/node-pre-gyp/lib sync recursive ^\\\\.\\\\/.*$\")(\"./\" + command)(self, argv, callback);\n    };\n  });\n}\ninherits(Run, EE);\nexports.Run = Run;\nvar proto = Run.prototype;\n\n/**\n * Export the contents of the package.json.\n */\n\nproto.package = __webpack_require__(/*! ../package.json */ \"./node_modules/node-pre-gyp/package.json\");\n\n/**\n * nopt configuration definitions\n */\n\nproto.configDefs = {\n    help: Boolean,     // everywhere\n    arch: String,      // 'configure'\n    debug: Boolean,    // 'build'\n    directory: String, // bin\n    proxy: String,     // 'install'\n    loglevel: String,  // everywhere\n};\n\n/**\n * nopt shorthands\n */\n\nproto.shorthands = {\n    release: '--no-debug',\n    C: '--directory',\n    debug: '--debug',\n    j: '--jobs',\n    silent: '--loglevel=silent',\n    silly: '--loglevel=silly',\n    verbose: '--loglevel=verbose',\n};\n\n/**\n * expose the command aliases for the bin file to use.\n */\n\nproto.aliases = aliases;\n\n/**\n * Parses the given argv array and sets the 'opts',\n * 'argv' and 'command' properties.\n */\n\nproto.parseArgv = function parseOpts (argv) {\n  this.opts = nopt(this.configDefs, this.shorthands, argv);\n  this.argv = this.opts.argv.remain.slice();\n  var commands = this.todo = [];\n\n  // create a copy of the argv array with aliases mapped\n  argv = this.argv.map(function (arg) {\n    // is this an alias?\n    if (arg in this.aliases) {\n      arg = this.aliases[arg];\n    }\n    return arg;\n  }, this);\n\n  // process the mapped args into \"command\" objects (\"name\" and \"args\" props)\n  argv.slice().forEach(function (arg) {\n    if (arg in this.commands) {\n      var args = argv.splice(0, argv.indexOf(arg));\n      argv.shift();\n      if (commands.length > 0) {\n        commands[commands.length - 1].args = args;\n      }\n      commands.push({ name: arg, args: [] });\n    }\n  }, this);\n  if (commands.length > 0) {\n    commands[commands.length - 1].args = argv.splice(0);\n  }\n\n  // expand commands entries for multiple napi builds\n  var dir = this.opts.directory;\n  if (dir == null) dir = process.cwd();\n  var package_json = JSON.parse(fs.readFileSync(path.join(dir,'package.json')));\n\n  this.todo = napi.expand_commands (package_json, this.opts, commands);\n\n  // support for inheriting config env variables from npm\n  var npm_config_prefix = 'npm_config_';\n  Object.keys(process.env).forEach(function (name) {\n    if (name.indexOf(npm_config_prefix) !== 0) return;\n    var val = process.env[name];\n    if (name === npm_config_prefix + 'loglevel') {\n      log.level = val;\n    } else {\n      // add the user-defined options to the config\n      name = name.substring(npm_config_prefix.length);\n      // avoid npm argv clobber already present args\n      // which avoids problem of 'npm test' calling\n      // script that runs unique npm install commands\n      if (name === 'argv') {\n         if (this.opts.argv &&\n             this.opts.argv.remain &&\n             this.opts.argv.remain.length) {\n            // do nothing\n         } else {\n            this.opts[name] = val;\n         }\n      } else {\n        this.opts[name] = val;\n      }\n    }\n  }, this);\n\n  if (this.opts.loglevel) {\n    log.level = this.opts.loglevel;\n  }\n  log.resume();\n};\n\n/**\n * Returns the usage instructions for node-pre-gyp.\n */\n\nproto.usage = function usage () {\n  var str = [\n      '',\n      '  Usage: node-pre-gyp <command> [options]',\n      '',\n      '  where <command> is one of:',\n      commands.map(function (c) {\n        return '    - ' + c + ' - ' + __webpack_require__(\"./node_modules/node-pre-gyp/lib sync recursive ^\\\\.\\\\/.*$\")(\"./\" + c).usage;\n      }).join('\\n'),\n      '',\n      'node-pre-gyp@' + this.version + '  ' + path.resolve(__dirname, '..'),\n      'node@' + process.versions.node\n  ].join('\\n');\n  return str;\n};\n\n/**\n * Version number getter.\n */\n\nObject.defineProperty(proto, 'version', {\n    get: function () {\n      return this.package.version;\n    },\n    enumerable: true\n});\n\n\n/* WEBPACK VAR INJECTION */}.call(this, \"/\"))\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/node-pre-gyp.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/package.js":
/*!**************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/package.js ***!
  \**************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = exports = _package;\n\nexports.usage = 'Packs binary (and enclosing directory) into locally staged tarball';\n\nvar fs = __webpack_require__(/*! fs */ \"fs\");\nvar path = __webpack_require__(/*! path */ \"path\");\nvar log = __webpack_require__(/*! npmlog */ \"./node_modules/npmlog/log.js\");\nvar versioning = __webpack_require__(/*! ./util/versioning.js */ \"./node_modules/node-pre-gyp/lib/util/versioning.js\");\nvar napi = __webpack_require__(/*! ./util/napi.js */ \"./node_modules/node-pre-gyp/lib/util/napi.js\");\nvar write = __webpack_require__(/*! fs */ \"fs\").createWriteStream;\nvar existsAsync = fs.exists || path.exists;\nvar mkdirp = __webpack_require__(/*! mkdirp */ \"mkdirp\");\nvar tar = __webpack_require__(/*! tar */ \"./node_modules/tar/index.js\");\n\nfunction _package(gyp, argv, callback) {\n    var packlist = __webpack_require__(/*! npm-packlist */ \"./node_modules/npm-packlist/index.js\");\n    var package_json = JSON.parse(fs.readFileSync('./package.json'));\n    var napi_build_version = napi.get_napi_build_version_from_command_args(argv);\n    var opts = versioning.evaluate(package_json, gyp.opts, napi_build_version);\n    var from = opts.module_path;\n    var binary_module = path.join(from,opts.module_name + '.node');\n    existsAsync(binary_module,function(found) {\n        if (!found) {\n            return callback(new Error(\"Cannot package because \" + binary_module + \" missing: run `node-pre-gyp rebuild` first\"));\n        }\n        var tarball = opts.staged_tarball;\n        var filter_func = function(entry) {\n            // ensure directories are +x\n            // https://github.com/mapnik/node-mapnik/issues/262\n            log.info('package','packing ' + entry.path);\n            return true;\n        };\n        mkdirp(path.dirname(tarball),function(err) {\n            if (err) return callback(err);\n            packlist({ path: from }).then(function(files) {\n                var base = path.basename(from);\n                files = files.map(function(file) {\n                    return path.join(base, file);\n                });\n                tar.create({\n                    portable: true,\n                    gzip: true,\n                    onentry: filter_func,\n                    file: tarball,\n                    cwd: path.dirname(from)\n                }, files, function(err) {\n                    if (err)  console.error('['+package_json.name+'] ' + err.message);\n                    else log.info('package','Binary staged at \"' + tarball + '\"');\n                    return callback(err);\n                });\n            }, callback);\n        });\n    });\n}\n\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/package.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/pre-binding.js":
/*!******************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/pre-binding.js ***!
  \******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nvar versioning = __webpack_require__(/*! ../lib/util/versioning.js */ \"./node_modules/node-pre-gyp/lib/util/versioning.js\");\nvar napi = __webpack_require__(/*! ../lib/util/napi.js */ \"./node_modules/node-pre-gyp/lib/util/napi.js\");\nvar existsSync = __webpack_require__(/*! fs */ \"fs\").existsSync || __webpack_require__(/*! path */ \"path\").existsSync;\nvar path = __webpack_require__(/*! path */ \"path\");\n\nmodule.exports = exports;\n\nexports.usage = 'Finds the require path for the node-pre-gyp installed module';\n\nexports.validate = function(package_json,opts) {\n    versioning.validate_config(package_json,opts);\n};\n\nexports.find = function(package_json_path,opts) {\n   if (!existsSync(package_json_path)) {\n        throw new Error(\"package.json does not exist at \" + package_json_path);\n   }\n   var package_json = __webpack_require__(\"./node_modules/node-pre-gyp/lib sync recursive\")(package_json_path);\n   versioning.validate_config(package_json,opts);\n   var napi_build_version;\n   if (napi.get_napi_build_versions (package_json, opts)) {\n       napi_build_version = napi.get_best_napi_build_version(package_json, opts);\n   }\n   opts = opts || {};\n   if (!opts.module_root) opts.module_root = path.dirname(package_json_path);\n   var meta = versioning.evaluate(package_json,opts,napi_build_version);\n   return meta.module;\n};\n\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/pre-binding.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/publish.js":
/*!**************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/publish.js ***!
  \**************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = exports = publish;\n\nexports.usage = 'Publishes pre-built binary (requires aws-sdk)';\n\nvar fs = __webpack_require__(/*! fs */ \"fs\");\nvar path = __webpack_require__(/*! path */ \"path\");\nvar log = __webpack_require__(/*! npmlog */ \"./node_modules/npmlog/log.js\");\nvar versioning = __webpack_require__(/*! ./util/versioning.js */ \"./node_modules/node-pre-gyp/lib/util/versioning.js\");\nvar napi = __webpack_require__(/*! ./util/napi.js */ \"./node_modules/node-pre-gyp/lib/util/napi.js\");\nvar s3_setup = __webpack_require__(/*! ./util/s3_setup.js */ \"./node_modules/node-pre-gyp/lib/util/s3_setup.js\");\nvar existsAsync = fs.exists || path.exists;\nvar url = __webpack_require__(/*! url */ \"url\");\nvar config = __webpack_require__(/*! rc */ \"./node_modules/rc/index.js\")(\"node_pre_gyp\",{acl:\"public-read\"});\n\nfunction publish(gyp, argv, callback) {\n    var AWS = __webpack_require__(!(function webpackMissingModule() { var e = new Error(\"Cannot find module 'aws-sdk'\"); e.code = 'MODULE_NOT_FOUND'; throw e; }()));\n    var package_json = JSON.parse(fs.readFileSync('./package.json'));\n    var napi_build_version = napi.get_napi_build_version_from_command_args(argv);\n    var opts = versioning.evaluate(package_json, gyp.opts, napi_build_version);\n    var tarball = opts.staged_tarball;\n    existsAsync(tarball,function(found) {\n        if (!found) {\n            return callback(new Error(\"Cannot publish because \" + tarball + \" missing: run `node-pre-gyp package` first\"));\n        }\n        log.info('publish', 'Detecting s3 credentials');\n        s3_setup.detect(opts.hosted_path,config);\n        var key_name = url.resolve(config.prefix,opts.package_name);\n        log.info('publish', 'Authenticating with s3');\n        AWS.config.update(config);\n        var s3 =  new AWS.S3();\n        var s3_opts = {  Bucket: config.bucket,\n                         Key: key_name\n                      };\n        var remote_package = 'https://' + s3_opts.Bucket + '.s3.amazonaws.com/' + s3_opts.Key;\n        log.info('publish', 'Checking for existing binary at ' + remote_package);\n        s3.headObject(s3_opts, function(err, meta){\n            if (meta) log.info('publish', JSON.stringify(meta));\n            if (err && err.code == 'NotFound') {\n                // we are safe to publish because\n                // the object does not already exist\n                log.info('publish', 'Preparing to put object');\n                var s3_put = new AWS.S3();\n                var s3_put_opts = {  ACL: config.acl,\n                                     Body: fs.createReadStream(tarball),\n                                     Bucket: config.bucket,\n                                     Key: key_name\n                                  };\n                log.info('publish', 'Putting object');\n                try {\n                    s3_put.putObject(s3_put_opts, function(err, resp){\n                        log.info('publish', 'returned from putting object');\n                        if(err) {\n                           log.info('publish', 's3 putObject error: \"' + err + '\"');\n                           return callback(err);\n                        }\n                        if (resp) log.info('publish', 's3 putObject response: \"' + JSON.stringify(resp) + '\"');\n                        log.info('publish', 'successfully put object');\n                        console.log('['+package_json.name+'] published to ' + remote_package);\n                        return callback();\n                    });\n              } catch (err) {\n                   log.info('publish', 's3 putObject error: \"' + err + '\"');\n                   return callback(err);\n              }\n            } else if (err) {\n                log.info('publish', 's3 headObject error: \"' + err + '\"');\n                return callback(err);\n            } else {\n                log.error('publish','Cannot publish over existing version');\n                log.error('publish',\"Update the 'version' field in package.json and try again\");\n                log.error('publish','If the previous version was published in error see:');\n                log.error('publish','\\t node-pre-gyp unpublish');\n                return callback(new Error('Failed publishing to ' + remote_package));\n            }\n        });\n    });\n}\n\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/publish.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/rebuild.js":
/*!**************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/rebuild.js ***!
  \**************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = exports = rebuild;\n\nexports.usage = 'Runs \"clean\" and \"build\" at once';\n\nvar fs = __webpack_require__(/*! fs */ \"fs\");\nvar napi = __webpack_require__(/*! ./util/napi.js */ \"./node_modules/node-pre-gyp/lib/util/napi.js\");\n\nfunction rebuild (gyp, argv, callback) {\n  var package_json = JSON.parse(fs.readFileSync('./package.json'));\n  var commands = [\n    { name: 'clean', args: [] },\n    { name: 'build', args: ['rebuild'] }\n    ];\n  commands = napi.expand_commands(package_json, gyp.opts, commands);\n  for (var i = commands.length; i !== 0; i--) {\n    gyp.todo.unshift(commands[i-1]);\n  }\n  process.nextTick(callback);\n}\n\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/rebuild.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/reinstall.js":
/*!****************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/reinstall.js ***!
  \****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = exports = rebuild;\n\nexports.usage = 'Runs \"clean\" and \"install\" at once';\n\nvar fs = __webpack_require__(/*! fs */ \"fs\");\nvar napi = __webpack_require__(/*! ./util/napi.js */ \"./node_modules/node-pre-gyp/lib/util/napi.js\");\n\nfunction rebuild (gyp, argv, callback) {\n  var package_json = JSON.parse(fs.readFileSync('./package.json'));\n  var installArgs = [];\n  var napi_build_version = napi.get_best_napi_build_version(package_json, gyp.opts);\n  if (napi_build_version != null) installArgs = [ napi.get_command_arg (napi_build_version) ];\n  gyp.todo.unshift(\n      { name: 'clean', args: [] },\n      { name: 'install', args: installArgs }\n  );\n  process.nextTick(callback);\n}\n\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/reinstall.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/reveal.js":
/*!*************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/reveal.js ***!
  \*************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = exports = reveal;\n\nexports.usage = 'Reveals data on the versioned binary';\n\nvar fs = __webpack_require__(/*! fs */ \"fs\");\nvar versioning = __webpack_require__(/*! ./util/versioning.js */ \"./node_modules/node-pre-gyp/lib/util/versioning.js\");\nvar napi = __webpack_require__(/*! ./util/napi.js */ \"./node_modules/node-pre-gyp/lib/util/napi.js\");\n\nfunction unix_paths(key, val) {\n    return val && val.replace ? val.replace(/\\\\/g, '/') : val;\n}\n\nfunction reveal(gyp, argv, callback) {\n    var package_json = JSON.parse(fs.readFileSync('./package.json'));\n    var napi_build_version = napi.get_napi_build_version_from_command_args(argv);\n    var opts = versioning.evaluate(package_json, gyp.opts, napi_build_version);\n    var hit = false;\n    // if a second arg is passed look to see\n    // if it is a known option\n    //console.log(JSON.stringify(gyp.opts,null,1))\n    var remain = gyp.opts.argv.remain[gyp.opts.argv.remain.length-1];\n    if (remain && opts.hasOwnProperty(remain)) {\n        console.log(opts[remain].replace(/\\\\/g, '/'));\n        hit = true;\n    }\n    // otherwise return all options as json\n    if (!hit) {\n        console.log(JSON.stringify(opts,unix_paths,2));\n    }\n    return callback();\n}\n\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/reveal.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/testbinary.js":
/*!*****************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/testbinary.js ***!
  \*****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("/* WEBPACK VAR INJECTION */(function(__dirname) {\n\nmodule.exports = exports = testbinary;\n\nexports.usage = 'Tests that the binary.node can be required';\n\nvar fs = __webpack_require__(/*! fs */ \"fs\");\nvar path = __webpack_require__(/*! path */ \"path\");\nvar log = __webpack_require__(/*! npmlog */ \"./node_modules/npmlog/log.js\");\nvar cp = __webpack_require__(/*! child_process */ \"child_process\");\nvar versioning = __webpack_require__(/*! ./util/versioning.js */ \"./node_modules/node-pre-gyp/lib/util/versioning.js\");\nvar napi = __webpack_require__(/*! ./util/napi.js */ \"./node_modules/node-pre-gyp/lib/util/napi.js\");\nvar path = __webpack_require__(/*! path */ \"path\");\n\nfunction testbinary(gyp, argv, callback) {\n    var args = [];\n    var options = {};\n    var shell_cmd = process.execPath;\n    var package_json = JSON.parse(fs.readFileSync('./package.json'));\n    var napi_build_version = napi.get_napi_build_version_from_command_args(argv);\n    var opts = versioning.evaluate(package_json, gyp.opts, napi_build_version);\n    // skip validation for runtimes we don't explicitly support (like electron)\n    if (opts.runtime &&\n        opts.runtime !== 'node-webkit' &&\n        opts.runtime !== 'node') {\n        return callback();\n    }\n    var nw = (opts.runtime && opts.runtime === 'node-webkit');\n    // ensure on windows that / are used for require path\n    var binary_module = opts.module.replace(/\\\\/g, '/');\n    if ((process.arch != opts.target_arch) ||\n        (process.platform != opts.target_platform)) {\n        var msg = \"skipping validation since host platform/arch (\";\n        msg += process.platform+'/'+process.arch+\")\";\n        msg += \" does not match target (\";\n        msg += opts.target_platform+'/'+opts.target_arch+\")\";\n        log.info('validate', msg);\n        return callback();\n    }\n    if (nw) {\n        options.timeout = 5000;\n        if (process.platform === 'darwin') {\n            shell_cmd = 'node-webkit';\n        } else if (process.platform === 'win32') {\n            shell_cmd = 'nw.exe';\n        } else {\n            shell_cmd = 'nw';\n        }\n        var modulePath = path.resolve(binary_module);\n        var appDir = path.join(__dirname, 'util', 'nw-pre-gyp');\n        args.push(appDir);\n        args.push(modulePath);\n        log.info(\"validate\",\"Running test command: '\" + shell_cmd + ' ' + args.join(' ') + \"'\");\n        cp.execFile(shell_cmd, args, options, function(err, stdout, stderr) {\n            // check for normal timeout for node-webkit\n            if (err) {\n                if (err.killed === true && err.signal && err.signal.indexOf('SIG') > -1) {\n                    return callback();\n                }\n                var stderrLog = stderr.toString();\n                log.info('stderr', stderrLog);\n                if( /^\\s*Xlib:\\s*extension\\s*\"RANDR\"\\s*missing\\s*on\\s*display\\s*\":\\d+\\.\\d+\"\\.\\s*$/.test(stderrLog) ){\n                    log.info('RANDR', 'stderr contains only RANDR error, ignored');\n                    return callback();\n                }\n                return callback(err);\n            }\n            return callback();\n        });\n        return;\n    }\n    args.push('--eval');\n    args.push(\"require('\" + binary_module.replace(/'/g, '\\'') +\"')\");\n    log.info(\"validate\",\"Running test command: '\" + shell_cmd + ' ' + args.join(' ') + \"'\");\n    cp.execFile(shell_cmd, args, options, function(err, stdout, stderr) {\n        if (err) {\n            return callback(err, { stdout:stdout, stderr:stderr});\n        }\n        return callback();\n    });\n}\n\n/* WEBPACK VAR INJECTION */}.call(this, \"/\"))\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/testbinary.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/testpackage.js":
/*!******************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/testpackage.js ***!
  \******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = exports = testpackage;\n\nexports.usage = 'Tests that the staged package is valid';\n\nvar fs = __webpack_require__(/*! fs */ \"fs\");\nvar path = __webpack_require__(/*! path */ \"path\");\nvar log = __webpack_require__(/*! npmlog */ \"./node_modules/npmlog/log.js\");\nvar existsAsync = fs.exists || path.exists;\nvar versioning = __webpack_require__(/*! ./util/versioning.js */ \"./node_modules/node-pre-gyp/lib/util/versioning.js\");\nvar napi = __webpack_require__(/*! ./util/napi.js */ \"./node_modules/node-pre-gyp/lib/util/napi.js\");\nvar testbinary = __webpack_require__(/*! ./testbinary.js */ \"./node_modules/node-pre-gyp/lib/testbinary.js\");\nvar tar = __webpack_require__(/*! tar */ \"./node_modules/tar/index.js\");\nvar mkdirp = __webpack_require__(/*! mkdirp */ \"mkdirp\");\n\nfunction testpackage(gyp, argv, callback) {\n    var package_json = JSON.parse(fs.readFileSync('./package.json'));\n    var napi_build_version = napi.get_napi_build_version_from_command_args(argv);\n    var opts = versioning.evaluate(package_json, gyp.opts, napi_build_version);\n    var tarball = opts.staged_tarball;\n    existsAsync(tarball, function(found) {\n        if (!found) {\n            return callback(new Error(\"Cannot test package because \" + tarball + \" missing: run `node-pre-gyp package` first\"));\n        }\n        var to = opts.module_path;\n        function filter_func(entry) {\n            log.info('install','unpacking [' + entry.path + ']');\n        }\n\n        mkdirp(to, function(err) {\n            if (err) {\n                return callback(err);\n            } else {\n                tar.extract({\n                    file: tarball,\n                    cwd: to,\n                    strip: 1,\n                    onentry: filter_func\n                }).then(after_extract, callback);\n            }\n        });\n\n        function after_extract() {\n            testbinary(gyp,argv,function(err) {\n                if (err) {\n                    return callback(err);\n                } else {\n                    console.log('['+package_json.name+'] Package appears valid');\n                    return callback();\n                }\n            });\n        }\n    });\n}\n\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/testpackage.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/unpublish.js":
/*!****************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/unpublish.js ***!
  \****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = exports = unpublish;\n\nexports.usage = 'Unpublishes pre-built binary (requires aws-sdk)';\n\nvar fs = __webpack_require__(/*! fs */ \"fs\");\nvar log = __webpack_require__(/*! npmlog */ \"./node_modules/npmlog/log.js\");\nvar versioning = __webpack_require__(/*! ./util/versioning.js */ \"./node_modules/node-pre-gyp/lib/util/versioning.js\");\nvar napi = __webpack_require__(/*! ./util/napi.js */ \"./node_modules/node-pre-gyp/lib/util/napi.js\");\nvar s3_setup = __webpack_require__(/*! ./util/s3_setup.js */ \"./node_modules/node-pre-gyp/lib/util/s3_setup.js\");\nvar url = __webpack_require__(/*! url */ \"url\");\nvar config = __webpack_require__(/*! rc */ \"./node_modules/rc/index.js\")(\"node_pre_gyp\",{acl:\"public-read\"});\n\nfunction unpublish(gyp, argv, callback) {\n    var AWS = __webpack_require__(!(function webpackMissingModule() { var e = new Error(\"Cannot find module 'aws-sdk'\"); e.code = 'MODULE_NOT_FOUND'; throw e; }()));\n    var package_json = JSON.parse(fs.readFileSync('./package.json'));\n    var napi_build_version = napi.get_napi_build_version_from_command_args(argv);\n    var opts = versioning.evaluate(package_json, gyp.opts, napi_build_version);\n    s3_setup.detect(opts.hosted_path,config);\n    AWS.config.update(config);\n    var key_name = url.resolve(config.prefix,opts.package_name);\n    var s3 =  new AWS.S3();\n    var s3_opts = {  Bucket: config.bucket,\n                     Key: key_name\n                  };\n    s3.headObject(s3_opts, function(err, meta) {\n        if (err && err.code == 'NotFound') {\n            console.log('['+package_json.name+'] Not found: https://' + s3_opts.Bucket + '.s3.amazonaws.com/' + s3_opts.Key);\n            return callback();\n        } else if(err) {\n            return callback(err);\n        } else {\n            log.info('unpublish', JSON.stringify(meta));\n            s3.deleteObject(s3_opts, function(err, resp) {\n                if (err) return callback(err);\n                log.info(JSON.stringify(resp));\n                console.log('['+package_json.name+'] Success: removed https://' + s3_opts.Bucket + '.s3.amazonaws.com/' + s3_opts.Key);\n                return callback();\n            });\n        }\n    });\n}\n\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/unpublish.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/util sync recursive":
/*!*************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/util sync ***!
  \*************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("function webpackEmptyContext(req) {\n\tvar e = new Error(\"Cannot find module '\" + req + \"'\");\n\te.code = 'MODULE_NOT_FOUND';\n\tthrow e;\n}\nwebpackEmptyContext.keys = function() { return []; };\nwebpackEmptyContext.resolve = webpackEmptyContext;\nmodule.exports = webpackEmptyContext;\nwebpackEmptyContext.id = \"./node_modules/node-pre-gyp/lib/util sync recursive\";\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/util_sync?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/util/abi_crosswalk.json":
/*!***************************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/util/abi_crosswalk.json ***!
  \***************************************************************/
/*! exports provided: 0.1.14, 0.1.15, 0.1.16, 0.1.17, 0.1.18, 0.1.19, 0.1.20, 0.1.21, 0.1.22, 0.1.23, 0.1.24, 0.1.25, 0.1.26, 0.1.27, 0.1.28, 0.1.29, 0.1.30, 0.1.31, 0.1.32, 0.1.33, 0.1.90, 0.1.91, 0.1.92, 0.1.93, 0.1.94, 0.1.95, 0.1.96, 0.1.97, 0.1.98, 0.1.99, 0.1.100, 0.1.101, 0.1.102, 0.1.103, 0.1.104, 0.2.0, 0.2.1, 0.2.2, 0.2.3, 0.2.4, 0.2.5, 0.2.6, 0.3.0, 0.3.1, 0.3.2, 0.3.3, 0.3.4, 0.3.5, 0.3.6, 0.3.7, 0.3.8, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.6, 0.4.7, 0.4.8, 0.4.9, 0.4.10, 0.4.11, 0.4.12, 0.5.0, 0.5.1, 0.5.2, 0.5.3, 0.5.4, 0.5.5, 0.5.6, 0.5.7, 0.5.8, 0.5.9, 0.5.10, 0.6.0, 0.6.1, 0.6.2, 0.6.3, 0.6.4, 0.6.5, 0.6.6, 0.6.7, 0.6.8, 0.6.9, 0.6.10, 0.6.11, 0.6.12, 0.6.13, 0.6.14, 0.6.15, 0.6.16, 0.6.17, 0.6.18, 0.6.19, 0.6.20, 0.6.21, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.7.9, 0.7.10, 0.7.11, 0.7.12, 0.8.0, 0.8.1, 0.8.2, 0.8.3, 0.8.4, 0.8.5, 0.8.6, 0.8.7, 0.8.8, 0.8.9, 0.8.10, 0.8.11, 0.8.12, 0.8.13, 0.8.14, 0.8.15, 0.8.16, 0.8.17, 0.8.18, 0.8.19, 0.8.20, 0.8.21, 0.8.22, 0.8.23, 0.8.24, 0.8.25, 0.8.26, 0.8.27, 0.8.28, 0.9.0, 0.9.1, 0.9.2, 0.9.3, 0.9.4, 0.9.5, 0.9.6, 0.9.7, 0.9.8, 0.9.9, 0.9.10, 0.9.11, 0.9.12, 0.10.0, 0.10.1, 0.10.2, 0.10.3, 0.10.4, 0.10.5, 0.10.6, 0.10.7, 0.10.8, 0.10.9, 0.10.10, 0.10.11, 0.10.12, 0.10.13, 0.10.14, 0.10.15, 0.10.16, 0.10.17, 0.10.18, 0.10.19, 0.10.20, 0.10.21, 0.10.22, 0.10.23, 0.10.24, 0.10.25, 0.10.26, 0.10.27, 0.10.28, 0.10.29, 0.10.30, 0.10.31, 0.10.32, 0.10.33, 0.10.34, 0.10.35, 0.10.36, 0.10.37, 0.10.38, 0.10.39, 0.10.40, 0.10.41, 0.10.42, 0.10.43, 0.10.44, 0.10.45, 0.10.46, 0.10.47, 0.10.48, 0.11.0, 0.11.1, 0.11.2, 0.11.3, 0.11.4, 0.11.5, 0.11.6, 0.11.7, 0.11.8, 0.11.9, 0.11.10, 0.11.11, 0.11.12, 0.11.13, 0.11.14, 0.11.15, 0.11.16, 0.12.0, 0.12.1, 0.12.2, 0.12.3, 0.12.4, 0.12.5, 0.12.6, 0.12.7, 0.12.8, 0.12.9, 0.12.10, 0.12.11, 0.12.12, 0.12.13, 0.12.14, 0.12.15, 0.12.16, 0.12.17, 0.12.18, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.1.0, 1.2.0, 1.3.0, 1.4.1, 1.4.2, 1.4.3, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.6.3, 1.6.4, 1.7.1, 1.8.1, 1.8.2, 1.8.3, 1.8.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0, 2.2.0, 2.2.1, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.5.0, 3.0.0, 3.1.0, 3.2.0, 3.3.0, 3.3.1, 4.0.0, 4.1.0, 4.1.1, 4.1.2, 4.2.0, 4.2.1, 4.2.2, 4.2.3, 4.2.4, 4.2.5, 4.2.6, 4.3.0, 4.3.1, 4.3.2, 4.4.0, 4.4.1, 4.4.2, 4.4.3, 4.4.4, 4.4.5, 4.4.6, 4.4.7, 4.5.0, 4.6.0, 4.6.1, 4.6.2, 4.7.0, 4.7.1, 4.7.2, 4.7.3, 4.8.0, 4.8.1, 4.8.2, 4.8.3, 4.8.4, 4.8.5, 4.8.6, 4.8.7, 4.9.0, 4.9.1, 5.0.0, 5.1.0, 5.1.1, 5.2.0, 5.3.0, 5.4.0, 5.4.1, 5.5.0, 5.6.0, 5.7.0, 5.7.1, 5.8.0, 5.9.0, 5.9.1, 5.10.0, 5.10.1, 5.11.0, 5.11.1, 5.12.0, 6.0.0, 6.1.0, 6.2.0, 6.2.1, 6.2.2, 6.3.0, 6.3.1, 6.4.0, 6.5.0, 6.6.0, 6.7.0, 6.8.0, 6.8.1, 6.9.0, 6.9.1, 6.9.2, 6.9.3, 6.9.4, 6.9.5, 6.10.0, 6.10.1, 6.10.2, 6.10.3, 6.11.0, 6.11.1, 6.11.2, 6.11.3, 6.11.4, 6.11.5, 6.12.0, 6.12.1, 6.12.2, 6.12.3, 6.13.0, 6.13.1, 6.14.0, 6.14.1, 6.14.2, 6.14.3, 6.14.4, 6.15.0, 6.15.1, 6.16.0, 6.17.0, 6.17.1, 7.0.0, 7.1.0, 7.2.0, 7.2.1, 7.3.0, 7.4.0, 7.5.0, 7.6.0, 7.7.0, 7.7.1, 7.7.2, 7.7.3, 7.7.4, 7.8.0, 7.9.0, 7.10.0, 7.10.1, 8.0.0, 8.1.0, 8.1.1, 8.1.2, 8.1.3, 8.1.4, 8.2.0, 8.2.1, 8.3.0, 8.4.0, 8.5.0, 8.6.0, 8.7.0, 8.8.0, 8.8.1, 8.9.0, 8.9.1, 8.9.2, 8.9.3, 8.9.4, 8.10.0, 8.11.0, 8.11.1, 8.11.2, 8.11.3, 8.11.4, 8.12.0, 8.13.0, 8.14.0, 8.14.1, 8.15.0, 8.15.1, 8.16.0, 8.16.1, 8.16.2, 8.17.0, 9.0.0, 9.1.0, 9.2.0, 9.2.1, 9.3.0, 9.4.0, 9.5.0, 9.6.0, 9.6.1, 9.7.0, 9.7.1, 9.8.0, 9.9.0, 9.10.0, 9.10.1, 9.11.0, 9.11.1, 9.11.2, 10.0.0, 10.1.0, 10.2.0, 10.2.1, 10.3.0, 10.4.0, 10.4.1, 10.5.0, 10.6.0, 10.7.0, 10.8.0, 10.9.0, 10.10.0, 10.11.0, 10.12.0, 10.13.0, 10.14.0, 10.14.1, 10.14.2, 10.15.0, 10.15.1, 10.15.2, 10.15.3, 10.16.0, 10.16.1, 10.16.2, 10.16.3, 10.17.0, 10.18.0, 10.18.1, 10.19.0, 10.20.0, 10.20.1, 11.0.0, 11.1.0, 11.2.0, 11.3.0, 11.4.0, 11.5.0, 11.6.0, 11.7.0, 11.8.0, 11.9.0, 11.10.0, 11.10.1, 11.11.0, 11.12.0, 11.13.0, 11.14.0, 11.15.0, 12.0.0, 12.1.0, 12.2.0, 12.3.0, 12.3.1, 12.4.0, 12.5.0, 12.6.0, 12.7.0, 12.8.0, 12.8.1, 12.9.0, 12.9.1, 12.10.0, 12.11.0, 12.11.1, 12.12.0, 12.13.0, 12.13.1, 12.14.0, 12.14.1, 12.15.0, 12.16.0, 12.16.1, 12.16.2, 12.16.3, 13.0.0, 13.0.1, 13.1.0, 13.2.0, 13.3.0, 13.4.0, 13.5.0, 13.6.0, 13.7.0, 13.8.0, 13.9.0, 13.10.0, 13.10.1, 13.11.0, 13.12.0, 13.13.0, 13.14.0, 14.0.0, 14.1.0, 14.2.0, 14.3.0, default */
/***/ (function(module) {

eval("module.exports = JSON.parse(\"{\\\"0.1.14\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"1.3\\\"},\\\"0.1.15\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"1.3\\\"},\\\"0.1.16\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"1.3\\\"},\\\"0.1.17\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"1.3\\\"},\\\"0.1.18\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"1.3\\\"},\\\"0.1.19\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.0\\\"},\\\"0.1.20\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.0\\\"},\\\"0.1.21\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.0\\\"},\\\"0.1.22\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.0\\\"},\\\"0.1.23\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.0\\\"},\\\"0.1.24\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.0\\\"},\\\"0.1.25\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.0\\\"},\\\"0.1.26\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.0\\\"},\\\"0.1.27\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.1\\\"},\\\"0.1.28\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.1\\\"},\\\"0.1.29\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.1\\\"},\\\"0.1.30\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.1\\\"},\\\"0.1.31\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.1\\\"},\\\"0.1.32\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.1\\\"},\\\"0.1.33\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.1\\\"},\\\"0.1.90\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.2\\\"},\\\"0.1.91\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.2\\\"},\\\"0.1.92\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.2\\\"},\\\"0.1.93\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.2\\\"},\\\"0.1.94\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.2\\\"},\\\"0.1.95\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.2\\\"},\\\"0.1.96\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.2\\\"},\\\"0.1.97\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.2\\\"},\\\"0.1.98\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.2\\\"},\\\"0.1.99\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.2\\\"},\\\"0.1.100\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.2\\\"},\\\"0.1.101\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.3\\\"},\\\"0.1.102\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.3\\\"},\\\"0.1.103\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.3\\\"},\\\"0.1.104\\\":{\\\"node_abi\\\":null,\\\"v8\\\":\\\"2.3\\\"},\\\"0.2.0\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"2.3\\\"},\\\"0.2.1\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"2.3\\\"},\\\"0.2.2\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"2.3\\\"},\\\"0.2.3\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"2.3\\\"},\\\"0.2.4\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"2.3\\\"},\\\"0.2.5\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"2.3\\\"},\\\"0.2.6\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"2.3\\\"},\\\"0.3.0\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"2.5\\\"},\\\"0.3.1\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"2.5\\\"},\\\"0.3.2\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.0\\\"},\\\"0.3.3\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.0\\\"},\\\"0.3.4\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.0\\\"},\\\"0.3.5\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.0\\\"},\\\"0.3.6\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.0\\\"},\\\"0.3.7\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.0\\\"},\\\"0.3.8\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.1\\\"},\\\"0.4.0\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.1\\\"},\\\"0.4.1\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.1\\\"},\\\"0.4.2\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.1\\\"},\\\"0.4.3\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.1\\\"},\\\"0.4.4\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.1\\\"},\\\"0.4.5\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.1\\\"},\\\"0.4.6\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.1\\\"},\\\"0.4.7\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.1\\\"},\\\"0.4.8\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.1\\\"},\\\"0.4.9\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.1\\\"},\\\"0.4.10\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.1\\\"},\\\"0.4.11\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.1\\\"},\\\"0.4.12\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.1\\\"},\\\"0.5.0\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.1\\\"},\\\"0.5.1\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.4\\\"},\\\"0.5.2\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.4\\\"},\\\"0.5.3\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.4\\\"},\\\"0.5.4\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.5\\\"},\\\"0.5.5\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.5\\\"},\\\"0.5.6\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.5.7\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.5.8\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.5.9\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.5.10\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.7\\\"},\\\"0.6.0\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.1\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.2\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.3\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.4\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.5\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.6\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.7\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.8\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.9\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.10\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.11\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.12\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.13\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.14\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.15\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.16\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.17\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.18\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.19\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.20\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.6.21\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.6\\\"},\\\"0.7.0\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.8\\\"},\\\"0.7.1\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.8\\\"},\\\"0.7.2\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.8\\\"},\\\"0.7.3\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.9\\\"},\\\"0.7.4\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.9\\\"},\\\"0.7.5\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.9\\\"},\\\"0.7.6\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.9\\\"},\\\"0.7.7\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.9\\\"},\\\"0.7.8\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.9\\\"},\\\"0.7.9\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.7.10\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.9\\\"},\\\"0.7.11\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.7.12\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.0\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.1\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.2\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.3\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.4\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.5\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.6\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.7\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.8\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.9\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.10\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.11\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.12\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.13\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.14\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.15\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.16\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.17\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.18\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.19\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.20\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.21\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.22\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.23\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.24\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.25\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.26\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.27\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.8.28\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.9.0\\\":{\\\"node_abi\\\":1,\\\"v8\\\":\\\"3.11\\\"},\\\"0.9.1\\\":{\\\"node_abi\\\":10,\\\"v8\\\":\\\"3.11\\\"},\\\"0.9.2\\\":{\\\"node_abi\\\":10,\\\"v8\\\":\\\"3.11\\\"},\\\"0.9.3\\\":{\\\"node_abi\\\":10,\\\"v8\\\":\\\"3.13\\\"},\\\"0.9.4\\\":{\\\"node_abi\\\":10,\\\"v8\\\":\\\"3.13\\\"},\\\"0.9.5\\\":{\\\"node_abi\\\":10,\\\"v8\\\":\\\"3.13\\\"},\\\"0.9.6\\\":{\\\"node_abi\\\":10,\\\"v8\\\":\\\"3.15\\\"},\\\"0.9.7\\\":{\\\"node_abi\\\":10,\\\"v8\\\":\\\"3.15\\\"},\\\"0.9.8\\\":{\\\"node_abi\\\":10,\\\"v8\\\":\\\"3.15\\\"},\\\"0.9.9\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.15\\\"},\\\"0.9.10\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.15\\\"},\\\"0.9.11\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.9.12\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.0\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.1\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.2\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.3\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.4\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.5\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.6\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.7\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.8\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.9\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.10\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.11\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.12\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.13\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.14\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.15\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.16\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.17\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.18\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.19\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.20\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.21\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.22\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.23\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.24\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.25\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.26\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.27\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.28\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.29\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.30\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.31\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.32\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.33\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.34\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.35\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.36\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.37\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.38\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.39\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.40\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.41\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.42\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.43\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.44\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.45\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.46\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.47\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.10.48\\\":{\\\"node_abi\\\":11,\\\"v8\\\":\\\"3.14\\\"},\\\"0.11.0\\\":{\\\"node_abi\\\":12,\\\"v8\\\":\\\"3.17\\\"},\\\"0.11.1\\\":{\\\"node_abi\\\":12,\\\"v8\\\":\\\"3.18\\\"},\\\"0.11.2\\\":{\\\"node_abi\\\":12,\\\"v8\\\":\\\"3.19\\\"},\\\"0.11.3\\\":{\\\"node_abi\\\":12,\\\"v8\\\":\\\"3.19\\\"},\\\"0.11.4\\\":{\\\"node_abi\\\":12,\\\"v8\\\":\\\"3.20\\\"},\\\"0.11.5\\\":{\\\"node_abi\\\":12,\\\"v8\\\":\\\"3.20\\\"},\\\"0.11.6\\\":{\\\"node_abi\\\":12,\\\"v8\\\":\\\"3.20\\\"},\\\"0.11.7\\\":{\\\"node_abi\\\":12,\\\"v8\\\":\\\"3.20\\\"},\\\"0.11.8\\\":{\\\"node_abi\\\":13,\\\"v8\\\":\\\"3.21\\\"},\\\"0.11.9\\\":{\\\"node_abi\\\":13,\\\"v8\\\":\\\"3.22\\\"},\\\"0.11.10\\\":{\\\"node_abi\\\":13,\\\"v8\\\":\\\"3.22\\\"},\\\"0.11.11\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.22\\\"},\\\"0.11.12\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.22\\\"},\\\"0.11.13\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.25\\\"},\\\"0.11.14\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.26\\\"},\\\"0.11.15\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.11.16\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.0\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.1\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.2\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.3\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.4\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.5\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.6\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.7\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.8\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.9\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.10\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.11\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.12\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.13\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.14\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.15\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.16\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.17\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"0.12.18\\\":{\\\"node_abi\\\":14,\\\"v8\\\":\\\"3.28\\\"},\\\"1.0.0\\\":{\\\"node_abi\\\":42,\\\"v8\\\":\\\"3.31\\\"},\\\"1.0.1\\\":{\\\"node_abi\\\":42,\\\"v8\\\":\\\"3.31\\\"},\\\"1.0.2\\\":{\\\"node_abi\\\":42,\\\"v8\\\":\\\"3.31\\\"},\\\"1.0.3\\\":{\\\"node_abi\\\":42,\\\"v8\\\":\\\"4.1\\\"},\\\"1.0.4\\\":{\\\"node_abi\\\":42,\\\"v8\\\":\\\"4.1\\\"},\\\"1.1.0\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.2.0\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.3.0\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.4.1\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.4.2\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.4.3\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.5.0\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.5.1\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.6.0\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.6.1\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.6.2\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.6.3\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.6.4\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.7.1\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.8.1\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.8.2\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.8.3\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"1.8.4\\\":{\\\"node_abi\\\":43,\\\"v8\\\":\\\"4.1\\\"},\\\"2.0.0\\\":{\\\"node_abi\\\":44,\\\"v8\\\":\\\"4.2\\\"},\\\"2.0.1\\\":{\\\"node_abi\\\":44,\\\"v8\\\":\\\"4.2\\\"},\\\"2.0.2\\\":{\\\"node_abi\\\":44,\\\"v8\\\":\\\"4.2\\\"},\\\"2.1.0\\\":{\\\"node_abi\\\":44,\\\"v8\\\":\\\"4.2\\\"},\\\"2.2.0\\\":{\\\"node_abi\\\":44,\\\"v8\\\":\\\"4.2\\\"},\\\"2.2.1\\\":{\\\"node_abi\\\":44,\\\"v8\\\":\\\"4.2\\\"},\\\"2.3.0\\\":{\\\"node_abi\\\":44,\\\"v8\\\":\\\"4.2\\\"},\\\"2.3.1\\\":{\\\"node_abi\\\":44,\\\"v8\\\":\\\"4.2\\\"},\\\"2.3.2\\\":{\\\"node_abi\\\":44,\\\"v8\\\":\\\"4.2\\\"},\\\"2.3.3\\\":{\\\"node_abi\\\":44,\\\"v8\\\":\\\"4.2\\\"},\\\"2.3.4\\\":{\\\"node_abi\\\":44,\\\"v8\\\":\\\"4.2\\\"},\\\"2.4.0\\\":{\\\"node_abi\\\":44,\\\"v8\\\":\\\"4.2\\\"},\\\"2.5.0\\\":{\\\"node_abi\\\":44,\\\"v8\\\":\\\"4.2\\\"},\\\"3.0.0\\\":{\\\"node_abi\\\":45,\\\"v8\\\":\\\"4.4\\\"},\\\"3.1.0\\\":{\\\"node_abi\\\":45,\\\"v8\\\":\\\"4.4\\\"},\\\"3.2.0\\\":{\\\"node_abi\\\":45,\\\"v8\\\":\\\"4.4\\\"},\\\"3.3.0\\\":{\\\"node_abi\\\":45,\\\"v8\\\":\\\"4.4\\\"},\\\"3.3.1\\\":{\\\"node_abi\\\":45,\\\"v8\\\":\\\"4.4\\\"},\\\"4.0.0\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.1.0\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.1.1\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.1.2\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.2.0\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.2.1\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.2.2\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.2.3\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.2.4\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.2.5\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.2.6\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.3.0\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.3.1\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.3.2\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.4.0\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.4.1\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.4.2\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.4.3\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.4.4\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.4.5\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.4.6\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.4.7\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.5.0\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.6.0\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.6.1\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.6.2\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.7.0\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.7.1\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.7.2\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.7.3\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.8.0\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.8.1\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.8.2\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.8.3\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.8.4\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.8.5\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.8.6\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.8.7\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.9.0\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"4.9.1\\\":{\\\"node_abi\\\":46,\\\"v8\\\":\\\"4.5\\\"},\\\"5.0.0\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.1.0\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.1.1\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.2.0\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.3.0\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.4.0\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.4.1\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.5.0\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.6.0\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.7.0\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.7.1\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.8.0\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.9.0\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.9.1\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.10.0\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.10.1\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.11.0\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.11.1\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"5.12.0\\\":{\\\"node_abi\\\":47,\\\"v8\\\":\\\"4.6\\\"},\\\"6.0.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.0\\\"},\\\"6.1.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.0\\\"},\\\"6.2.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.0\\\"},\\\"6.2.1\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.0\\\"},\\\"6.2.2\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.0\\\"},\\\"6.3.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.0\\\"},\\\"6.3.1\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.0\\\"},\\\"6.4.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.0\\\"},\\\"6.5.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.6.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.7.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.8.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.8.1\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.9.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.9.1\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.9.2\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.9.3\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.9.4\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.9.5\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.10.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.10.1\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.10.2\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.10.3\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.11.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.11.1\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.11.2\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.11.3\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.11.4\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.11.5\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.12.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.12.1\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.12.2\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.12.3\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.13.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.13.1\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.14.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.14.1\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.14.2\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.14.3\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.14.4\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.15.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.15.1\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.16.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.17.0\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"6.17.1\\\":{\\\"node_abi\\\":48,\\\"v8\\\":\\\"5.1\\\"},\\\"7.0.0\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.4\\\"},\\\"7.1.0\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.4\\\"},\\\"7.2.0\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.4\\\"},\\\"7.2.1\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.4\\\"},\\\"7.3.0\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.4\\\"},\\\"7.4.0\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.4\\\"},\\\"7.5.0\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.4\\\"},\\\"7.6.0\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.5\\\"},\\\"7.7.0\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.5\\\"},\\\"7.7.1\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.5\\\"},\\\"7.7.2\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.5\\\"},\\\"7.7.3\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.5\\\"},\\\"7.7.4\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.5\\\"},\\\"7.8.0\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.5\\\"},\\\"7.9.0\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.5\\\"},\\\"7.10.0\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.5\\\"},\\\"7.10.1\\\":{\\\"node_abi\\\":51,\\\"v8\\\":\\\"5.5\\\"},\\\"8.0.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"5.8\\\"},\\\"8.1.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"5.8\\\"},\\\"8.1.1\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"5.8\\\"},\\\"8.1.2\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"5.8\\\"},\\\"8.1.3\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"5.8\\\"},\\\"8.1.4\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"5.8\\\"},\\\"8.2.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"5.8\\\"},\\\"8.2.1\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"5.8\\\"},\\\"8.3.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.0\\\"},\\\"8.4.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.0\\\"},\\\"8.5.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.0\\\"},\\\"8.6.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.0\\\"},\\\"8.7.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.1\\\"},\\\"8.8.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.1\\\"},\\\"8.8.1\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.1\\\"},\\\"8.9.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.1\\\"},\\\"8.9.1\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.1\\\"},\\\"8.9.2\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.1\\\"},\\\"8.9.3\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.1\\\"},\\\"8.9.4\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.1\\\"},\\\"8.10.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.2\\\"},\\\"8.11.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.2\\\"},\\\"8.11.1\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.2\\\"},\\\"8.11.2\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.2\\\"},\\\"8.11.3\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.2\\\"},\\\"8.11.4\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.2\\\"},\\\"8.12.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.2\\\"},\\\"8.13.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.2\\\"},\\\"8.14.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.2\\\"},\\\"8.14.1\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.2\\\"},\\\"8.15.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.2\\\"},\\\"8.15.1\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.2\\\"},\\\"8.16.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.2\\\"},\\\"8.16.1\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.2\\\"},\\\"8.16.2\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.2\\\"},\\\"8.17.0\\\":{\\\"node_abi\\\":57,\\\"v8\\\":\\\"6.2\\\"},\\\"9.0.0\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.1.0\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.2.0\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.2.1\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.3.0\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.4.0\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.5.0\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.6.0\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.6.1\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.7.0\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.7.1\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.8.0\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.9.0\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.10.0\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.10.1\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.11.0\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.11.1\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"9.11.2\\\":{\\\"node_abi\\\":59,\\\"v8\\\":\\\"6.2\\\"},\\\"10.0.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.6\\\"},\\\"10.1.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.6\\\"},\\\"10.2.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.6\\\"},\\\"10.2.1\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.6\\\"},\\\"10.3.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.6\\\"},\\\"10.4.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.7\\\"},\\\"10.4.1\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.7\\\"},\\\"10.5.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.7\\\"},\\\"10.6.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.7\\\"},\\\"10.7.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.7\\\"},\\\"10.8.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.7\\\"},\\\"10.9.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.10.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.11.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.12.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.13.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.14.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.14.1\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.14.2\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.15.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.15.1\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.15.2\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.15.3\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.16.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.16.1\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.16.2\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.16.3\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.17.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.18.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.18.1\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.19.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.20.0\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"10.20.1\\\":{\\\"node_abi\\\":64,\\\"v8\\\":\\\"6.8\\\"},\\\"11.0.0\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"11.1.0\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"11.2.0\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"11.3.0\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"11.4.0\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"11.5.0\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"11.6.0\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"11.7.0\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"11.8.0\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"11.9.0\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"11.10.0\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"11.10.1\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"11.11.0\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"11.12.0\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"11.13.0\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"11.14.0\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"11.15.0\\\":{\\\"node_abi\\\":67,\\\"v8\\\":\\\"7.0\\\"},\\\"12.0.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.4\\\"},\\\"12.1.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.4\\\"},\\\"12.2.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.4\\\"},\\\"12.3.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.4\\\"},\\\"12.3.1\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.4\\\"},\\\"12.4.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.4\\\"},\\\"12.5.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.5\\\"},\\\"12.6.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.5\\\"},\\\"12.7.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.5\\\"},\\\"12.8.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.5\\\"},\\\"12.8.1\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.5\\\"},\\\"12.9.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.6\\\"},\\\"12.9.1\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.6\\\"},\\\"12.10.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.6\\\"},\\\"12.11.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.7\\\"},\\\"12.11.1\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.7\\\"},\\\"12.12.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.7\\\"},\\\"12.13.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.7\\\"},\\\"12.13.1\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.7\\\"},\\\"12.14.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.7\\\"},\\\"12.14.1\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.7\\\"},\\\"12.15.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.7\\\"},\\\"12.16.0\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.8\\\"},\\\"12.16.1\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.8\\\"},\\\"12.16.2\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.8\\\"},\\\"12.16.3\\\":{\\\"node_abi\\\":72,\\\"v8\\\":\\\"7.8\\\"},\\\"13.0.0\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.8\\\"},\\\"13.0.1\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.8\\\"},\\\"13.1.0\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.8\\\"},\\\"13.2.0\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.9\\\"},\\\"13.3.0\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.9\\\"},\\\"13.4.0\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.9\\\"},\\\"13.5.0\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.9\\\"},\\\"13.6.0\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.9\\\"},\\\"13.7.0\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.9\\\"},\\\"13.8.0\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.9\\\"},\\\"13.9.0\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.9\\\"},\\\"13.10.0\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.9\\\"},\\\"13.10.1\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.9\\\"},\\\"13.11.0\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.9\\\"},\\\"13.12.0\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.9\\\"},\\\"13.13.0\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.9\\\"},\\\"13.14.0\\\":{\\\"node_abi\\\":79,\\\"v8\\\":\\\"7.9\\\"},\\\"14.0.0\\\":{\\\"node_abi\\\":83,\\\"v8\\\":\\\"8.1\\\"},\\\"14.1.0\\\":{\\\"node_abi\\\":83,\\\"v8\\\":\\\"8.1\\\"},\\\"14.2.0\\\":{\\\"node_abi\\\":83,\\\"v8\\\":\\\"8.1\\\"},\\\"14.3.0\\\":{\\\"node_abi\\\":83,\\\"v8\\\":\\\"8.1\\\"}}\");\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/util/abi_crosswalk.json?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/util/compile.js":
/*!*******************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/util/compile.js ***!
  \*******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = exports;\n\nvar fs = __webpack_require__(/*! fs */ \"fs\");\nvar path = __webpack_require__(/*! path */ \"path\");\nvar win = process.platform == 'win32';\nvar existsSync = fs.existsSync || path.existsSync;\nvar cp = __webpack_require__(/*! child_process */ \"child_process\");\n\n// try to build up the complete path to node-gyp\n/* priority:\n  - node-gyp on ENV:npm_config_node_gyp (https://github.com/npm/npm/pull/4887)\n  - node-gyp on NODE_PATH\n  - node-gyp inside npm on NODE_PATH (ignore on iojs)\n  - node-gyp inside npm beside node exe\n*/\nfunction which_node_gyp() {\n    var node_gyp_bin;\n    if (process.env.npm_config_node_gyp) {\n      try {\n          node_gyp_bin = process.env.npm_config_node_gyp;\n          if (existsSync(node_gyp_bin)) {\n              return node_gyp_bin;\n          }\n      } catch (err) { }\n    }\n    try {\n        var node_gyp_main = /*require.resolve*/(!(function webpackMissingModule() { var e = new Error(\"Cannot find module 'node-gyp'\"); e.code = 'MODULE_NOT_FOUND'; throw e; }()));\n        node_gyp_bin = path.join(path.dirname(\n                                     path.dirname(node_gyp_main)),\n                                     'bin/node-gyp.js');\n        if (existsSync(node_gyp_bin)) {\n            return node_gyp_bin;\n        }\n    } catch (err) { }\n    if (process.execPath.indexOf('iojs') === -1) {\n        try {\n            var npm_main = /*require.resolve*/(!(function webpackMissingModule() { var e = new Error(\"Cannot find module 'npm'\"); e.code = 'MODULE_NOT_FOUND'; throw e; }()));\n            node_gyp_bin = path.join(path.dirname(\n                                         path.dirname(npm_main)),\n                                         'node_modules/node-gyp/bin/node-gyp.js');\n            if (existsSync(node_gyp_bin)) {\n                return node_gyp_bin;\n            }\n        } catch (err) { }\n    }\n    var npm_base = path.join(path.dirname(\n                             path.dirname(process.execPath)),\n                             'lib/node_modules/npm/');\n    node_gyp_bin = path.join(npm_base, 'node_modules/node-gyp/bin/node-gyp.js');\n    if (existsSync(node_gyp_bin)) {\n        return node_gyp_bin;\n    }\n}\n\nmodule.exports.run_gyp = function(args,opts,callback) {\n    var shell_cmd = '';\n    var cmd_args = [];\n    if (opts.runtime && opts.runtime == 'node-webkit') {\n        shell_cmd = 'nw-gyp';\n        if (win) shell_cmd += '.cmd';\n    } else {\n        var node_gyp_path = which_node_gyp();\n        if (node_gyp_path) {\n            shell_cmd = process.execPath;\n            cmd_args.push(node_gyp_path);\n        } else {\n            shell_cmd = 'node-gyp';\n            if (win) shell_cmd += '.cmd';\n        }\n    }\n    var final_args = cmd_args.concat(args);\n    var cmd = cp.spawn(shell_cmd, final_args, {cwd: undefined, env: process.env, stdio: [ 0, 1, 2]});\n    cmd.on('error', function (err) {\n        if (err) {\n            return callback(new Error(\"Failed to execute '\" + shell_cmd + ' ' + final_args.join(' ') + \"' (\" + err + \")\"));\n        }\n        callback(null,opts);\n    });\n    cmd.on('close', function (code) {\n        if (code && code !== 0) {\n            return callback(new Error(\"Failed to execute '\" + shell_cmd + ' ' + final_args.join(' ') + \"' (\" + code + \")\"));\n        }\n        callback(null,opts);\n    });\n};\n\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/util/compile.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/util/handle_gyp_opts.js":
/*!***************************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/util/handle_gyp_opts.js ***!
  \***************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = exports = handle_gyp_opts;\n\nvar fs = __webpack_require__(/*! fs */ \"fs\");\nvar versioning = __webpack_require__(/*! ./versioning.js */ \"./node_modules/node-pre-gyp/lib/util/versioning.js\");\nvar napi = __webpack_require__(/*! ./napi.js */ \"./node_modules/node-pre-gyp/lib/util/napi.js\");\n\n/*\n\nHere we gather node-pre-gyp generated options (from versioning) and pass them along to node-gyp.\n\nWe massage the args and options slightly to account for differences in what commands mean between\nnode-pre-gyp and node-gyp (e.g. see the difference between \"build\" and \"rebuild\" below)\n\nKeep in mind: the values inside `argv` and `gyp.opts` below are different depending on whether\nnode-pre-gyp is called directory, or if it is called in a `run-script` phase of npm.\n\nWe also try to preserve any command line options that might have been passed to npm or node-pre-gyp.\nBut this is fairly difficult without passing way to much through. For example `gyp.opts` contains all\nthe process.env and npm pushes a lot of variables into process.env which node-pre-gyp inherits. So we have\nto be very selective about what we pass through.\n\nFor example:\n\n`npm install --build-from-source` will give:\n\nargv == [ 'rebuild' ]\ngyp.opts.argv == { remain: [ 'install' ],\n  cooked: [ 'install', '--fallback-to-build' ],\n  original: [ 'install', '--fallback-to-build' ] }\n\n`./bin/node-pre-gyp build` will give:\n\nargv == []\ngyp.opts.argv == { remain: [ 'build' ],\n  cooked: [ 'build' ],\n  original: [ '-C', 'test/app1', 'build' ] }\n\n*/\n\n// select set of node-pre-gyp versioning info\n// to share with node-gyp\nvar share_with_node_gyp = [\n  'module',\n  'module_name',\n  'module_path',\n  'napi_version',\n  'node_abi_napi',\n  'napi_build_version',\n  'node_napi_label'\n];\n\nfunction handle_gyp_opts(gyp, argv, callback) {\n\n    // Collect node-pre-gyp specific variables to pass to node-gyp\n    var node_pre_gyp_options = [];\n    // generate custom node-pre-gyp versioning info\n    var napi_build_version = napi.get_napi_build_version_from_command_args(argv);\n    var opts = versioning.evaluate(JSON.parse(fs.readFileSync('./package.json')), gyp.opts, napi_build_version);\n    share_with_node_gyp.forEach(function(key) {\n        var val = opts[key];\n        if (val) {\n            node_pre_gyp_options.push('--' + key + '=' + val);\n        } else if (key === 'napi_build_version') {\n            node_pre_gyp_options.push('--' + key + '=0');\n        } else {\n            if (key !== 'napi_version' && key !== 'node_abi_napi')\n                return callback(new Error(\"Option \" + key + \" required but not found by node-pre-gyp\"));\n        }\n    });\n\n    // Collect options that follow the special -- which disables nopt parsing\n    var unparsed_options = [];\n    var double_hyphen_found = false;\n    gyp.opts.argv.original.forEach(function(opt) {\n        if (double_hyphen_found) {\n            unparsed_options.push(opt);\n        }\n        if (opt == '--') {\n            double_hyphen_found = true;\n        }\n    });\n\n    // We try respect and pass through remaining command\n    // line options (like --foo=bar) to node-gyp\n    var cooked = gyp.opts.argv.cooked;\n    var node_gyp_options = [];\n    cooked.forEach(function(value) {\n        if (value.length > 2 && value.slice(0,2) == '--') {\n            var key = value.slice(2);\n            var val = cooked[cooked.indexOf(value)+1];\n            if (val && val.indexOf('--') === -1) { // handle '--foo=bar' or ['--foo','bar']\n                node_gyp_options.push('--' + key + '=' + val);\n            } else { // pass through --foo\n                node_gyp_options.push(value);\n            }\n        }\n    });\n\n    var result = {'opts':opts,'gyp':node_gyp_options,'pre':node_pre_gyp_options,'unparsed':unparsed_options};\n    return callback(null,result);\n}\n\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/util/handle_gyp_opts.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/util/napi.js":
/*!****************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/util/napi.js ***!
  \****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nvar fs = __webpack_require__(/*! fs */ \"fs\");\n\nmodule.exports = exports;\n\nvar versionArray = process.version\n\t.substr(1)\n\t.replace(/-.*$/, '')\n\t.split('.')\n\t.map(function(item) {\n\t\treturn +item;\n\t});\n\nvar napi_multiple_commands = [\n\t'build',\n\t'clean',\n\t'configure',\n\t'package',\n\t'publish',\n\t'reveal',\n\t'testbinary',\n\t'testpackage',\n\t'unpublish'\n];\n\nvar napi_build_version_tag = 'napi_build_version=';\n\nmodule.exports.get_napi_version = function(target) { // target may be undefined\n\t// returns the non-zero numeric napi version or undefined if napi is not supported.\n\t// correctly supporting target requires an updated cross-walk\n\tvar version = process.versions.napi; // can be undefined\n\tif (!version) { // this code should never need to be updated\n\t\tif (versionArray[0] === 9 && versionArray[1] >= 3) version = 2; // 9.3.0+\n\t\telse if (versionArray[0] === 8) version = 1; // 8.0.0+\n\t}\n\treturn version;\n};\n\nmodule.exports.get_napi_version_as_string = function(target) {\n\t// returns the napi version as a string or an empty string if napi is not supported.\n\tvar version = module.exports.get_napi_version(target);\n\treturn version ? ''+version : '';\n};\n\nmodule.exports.validate_package_json = function(package_json, opts) { // throws Error\n\n\tvar binary = package_json.binary;\n\tvar module_path_ok = pathOK(binary.module_path);\n\tvar remote_path_ok = pathOK(binary.remote_path);\n\tvar package_name_ok = pathOK(binary.package_name);\n\tvar napi_build_versions = module.exports.get_napi_build_versions(package_json,opts,true);\n\tvar napi_build_versions_raw = module.exports.get_napi_build_versions_raw(package_json);\n\n\tif (napi_build_versions) {\n\t\tnapi_build_versions.forEach(function(napi_build_version){\n\t\t\tif (!(parseInt(napi_build_version,10) === napi_build_version && napi_build_version > 0)) {\n\t\t\t\tthrow new Error(\"All values specified in napi_versions must be positive integers.\");\n\t\t\t}\n\t\t});\n\t}\n\n\tif (napi_build_versions && (!module_path_ok || (!remote_path_ok && !package_name_ok))) {\n\t\tthrow new Error(\"When napi_versions is specified; module_path and either remote_path or \" +\n\t\t\t\"package_name must contain the substitution string '{napi_build_version}`.\");\n\t}\n\n\tif ((module_path_ok || remote_path_ok || package_name_ok) && !napi_build_versions_raw) {\n\t\tthrow new Error(\"When the substitution string '{napi_build_version}` is specified in \" +\n\t\t\t\"module_path, remote_path, or package_name; napi_versions must also be specified.\");\n\t}\n\n\tif (napi_build_versions && !module.exports.get_best_napi_build_version(package_json, opts) && \n\tmodule.exports.build_napi_only(package_json)) {\n\t\tthrow new Error(\n\t\t\t'The N-API version of this Node instance is ' + module.exports.get_napi_version(opts ? opts.target : undefined) + '. ' +\n\t\t\t'This module supports N-API version(s) ' + module.exports.get_napi_build_versions_raw(package_json) + '. ' +\n\t\t\t'This Node instance cannot run this module.');\n\t}\n\n\tif (napi_build_versions_raw && !napi_build_versions && module.exports.build_napi_only(package_json)) {\n\t\tthrow new Error(\n\t\t\t'The N-API version of this Node instance is ' + module.exports.get_napi_version(opts ? opts.target : undefined) + '. ' +\n\t\t\t'This module supports N-API version(s) ' + module.exports.get_napi_build_versions_raw(package_json) + '. ' +\n\t\t\t'This Node instance cannot run this module.');\n\t}\n\n};\n\nfunction pathOK (path) {\n\treturn path && (path.indexOf('{napi_build_version}') !== -1 || path.indexOf('{node_napi_label}') !== -1);\n}\n\nmodule.exports.expand_commands = function(package_json, opts, commands) {\n\tvar expanded_commands = [];\n\tvar napi_build_versions = module.exports.get_napi_build_versions(package_json, opts);\n\tcommands.forEach(function(command){\n\t\tif (napi_build_versions && command.name === 'install') {\n\t\t\tvar napi_build_version = module.exports.get_best_napi_build_version(package_json, opts);\n\t\t\tvar args = napi_build_version ? [ napi_build_version_tag+napi_build_version ] : [ ];\n\t\t\texpanded_commands.push ({ name: command.name, args: args });\n\t\t} else if (napi_build_versions && napi_multiple_commands.indexOf(command.name) !== -1) {\n\t\t\tnapi_build_versions.forEach(function(napi_build_version){\n\t\t\t\tvar args = command.args.slice();\n\t\t\t\targs.push (napi_build_version_tag+napi_build_version);\n\t\t\t\texpanded_commands.push ({ name: command.name, args: args });\n\t\t\t});\n\t\t} else {\n\t\t\texpanded_commands.push (command);\n\t\t}\n\t});\n\treturn expanded_commands;\n};\n\nmodule.exports.get_napi_build_versions = function(package_json, opts, warnings) { // opts may be undefined\n\tvar log = __webpack_require__(/*! npmlog */ \"./node_modules/npmlog/log.js\");\n\tvar napi_build_versions = [];\n\tvar supported_napi_version = module.exports.get_napi_version(opts ? opts.target : undefined);\n\t// remove duplicates, verify each napi version can actaully be built\n\tif (package_json.binary && package_json.binary.napi_versions) {\n\t\tpackage_json.binary.napi_versions.forEach(function(napi_version) {\n\t\t\tvar duplicated = napi_build_versions.indexOf(napi_version) !== -1;\n\t\t\tif (!duplicated && supported_napi_version && napi_version <= supported_napi_version) {\n\t\t\t\tnapi_build_versions.push(napi_version);\n\t\t\t} else if (warnings && !duplicated && supported_napi_version) {\n\t\t\t\tlog.info('This Node instance does not support builds for N-API version', napi_version);\n\t\t\t}\n\t\t});\n\t}\n\tif (opts && opts[\"build-latest-napi-version-only\"]) {\n\t\tvar latest_version = 0;\n\t\tnapi_build_versions.forEach(function(napi_version) {\n\t\t\tif (napi_version > latest_version) latest_version = napi_version;\n\t\t});\n\t\tnapi_build_versions = latest_version ? [ latest_version ] : [];\n\t}\n\treturn napi_build_versions.length ? napi_build_versions : undefined;\n};\n\nmodule.exports.get_napi_build_versions_raw = function(package_json) {\n\tvar napi_build_versions = [];\n\t// remove duplicates\n\tif (package_json.binary && package_json.binary.napi_versions) {\n\t\tpackage_json.binary.napi_versions.forEach(function(napi_version) {\n\t\t\tif (napi_build_versions.indexOf(napi_version) === -1) {\n\t\t\t\tnapi_build_versions.push(napi_version);\n\t\t\t}\n\t\t});\n\t}\n\treturn napi_build_versions.length ? napi_build_versions : undefined;\n};\n\nmodule.exports.get_command_arg = function(napi_build_version) {\n\treturn napi_build_version_tag + napi_build_version;\n};\n\nmodule.exports.get_napi_build_version_from_command_args = function(command_args) {\n\tfor (var i = 0; i < command_args.length; i++) {\n\t\tvar arg = command_args[i];\n\t\tif (arg.indexOf(napi_build_version_tag) === 0) {\n\t\t\treturn parseInt(arg.substr(napi_build_version_tag.length),10);\n\t\t}\n\t}\n\treturn undefined;\n};\n\nmodule.exports.swap_build_dir_out = function(napi_build_version) {\n\tif (napi_build_version) {\n\t\tvar rm = __webpack_require__(/*! rimraf */ \"rimraf\");\n\t\trm.sync(module.exports.get_build_dir(napi_build_version));\n\t\tfs.renameSync('build', module.exports.get_build_dir(napi_build_version));\n\t}\n};\n\nmodule.exports.swap_build_dir_in = function(napi_build_version) {\n\tif (napi_build_version) {\n\t\tvar rm = __webpack_require__(/*! rimraf */ \"rimraf\");\n\t\trm.sync('build');\n\t\tfs.renameSync(module.exports.get_build_dir(napi_build_version), 'build');\n\t}\n};\n\nmodule.exports.get_build_dir = function(napi_build_version) {\n\treturn 'build-tmp-napi-v'+napi_build_version;\n};\n\nmodule.exports.get_best_napi_build_version = function(package_json, opts) {\n\tvar best_napi_build_version = 0;\n\tvar napi_build_versions = module.exports.get_napi_build_versions (package_json, opts);\n\tif (napi_build_versions) {\n\t\tvar our_napi_version = module.exports.get_napi_version(opts ? opts.target : undefined);\n\t\tnapi_build_versions.forEach(function(napi_build_version){\n\t\t\tif (napi_build_version > best_napi_build_version &&\n\t\t\t\tnapi_build_version <= our_napi_version) {\n\t\t\t\tbest_napi_build_version = napi_build_version;\n\t\t\t}\n\t\t});\n\t}\n\treturn best_napi_build_version === 0 ? undefined : best_napi_build_version;\n};\n\nmodule.exports.build_napi_only = function(package_json) {\n\treturn package_json.binary && package_json.binary.package_name && \n\tpackage_json.binary.package_name.indexOf('{node_napi_label}') === -1;\n};\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/util/napi.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/util/nw-pre-gyp/index.html":
/*!******************************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/util/nw-pre-gyp/index.html ***!
  \******************************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("throw new Error(\"Module parse failed: Unexpected token (1:0)\\nYou may need an appropriate loader to handle this file type, currently no loaders are configured to process this file. See https://webpack.js.org/concepts#loaders\\n> <!doctype html>\\n| <html>\\n| <head>\");\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/util/nw-pre-gyp/index.html?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/util/nw-pre-gyp/package.json":
/*!********************************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/util/nw-pre-gyp/package.json ***!
  \********************************************************************/
/*! exports provided: main, name, description, version, window, default */
/***/ (function(module) {

eval("module.exports = JSON.parse(\"{\\\"main\\\":\\\"index.html\\\",\\\"name\\\":\\\"nw-pre-gyp-module-test\\\",\\\"description\\\":\\\"Node-webkit-based module test.\\\",\\\"version\\\":\\\"0.0.1\\\",\\\"window\\\":{\\\"show\\\":false}}\");\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/util/nw-pre-gyp/package.json?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/util/s3_setup.js":
/*!********************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/util/s3_setup.js ***!
  \********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = exports;\n\nvar url = __webpack_require__(/*! url */ \"url\");\n\nvar URI_REGEX=\"^(.*)\\.(s3(?:-.*)?)\\.amazonaws\\.com$\";\n\nmodule.exports.detect = function(to,config) {\n    var uri = url.parse(to);\n    var hostname_matches = uri.hostname.match(URI_REGEX);\n    config.prefix = (!uri.pathname || uri.pathname == '/') ? '' : uri.pathname.replace('/','');\n    if(!hostname_matches) {\n        return;\n    }\n    if (!config.bucket) {\n        config.bucket = hostname_matches[1];\n    }\n    if (!config.region) {\n        var s3_domain = hostname_matches[2];\n        if (s3_domain.slice(0,3) == 's3-' &&\n            s3_domain.length >= 3) {\n            // it appears the region is explicit in the url\n            config.region = s3_domain.replace('s3-','');\n        }\n    }\n};\n\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/util/s3_setup.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/lib/util/versioning.js":
/*!**********************************************************!*\
  !*** ./node_modules/node-pre-gyp/lib/util/versioning.js ***!
  \**********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nmodule.exports = exports;\n\nvar path = __webpack_require__(/*! path */ \"path\");\nvar semver = __webpack_require__(/*! semver */ \"semver\");\nvar url = __webpack_require__(/*! url */ \"url\");\nvar detect_libc = __webpack_require__(/*! detect-libc */ \"./node_modules/detect-libc/lib/detect-libc.js\");\nvar napi = __webpack_require__(/*! ./napi.js */ \"./node_modules/node-pre-gyp/lib/util/napi.js\");\n\nvar abi_crosswalk;\n\n// This is used for unit testing to provide a fake\n// ABI crosswalk that emulates one that is not updated\n// for the current version\nif (process.env.NODE_PRE_GYP_ABI_CROSSWALK) {\n    abi_crosswalk = __webpack_require__(\"./node_modules/node-pre-gyp/lib/util sync recursive\")(process.env.NODE_PRE_GYP_ABI_CROSSWALK);\n} else {\n    abi_crosswalk = __webpack_require__(/*! ./abi_crosswalk.json */ \"./node_modules/node-pre-gyp/lib/util/abi_crosswalk.json\");\n}\n\nvar major_versions = {};\nObject.keys(abi_crosswalk).forEach(function(v) {\n    var major = v.split('.')[0];\n    if (!major_versions[major]) {\n        major_versions[major] = v;\n    }\n});\n\nfunction get_electron_abi(runtime, target_version) {\n    if (!runtime) {\n        throw new Error(\"get_electron_abi requires valid runtime arg\");\n    }\n    if (typeof target_version === 'undefined') {\n        // erroneous CLI call\n        throw new Error(\"Empty target version is not supported if electron is the target.\");\n    }\n    // Electron guarantees that patch version update won't break native modules.\n    var sem_ver = semver.parse(target_version);\n    return runtime + '-v' + sem_ver.major + '.' + sem_ver.minor;\n}\nmodule.exports.get_electron_abi = get_electron_abi;\n\nfunction get_node_webkit_abi(runtime, target_version) {\n    if (!runtime) {\n        throw new Error(\"get_node_webkit_abi requires valid runtime arg\");\n    }\n    if (typeof target_version === 'undefined') {\n        // erroneous CLI call\n        throw new Error(\"Empty target version is not supported if node-webkit is the target.\");\n    }\n    return runtime + '-v' + target_version;\n}\nmodule.exports.get_node_webkit_abi = get_node_webkit_abi;\n\nfunction get_node_abi(runtime, versions) {\n    if (!runtime) {\n        throw new Error(\"get_node_abi requires valid runtime arg\");\n    }\n    if (!versions) {\n        throw new Error(\"get_node_abi requires valid process.versions object\");\n    }\n    var sem_ver = semver.parse(versions.node);\n    if (sem_ver.major === 0 && sem_ver.minor % 2) { // odd series\n        // https://github.com/mapbox/node-pre-gyp/issues/124\n        return runtime+'-v'+versions.node;\n    } else {\n        // process.versions.modules added in >= v0.10.4 and v0.11.7\n        // https://github.com/joyent/node/commit/ccabd4a6fa8a6eb79d29bc3bbe9fe2b6531c2d8e\n        return versions.modules ? runtime+'-v' + (+versions.modules) :\n            'v8-' + versions.v8.split('.').slice(0,2).join('.');\n    }\n}\nmodule.exports.get_node_abi = get_node_abi;\n\nfunction get_runtime_abi(runtime, target_version) {\n    if (!runtime) {\n        throw new Error(\"get_runtime_abi requires valid runtime arg\");\n    }\n    if (runtime === 'node-webkit') {\n        return get_node_webkit_abi(runtime, target_version || process.versions['node-webkit']);\n    } else if (runtime === 'electron') {\n        return get_electron_abi(runtime, target_version || process.versions.electron);\n    } else {\n        if (runtime != 'node') {\n            throw new Error(\"Unknown Runtime: '\" + runtime + \"'\");\n        }\n        if (!target_version) {\n            return get_node_abi(runtime,process.versions);\n        } else {\n            var cross_obj;\n            // abi_crosswalk generated with ./scripts/abi_crosswalk.js\n            if (abi_crosswalk[target_version]) {\n                cross_obj = abi_crosswalk[target_version];\n            } else {\n                var target_parts = target_version.split('.').map(function(i) { return +i; });\n                if (target_parts.length != 3) { // parse failed\n                    throw new Error(\"Unknown target version: \" + target_version);\n                }\n                /*\n                    The below code tries to infer the last known ABI compatible version\n                    that we have recorded in the abi_crosswalk.json when an exact match\n                    is not possible. The reasons for this to exist are complicated:\n\n                       - We support passing --target to be able to allow developers to package binaries for versions of node\n                         that are not the same one as they are running. This might also be used in combination with the\n                         --target_arch or --target_platform flags to also package binaries for alternative platforms\n                       - When --target is passed we can't therefore determine the ABI (process.versions.modules) from the node\n                         version that is running in memory\n                       - So, therefore node-pre-gyp keeps an \"ABI crosswalk\" (lib/util/abi_crosswalk.json) to be able to look\n                         this info up for all versions\n                       - But we cannot easily predict what the future ABI will be for released versions\n                       - And node-pre-gyp needs to be a `bundledDependency` in apps that depend on it in order to work correctly\n                         by being fully available at install time.\n                       - So, the speed of node releases and the bundled nature of node-pre-gyp mean that a new node-pre-gyp release\n                         need to happen for every node.js/io.js/node-webkit/nw.js/atom-shell/etc release that might come online if\n                         you want the `--target` flag to keep working for the latest version\n                       - Which is impractical ^^\n                       - Hence the below code guesses about future ABI to make the need to update node-pre-gyp less demanding.\n\n                    In practice then you can have a dependency of your app like `node-sqlite3` that bundles a `node-pre-gyp` that\n                    only knows about node v0.10.33 in the `abi_crosswalk.json` but target node v0.10.34 (which is assumed to be\n                    ABI compatible with v0.10.33).\n\n                    TODO: use semver module instead of custom version parsing\n                */\n                var major = target_parts[0];\n                var minor = target_parts[1];\n                var patch = target_parts[2];\n                // io.js: yeah if node.js ever releases 1.x this will break\n                // but that is unlikely to happen: https://github.com/iojs/io.js/pull/253#issuecomment-69432616\n                if (major === 1) {\n                    // look for last release that is the same major version\n                    // e.g. we assume io.js 1.x is ABI compatible with >= 1.0.0\n                    while (true) {\n                        if (minor > 0) --minor;\n                        if (patch > 0) --patch;\n                        var new_iojs_target = '' + major + '.' + minor + '.' + patch;\n                        if (abi_crosswalk[new_iojs_target]) {\n                            cross_obj = abi_crosswalk[new_iojs_target];\n                            console.log('Warning: node-pre-gyp could not find exact match for ' + target_version);\n                            console.log('Warning: but node-pre-gyp successfully choose ' + new_iojs_target + ' as ABI compatible target');\n                            break;\n                        }\n                        if (minor === 0 && patch === 0) {\n                            break;\n                        }\n                    }\n                } else if (major >= 2) {\n                    // look for last release that is the same major version\n                    if (major_versions[major]) {\n                        cross_obj = abi_crosswalk[major_versions[major]];\n                        console.log('Warning: node-pre-gyp could not find exact match for ' + target_version);\n                        console.log('Warning: but node-pre-gyp successfully choose ' + major_versions[major] + ' as ABI compatible target');\n                    }\n                } else if (major === 0) { // node.js\n                    if (target_parts[1] % 2 === 0) { // for stable/even node.js series\n                        // look for the last release that is the same minor release\n                        // e.g. we assume node 0.10.x is ABI compatible with >= 0.10.0\n                        while (--patch > 0) {\n                            var new_node_target = '' + major + '.' + minor + '.' + patch;\n                            if (abi_crosswalk[new_node_target]) {\n                                cross_obj = abi_crosswalk[new_node_target];\n                                console.log('Warning: node-pre-gyp could not find exact match for ' + target_version);\n                                console.log('Warning: but node-pre-gyp successfully choose ' + new_node_target + ' as ABI compatible target');\n                                break;\n                            }\n                        }\n                    }\n                }\n            }\n            if (!cross_obj) {\n                throw new Error(\"Unsupported target version: \" + target_version);\n            }\n            // emulate process.versions\n            var versions_obj = {\n                node: target_version,\n                v8: cross_obj.v8+'.0',\n                // abi_crosswalk uses 1 for node versions lacking process.versions.modules\n                // process.versions.modules added in >= v0.10.4 and v0.11.7\n                modules: cross_obj.node_abi > 1 ? cross_obj.node_abi : undefined\n            };\n            return get_node_abi(runtime, versions_obj);\n        }\n    }\n}\nmodule.exports.get_runtime_abi = get_runtime_abi;\n\nvar required_parameters = [\n    'module_name',\n    'module_path',\n    'host'\n];\n\nfunction validate_config(package_json,opts) {\n    var msg = package_json.name + ' package.json is not node-pre-gyp ready:\\n';\n    var missing = [];\n    if (!package_json.main) {\n        missing.push('main');\n    }\n    if (!package_json.version) {\n        missing.push('version');\n    }\n    if (!package_json.name) {\n        missing.push('name');\n    }\n    if (!package_json.binary) {\n        missing.push('binary');\n    }\n    var o = package_json.binary;\n    required_parameters.forEach(function(p) {\n        if (missing.indexOf('binary') > -1) {\n            missing.pop('binary');\n        }\n        if (!o || o[p] === undefined || o[p] === \"\") {\n            missing.push('binary.' + p);\n        }\n    });\n    if (missing.length >= 1) {\n        throw new Error(msg+\"package.json must declare these properties: \\n\" + missing.join('\\n'));\n    }\n    if (o) {\n        // enforce https over http\n        var protocol = url.parse(o.host).protocol;\n        if (protocol === 'http:') {\n            throw new Error(\"'host' protocol (\"+protocol+\") is invalid - only 'https:' is accepted\");\n        }\n    }\n    napi.validate_package_json(package_json,opts);\n}\n\nmodule.exports.validate_config = validate_config;\n\nfunction eval_template(template,opts) {\n    Object.keys(opts).forEach(function(key) {\n        var pattern = '{'+key+'}';\n        while (template.indexOf(pattern) > -1) {\n            template = template.replace(pattern,opts[key]);\n        }\n    });\n    return template;\n}\n\n// url.resolve needs single trailing slash\n// to behave correctly, otherwise a double slash\n// may end up in the url which breaks requests\n// and a lacking slash may not lead to proper joining\nfunction fix_slashes(pathname) {\n    if (pathname.slice(-1) != '/') {\n        return pathname + '/';\n    }\n    return pathname;\n}\n\n// remove double slashes\n// note: path.normalize will not work because\n// it will convert forward to back slashes\nfunction drop_double_slashes(pathname) {\n    return pathname.replace(/\\/\\//g,'/');\n}\n\nfunction get_process_runtime(versions) {\n    var runtime = 'node';\n    if (versions['node-webkit']) {\n        runtime = 'node-webkit';\n    } else if (versions.electron) {\n        runtime = 'electron';\n    }\n    return runtime;\n}\n\nmodule.exports.get_process_runtime = get_process_runtime;\n\nvar default_package_name = '{module_name}-v{version}-{node_abi}-{platform}-{arch}.tar.gz';\nvar default_remote_path = '';\n\nmodule.exports.evaluate = function(package_json,options,napi_build_version) {\n    options = options || {};\n    validate_config(package_json,options); // options is a suitable substitute for opts in this case\n    var v = package_json.version;\n    var module_version = semver.parse(v);\n    var runtime = options.runtime || get_process_runtime(process.versions);\n    var opts = {\n        name: package_json.name,\n        configuration: Boolean(options.debug) ? 'Debug' : 'Release',\n        debug: options.debug,\n        module_name: package_json.binary.module_name,\n        version: module_version.version,\n        prerelease: module_version.prerelease.length ? module_version.prerelease.join('.') : '',\n        build: module_version.build.length ? module_version.build.join('.') : '',\n        major: module_version.major,\n        minor: module_version.minor,\n        patch: module_version.patch,\n        runtime: runtime,\n        node_abi: get_runtime_abi(runtime,options.target),\n        node_abi_napi: napi.get_napi_version(options.target) ? 'napi' : get_runtime_abi(runtime,options.target),\n        napi_version: napi.get_napi_version(options.target), // non-zero numeric, undefined if unsupported\n        napi_build_version: napi_build_version || '',\n        node_napi_label: napi_build_version ? 'napi-v' + napi_build_version : get_runtime_abi(runtime,options.target),\n        target: options.target || '',\n        platform: options.target_platform || process.platform,\n        target_platform: options.target_platform || process.platform,\n        arch: options.target_arch || process.arch,\n        target_arch: options.target_arch || process.arch,\n        libc: options.target_libc || detect_libc.family || 'unknown',\n        module_main: package_json.main,\n        toolset : options.toolset || '' // address https://github.com/mapbox/node-pre-gyp/issues/119\n    };\n    // support host mirror with npm config `--{module_name}_binary_host_mirror`\n    // e.g.: https://github.com/node-inspector/v8-profiler/blob/master/package.json#L25\n    // > npm install v8-profiler --profiler_binary_host_mirror=https://npm.taobao.org/mirrors/node-inspector/\n    var host = process.env['npm_config_' + opts.module_name + '_binary_host_mirror'] || package_json.binary.host;\n    opts.host = fix_slashes(eval_template(host,opts));\n    opts.module_path = eval_template(package_json.binary.module_path,opts);\n    // now we resolve the module_path to ensure it is absolute so that binding.gyp variables work predictably\n    if (options.module_root) {\n        // resolve relative to known module root: works for pre-binding require\n        opts.module_path = path.join(options.module_root,opts.module_path);\n    } else {\n        // resolve relative to current working directory: works for node-pre-gyp commands\n        opts.module_path = path.resolve(opts.module_path);\n    }\n    opts.module = path.join(opts.module_path,opts.module_name + '.node');\n    opts.remote_path = package_json.binary.remote_path ? drop_double_slashes(fix_slashes(eval_template(package_json.binary.remote_path,opts))) : default_remote_path;\n    var package_name = package_json.binary.package_name ? package_json.binary.package_name : default_package_name;\n    opts.package_name = eval_template(package_name,opts);\n    opts.staged_tarball = path.join('build/stage',opts.remote_path,opts.package_name);\n    opts.hosted_path = url.resolve(opts.host,opts.remote_path);\n    opts.hosted_tarball = url.resolve(opts.hosted_path,opts.package_name);\n    return opts;\n};\n\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/lib/util/versioning.js?");

/***/ }),

/***/ "./node_modules/node-pre-gyp/package.json":
/*!************************************************!*\
  !*** ./node_modules/node-pre-gyp/package.json ***!
  \************************************************/
/*! exports provided: _from, _id, _inBundle, _integrity, _location, _phantomChildren, _requested, _requiredBy, _resolved, _shasum, _spec, _where, author, bin, bugs, bundleDependencies, dependencies, deprecated, description, devDependencies, homepage, jshintConfig, keywords, license, main, name, repository, scripts, version, default */
/***/ (function(module) {

eval("module.exports = JSON.parse(\"{\\\"_from\\\":\\\"node-pre-gyp@0.15.0\\\",\\\"_id\\\":\\\"node-pre-gyp@0.15.0\\\",\\\"_inBundle\\\":false,\\\"_integrity\\\":\\\"sha512-7QcZa8/fpaU/BKenjcaeFF9hLz2+7S9AqyXFhlH/rilsQ/hPZKK32RtR5EQHJElgu+q5RfbJ34KriI79UWaorA==\\\",\\\"_location\\\":\\\"/node-pre-gyp\\\",\\\"_phantomChildren\\\":{},\\\"_requested\\\":{\\\"type\\\":\\\"version\\\",\\\"registry\\\":true,\\\"raw\\\":\\\"node-pre-gyp@0.15.0\\\",\\\"name\\\":\\\"node-pre-gyp\\\",\\\"escapedName\\\":\\\"node-pre-gyp\\\",\\\"rawSpec\\\":\\\"0.15.0\\\",\\\"saveSpec\\\":null,\\\"fetchSpec\\\":\\\"0.15.0\\\"},\\\"_requiredBy\\\":[\\\"/bcrypt\\\"],\\\"_resolved\\\":\\\"https://registry.npmjs.org/node-pre-gyp/-/node-pre-gyp-0.15.0.tgz\\\",\\\"_shasum\\\":\\\"c2fc383276b74c7ffa842925241553e8b40f1087\\\",\\\"_spec\\\":\\\"node-pre-gyp@0.15.0\\\",\\\"_where\\\":\\\"/media/zuares/Zuares1/Programing/Source Code/Javascript/Typescript/Personal/expressJs/node_modules/bcrypt\\\",\\\"author\\\":{\\\"name\\\":\\\"Dane Springmeyer\\\",\\\"email\\\":\\\"dane@mapbox.com\\\"},\\\"bin\\\":{\\\"node-pre-gyp\\\":\\\"bin/node-pre-gyp\\\"},\\\"bugs\\\":{\\\"url\\\":\\\"https://github.com/mapbox/node-pre-gyp/issues\\\"},\\\"bundleDependencies\\\":false,\\\"dependencies\\\":{\\\"detect-libc\\\":\\\"^1.0.2\\\",\\\"mkdirp\\\":\\\"^0.5.3\\\",\\\"needle\\\":\\\"^2.5.0\\\",\\\"nopt\\\":\\\"^4.0.1\\\",\\\"npm-packlist\\\":\\\"^1.1.6\\\",\\\"npmlog\\\":\\\"^4.0.2\\\",\\\"rc\\\":\\\"^1.2.7\\\",\\\"rimraf\\\":\\\"^2.6.1\\\",\\\"semver\\\":\\\"^5.3.0\\\",\\\"tar\\\":\\\"^4.4.2\\\"},\\\"deprecated\\\":false,\\\"description\\\":\\\"Node.js native addon binary install tool\\\",\\\"devDependencies\\\":{\\\"aws-sdk\\\":\\\"^2.28.0\\\",\\\"jshint\\\":\\\"^2.9.5\\\",\\\"nock\\\":\\\"^9.2.3\\\",\\\"tape\\\":\\\"^4.6.3\\\"},\\\"homepage\\\":\\\"https://github.com/mapbox/node-pre-gyp#readme\\\",\\\"jshintConfig\\\":{\\\"node\\\":true,\\\"globalstrict\\\":true,\\\"undef\\\":true,\\\"unused\\\":false,\\\"noarg\\\":true},\\\"keywords\\\":[\\\"native\\\",\\\"addon\\\",\\\"module\\\",\\\"c\\\",\\\"c++\\\",\\\"bindings\\\",\\\"binary\\\"],\\\"license\\\":\\\"BSD-3-Clause\\\",\\\"main\\\":\\\"./lib/node-pre-gyp.js\\\",\\\"name\\\":\\\"node-pre-gyp\\\",\\\"repository\\\":{\\\"type\\\":\\\"git\\\",\\\"url\\\":\\\"git://github.com/mapbox/node-pre-gyp.git\\\"},\\\"scripts\\\":{\\\"pretest\\\":\\\"jshint test/build.test.js test/s3_setup.test.js test/versioning.test.js test/fetch.test.js lib lib/util scripts bin/node-pre-gyp\\\",\\\"test\\\":\\\"jshint lib lib/util scripts bin/node-pre-gyp && tape test/*test.js\\\",\\\"update-crosswalk\\\":\\\"node scripts/abi_crosswalk.js\\\"},\\\"version\\\":\\\"0.15.0\\\"}\");\n\n//# sourceURL=webpack:///./node_modules/node-pre-gyp/package.json?");

/***/ }),

/***/ "./node_modules/nopt/lib/nopt.js":
/*!***************************************!*\
  !*** ./node_modules/nopt/lib/nopt.js ***!
  \***************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("// info about each config option.\n\nvar debug = process.env.DEBUG_NOPT || process.env.NOPT_DEBUG\n  ? function () { console.error.apply(console, arguments) }\n  : function () {}\n\nvar url = __webpack_require__(/*! url */ \"url\")\n  , path = __webpack_require__(/*! path */ \"path\")\n  , Stream = __webpack_require__(/*! stream */ \"stream\").Stream\n  , abbrev = __webpack_require__(/*! abbrev */ \"./node_modules/abbrev/abbrev.js\")\n  , osenv = __webpack_require__(/*! osenv */ \"./node_modules/osenv/osenv.js\")\n\nmodule.exports = exports = nopt\nexports.clean = clean\n\nexports.typeDefs =\n  { String  : { type: String,  validate: validateString  }\n  , Boolean : { type: Boolean, validate: validateBoolean }\n  , url     : { type: url,     validate: validateUrl     }\n  , Number  : { type: Number,  validate: validateNumber  }\n  , path    : { type: path,    validate: validatePath    }\n  , Stream  : { type: Stream,  validate: validateStream  }\n  , Date    : { type: Date,    validate: validateDate    }\n  }\n\nfunction nopt (types, shorthands, args, slice) {\n  args = args || process.argv\n  types = types || {}\n  shorthands = shorthands || {}\n  if (typeof slice !== \"number\") slice = 2\n\n  debug(types, shorthands, args, slice)\n\n  args = args.slice(slice)\n  var data = {}\n    , key\n    , argv = {\n        remain: [],\n        cooked: args,\n        original: args.slice(0)\n      }\n\n  parse(args, data, argv.remain, types, shorthands)\n  // now data is full\n  clean(data, types, exports.typeDefs)\n  data.argv = argv\n  Object.defineProperty(data.argv, 'toString', { value: function () {\n    return this.original.map(JSON.stringify).join(\" \")\n  }, enumerable: false })\n  return data\n}\n\nfunction clean (data, types, typeDefs) {\n  typeDefs = typeDefs || exports.typeDefs\n  var remove = {}\n    , typeDefault = [false, true, null, String, Array]\n\n  Object.keys(data).forEach(function (k) {\n    if (k === \"argv\") return\n    var val = data[k]\n      , isArray = Array.isArray(val)\n      , type = types[k]\n    if (!isArray) val = [val]\n    if (!type) type = typeDefault\n    if (type === Array) type = typeDefault.concat(Array)\n    if (!Array.isArray(type)) type = [type]\n\n    debug(\"val=%j\", val)\n    debug(\"types=\", type)\n    val = val.map(function (val) {\n      // if it's an unknown value, then parse false/true/null/numbers/dates\n      if (typeof val === \"string\") {\n        debug(\"string %j\", val)\n        val = val.trim()\n        if ((val === \"null\" && ~type.indexOf(null))\n            || (val === \"true\" &&\n               (~type.indexOf(true) || ~type.indexOf(Boolean)))\n            || (val === \"false\" &&\n               (~type.indexOf(false) || ~type.indexOf(Boolean)))) {\n          val = JSON.parse(val)\n          debug(\"jsonable %j\", val)\n        } else if (~type.indexOf(Number) && !isNaN(val)) {\n          debug(\"convert to number\", val)\n          val = +val\n        } else if (~type.indexOf(Date) && !isNaN(Date.parse(val))) {\n          debug(\"convert to date\", val)\n          val = new Date(val)\n        }\n      }\n\n      if (!types.hasOwnProperty(k)) {\n        return val\n      }\n\n      // allow `--no-blah` to set 'blah' to null if null is allowed\n      if (val === false && ~type.indexOf(null) &&\n          !(~type.indexOf(false) || ~type.indexOf(Boolean))) {\n        val = null\n      }\n\n      var d = {}\n      d[k] = val\n      debug(\"prevalidated val\", d, val, types[k])\n      if (!validate(d, k, val, types[k], typeDefs)) {\n        if (exports.invalidHandler) {\n          exports.invalidHandler(k, val, types[k], data)\n        } else if (exports.invalidHandler !== false) {\n          debug(\"invalid: \"+k+\"=\"+val, types[k])\n        }\n        return remove\n      }\n      debug(\"validated val\", d, val, types[k])\n      return d[k]\n    }).filter(function (val) { return val !== remove })\n\n    // if we allow Array specifically, then an empty array is how we\n    // express 'no value here', not null.  Allow it.\n    if (!val.length && type.indexOf(Array) === -1) {\n      debug('VAL HAS NO LENGTH, DELETE IT', val, k, type.indexOf(Array))\n      delete data[k]\n    }\n    else if (isArray) {\n      debug(isArray, data[k], val)\n      data[k] = val\n    } else data[k] = val[0]\n\n    debug(\"k=%s val=%j\", k, val, data[k])\n  })\n}\n\nfunction validateString (data, k, val) {\n  data[k] = String(val)\n}\n\nfunction validatePath (data, k, val) {\n  if (val === true) return false\n  if (val === null) return true\n\n  val = String(val)\n\n  var isWin       = process.platform === 'win32'\n    , homePattern = isWin ? /^~(\\/|\\\\)/ : /^~\\//\n    , home        = osenv.home()\n\n  if (home && val.match(homePattern)) {\n    data[k] = path.resolve(home, val.substr(2))\n  } else {\n    data[k] = path.resolve(val)\n  }\n  return true\n}\n\nfunction validateNumber (data, k, val) {\n  debug(\"validate Number %j %j %j\", k, val, isNaN(val))\n  if (isNaN(val)) return false\n  data[k] = +val\n}\n\nfunction validateDate (data, k, val) {\n  var s = Date.parse(val)\n  debug(\"validate Date %j %j %j\", k, val, s)\n  if (isNaN(s)) return false\n  data[k] = new Date(val)\n}\n\nfunction validateBoolean (data, k, val) {\n  if (val instanceof Boolean) val = val.valueOf()\n  else if (typeof val === \"string\") {\n    if (!isNaN(val)) val = !!(+val)\n    else if (val === \"null\" || val === \"false\") val = false\n    else val = true\n  } else val = !!val\n  data[k] = val\n}\n\nfunction validateUrl (data, k, val) {\n  val = url.parse(String(val))\n  if (!val.host) return false\n  data[k] = val.href\n}\n\nfunction validateStream (data, k, val) {\n  if (!(val instanceof Stream)) return false\n  data[k] = val\n}\n\nfunction validate (data, k, val, type, typeDefs) {\n  // arrays are lists of types.\n  if (Array.isArray(type)) {\n    for (var i = 0, l = type.length; i < l; i ++) {\n      if (type[i] === Array) continue\n      if (validate(data, k, val, type[i], typeDefs)) return true\n    }\n    delete data[k]\n    return false\n  }\n\n  // an array of anything?\n  if (type === Array) return true\n\n  // NaN is poisonous.  Means that something is not allowed.\n  if (type !== type) {\n    debug(\"Poison NaN\", k, val, type)\n    delete data[k]\n    return false\n  }\n\n  // explicit list of values\n  if (val === type) {\n    debug(\"Explicitly allowed %j\", val)\n    // if (isArray) (data[k] = data[k] || []).push(val)\n    // else data[k] = val\n    data[k] = val\n    return true\n  }\n\n  // now go through the list of typeDefs, validate against each one.\n  var ok = false\n    , types = Object.keys(typeDefs)\n  for (var i = 0, l = types.length; i < l; i ++) {\n    debug(\"test type %j %j %j\", k, val, types[i])\n    var t = typeDefs[types[i]]\n    if (t &&\n      ((type && type.name && t.type && t.type.name) ? (type.name === t.type.name) : (type === t.type))) {\n      var d = {}\n      ok = false !== t.validate(d, k, val)\n      val = d[k]\n      if (ok) {\n        // if (isArray) (data[k] = data[k] || []).push(val)\n        // else data[k] = val\n        data[k] = val\n        break\n      }\n    }\n  }\n  debug(\"OK? %j (%j %j %j)\", ok, k, val, types[i])\n\n  if (!ok) delete data[k]\n  return ok\n}\n\nfunction parse (args, data, remain, types, shorthands) {\n  debug(\"parse\", args, data, remain)\n\n  var key = null\n    , abbrevs = abbrev(Object.keys(types))\n    , shortAbbr = abbrev(Object.keys(shorthands))\n\n  for (var i = 0; i < args.length; i ++) {\n    var arg = args[i]\n    debug(\"arg\", arg)\n\n    if (arg.match(/^-{2,}$/)) {\n      // done with keys.\n      // the rest are args.\n      remain.push.apply(remain, args.slice(i + 1))\n      args[i] = \"--\"\n      break\n    }\n    var hadEq = false\n    if (arg.charAt(0) === \"-\" && arg.length > 1) {\n      var at = arg.indexOf('=')\n      if (at > -1) {\n        hadEq = true\n        var v = arg.substr(at + 1)\n        arg = arg.substr(0, at)\n        args.splice(i, 1, arg, v)\n      }\n\n      // see if it's a shorthand\n      // if so, splice and back up to re-parse it.\n      var shRes = resolveShort(arg, shorthands, shortAbbr, abbrevs)\n      debug(\"arg=%j shRes=%j\", arg, shRes)\n      if (shRes) {\n        debug(arg, shRes)\n        args.splice.apply(args, [i, 1].concat(shRes))\n        if (arg !== shRes[0]) {\n          i --\n          continue\n        }\n      }\n      arg = arg.replace(/^-+/, \"\")\n      var no = null\n      while (arg.toLowerCase().indexOf(\"no-\") === 0) {\n        no = !no\n        arg = arg.substr(3)\n      }\n\n      if (abbrevs[arg]) arg = abbrevs[arg]\n\n      var argType = types[arg]\n      var isTypeArray = Array.isArray(argType)\n      if (isTypeArray && argType.length === 1) {\n        isTypeArray = false\n        argType = argType[0]\n      }\n\n      var isArray = argType === Array ||\n        isTypeArray && argType.indexOf(Array) !== -1\n\n      // allow unknown things to be arrays if specified multiple times.\n      if (!types.hasOwnProperty(arg) && data.hasOwnProperty(arg)) {\n        if (!Array.isArray(data[arg]))\n          data[arg] = [data[arg]]\n        isArray = true\n      }\n\n      var val\n        , la = args[i + 1]\n\n      var isBool = typeof no === 'boolean' ||\n        argType === Boolean ||\n        isTypeArray && argType.indexOf(Boolean) !== -1 ||\n        (typeof argType === 'undefined' && !hadEq) ||\n        (la === \"false\" &&\n         (argType === null ||\n          isTypeArray && ~argType.indexOf(null)))\n\n      if (isBool) {\n        // just set and move along\n        val = !no\n        // however, also support --bool true or --bool false\n        if (la === \"true\" || la === \"false\") {\n          val = JSON.parse(la)\n          la = null\n          if (no) val = !val\n          i ++\n        }\n\n        // also support \"foo\":[Boolean, \"bar\"] and \"--foo bar\"\n        if (isTypeArray && la) {\n          if (~argType.indexOf(la)) {\n            // an explicit type\n            val = la\n            i ++\n          } else if ( la === \"null\" && ~argType.indexOf(null) ) {\n            // null allowed\n            val = null\n            i ++\n          } else if ( !la.match(/^-{2,}[^-]/) &&\n                      !isNaN(la) &&\n                      ~argType.indexOf(Number) ) {\n            // number\n            val = +la\n            i ++\n          } else if ( !la.match(/^-[^-]/) && ~argType.indexOf(String) ) {\n            // string\n            val = la\n            i ++\n          }\n        }\n\n        if (isArray) (data[arg] = data[arg] || []).push(val)\n        else data[arg] = val\n\n        continue\n      }\n\n      if (argType === String) {\n        if (la === undefined) {\n          la = \"\"\n        } else if (la.match(/^-{1,2}[^-]+/)) {\n          la = \"\"\n          i --\n        }\n      }\n\n      if (la && la.match(/^-{2,}$/)) {\n        la = undefined\n        i --\n      }\n\n      val = la === undefined ? true : la\n      if (isArray) (data[arg] = data[arg] || []).push(val)\n      else data[arg] = val\n\n      i ++\n      continue\n    }\n    remain.push(arg)\n  }\n}\n\nfunction resolveShort (arg, shorthands, shortAbbr, abbrevs) {\n  // handle single-char shorthands glommed together, like\n  // npm ls -glp, but only if there is one dash, and only if\n  // all of the chars are single-char shorthands, and it's\n  // not a match to some other abbrev.\n  arg = arg.replace(/^-+/, '')\n\n  // if it's an exact known option, then don't go any further\n  if (abbrevs[arg] === arg)\n    return null\n\n  // if it's an exact known shortopt, same deal\n  if (shorthands[arg]) {\n    // make it an array, if it's a list of words\n    if (shorthands[arg] && !Array.isArray(shorthands[arg]))\n      shorthands[arg] = shorthands[arg].split(/\\s+/)\n\n    return shorthands[arg]\n  }\n\n  // first check to see if this arg is a set of single-char shorthands\n  var singles = shorthands.___singles\n  if (!singles) {\n    singles = Object.keys(shorthands).filter(function (s) {\n      return s.length === 1\n    }).reduce(function (l,r) {\n      l[r] = true\n      return l\n    }, {})\n    shorthands.___singles = singles\n    debug('shorthand singles', singles)\n  }\n\n  var chrs = arg.split(\"\").filter(function (c) {\n    return singles[c]\n  })\n\n  if (chrs.join(\"\") === arg) return chrs.map(function (c) {\n    return shorthands[c]\n  }).reduce(function (l, r) {\n    return l.concat(r)\n  }, [])\n\n\n  // if it's an arg abbrev, and not a literal shorthand, then prefer the arg\n  if (abbrevs[arg] && !shorthands[arg])\n    return null\n\n  // if it's an abbr for a shorthand, then use that\n  if (shortAbbr[arg])\n    arg = shortAbbr[arg]\n\n  // make it an array, if it's a list of words\n  if (shorthands[arg] && !Array.isArray(shorthands[arg]))\n    shorthands[arg] = shorthands[arg].split(/\\s+/)\n\n  return shorthands[arg]\n}\n\n\n//# sourceURL=webpack:///./node_modules/nopt/lib/nopt.js?");

/***/ }),

/***/ "./node_modules/npm-bundled/index.js":
/*!*******************************************!*\
  !*** ./node_modules/npm-bundled/index.js ***!
  \*******************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\n// walk the tree of deps starting from the top level list of bundled deps\n// Any deps at the top level that are depended on by a bundled dep that\n// does not have that dep in its own node_modules folder are considered\n// bundled deps as well.  This list of names can be passed to npm-packlist\n// as the \"bundled\" argument.  Additionally, packageJsonCache is shared so\n// packlist doesn't have to re-read files already consumed in this pass\n\nconst fs = __webpack_require__(/*! fs */ \"fs\")\nconst path = __webpack_require__(/*! path */ \"path\")\nconst EE = __webpack_require__(/*! events */ \"events\").EventEmitter\n// we don't care about the package bins, but we share a pj cache\n// with other modules that DO care about it, so keep it nice.\nconst normalizePackageBin = __webpack_require__(/*! npm-normalize-package-bin */ \"./node_modules/npm-normalize-package-bin/index.js\")\n\nclass BundleWalker extends EE {\n  constructor (opt) {\n    opt = opt || {}\n    super(opt)\n    this.path = path.resolve(opt.path || process.cwd())\n\n    this.parent = opt.parent || null\n    if (this.parent) {\n      this.result = this.parent.result\n      // only collect results in node_modules folders at the top level\n      // since the node_modules in a bundled dep is included always\n      if (!this.parent.parent) {\n        const base = path.basename(this.path)\n        const scope = path.basename(path.dirname(this.path))\n        this.result.add(/^@/.test(scope) ? scope + '/' + base : base)\n      }\n      this.root = this.parent.root\n      this.packageJsonCache = this.parent.packageJsonCache\n    } else {\n      this.result = new Set()\n      this.root = this.path\n      this.packageJsonCache = opt.packageJsonCache || new Map()\n    }\n\n    this.seen = new Set()\n    this.didDone = false\n    this.children = 0\n    this.node_modules = []\n    this.package = null\n    this.bundle = null\n  }\n\n  addListener (ev, fn) {\n    return this.on(ev, fn)\n  }\n\n  on (ev, fn) {\n    const ret = super.on(ev, fn)\n    if (ev === 'done' && this.didDone) {\n      this.emit('done', this.result)\n    }\n    return ret\n  }\n\n  done () {\n    if (!this.didDone) {\n      this.didDone = true\n      if (!this.parent) {\n        const res = Array.from(this.result)\n        this.result = res\n        this.emit('done', res)\n      } else {\n        this.emit('done')\n      }\n    }\n  }\n\n  start () {\n    const pj = path.resolve(this.path, 'package.json')\n    if (this.packageJsonCache.has(pj))\n      this.onPackage(this.packageJsonCache.get(pj))\n    else\n      this.readPackageJson(pj)\n    return this\n  }\n\n  readPackageJson (pj) {\n    fs.readFile(pj, (er, data) =>\n      er ? this.done() : this.onPackageJson(pj, data))\n  }\n\n  onPackageJson (pj, data) {\n    try {\n      this.package = normalizePackageBin(JSON.parse(data + ''))\n    } catch (er) {\n      return this.done()\n    }\n    this.packageJsonCache.set(pj, this.package)\n    this.onPackage(this.package)\n  }\n\n  allDepsBundled (pkg) {\n    return Object.keys(pkg.dependencies || {}).concat(\n      Object.keys(pkg.optionalDependencies || {}))\n  }\n\n  onPackage (pkg) {\n    // all deps are bundled if we got here as a child.\n    // otherwise, only bundle bundledDeps\n    // Get a unique-ified array with a short-lived Set\n    const bdRaw = this.parent ? this.allDepsBundled(pkg)\n      : pkg.bundleDependencies || pkg.bundledDependencies || []\n\n    const bd = Array.from(new Set(\n      Array.isArray(bdRaw) ? bdRaw\n      : bdRaw === true ? this.allDepsBundled(pkg)\n      : Object.keys(bdRaw)))\n\n    if (!bd.length)\n      return this.done()\n\n    this.bundle = bd\n    const nm = this.path + '/node_modules'\n    this.readModules()\n  }\n\n  readModules () {\n    readdirNodeModules(this.path + '/node_modules', (er, nm) =>\n      er ? this.onReaddir([]) : this.onReaddir(nm))\n  }\n\n  onReaddir (nm) {\n    // keep track of what we have, in case children need it\n    this.node_modules = nm\n\n    this.bundle.forEach(dep => this.childDep(dep))\n    if (this.children === 0)\n      this.done()\n  }\n\n  childDep (dep) {\n    if (this.node_modules.indexOf(dep) !== -1 && !this.seen.has(dep)) {\n      this.seen.add(dep)\n      this.child(dep)\n    } else if (this.parent) {\n      this.parent.childDep(dep)\n    }\n  }\n\n  child (dep) {\n    const p = this.path + '/node_modules/' + dep\n    this.children += 1\n    const child = new BundleWalker({\n      path: p,\n      parent: this\n    })\n    child.on('done', _ => {\n      if (--this.children === 0)\n        this.done()\n    })\n    child.start()\n  }\n}\n\nclass BundleWalkerSync extends BundleWalker {\n  constructor (opt) {\n    super(opt)\n  }\n\n  start () {\n    super.start()\n    this.done()\n    return this\n  }\n\n  readPackageJson (pj) {\n    try {\n      this.onPackageJson(pj, fs.readFileSync(pj))\n    } catch (er) {}\n    return this\n  }\n\n  readModules () {\n    try {\n      this.onReaddir(readdirNodeModulesSync(this.path + '/node_modules'))\n    } catch (er) {\n      this.onReaddir([])\n    }\n  }\n\n  child (dep) {\n    new BundleWalkerSync({\n      path: this.path + '/node_modules/' + dep,\n      parent: this\n    }).start()\n  }\n}\n\nconst readdirNodeModules = (nm, cb) => {\n  fs.readdir(nm, (er, set) => {\n    if (er)\n      cb(er)\n    else {\n      const scopes = set.filter(f => /^@/.test(f))\n      if (!scopes.length)\n        cb(null, set)\n      else {\n        const unscoped = set.filter(f => !/^@/.test(f))\n        let count = scopes.length\n        scopes.forEach(scope => {\n          fs.readdir(nm + '/' + scope, (er, pkgs) => {\n            if (er || !pkgs.length)\n              unscoped.push(scope)\n            else\n              unscoped.push.apply(unscoped, pkgs.map(p => scope + '/' + p))\n            if (--count === 0)\n              cb(null, unscoped)\n          })\n        })\n      }\n    }\n  })\n}\n\nconst readdirNodeModulesSync = nm => {\n  const set = fs.readdirSync(nm)\n  const unscoped = set.filter(f => !/^@/.test(f))\n  const scopes = set.filter(f => /^@/.test(f)).map(scope => {\n    try {\n      const pkgs = fs.readdirSync(nm + '/' + scope)\n      return pkgs.length ? pkgs.map(p => scope + '/' + p) : [scope]\n    } catch (er) {\n      return [scope]\n    }\n  }).reduce((a, b) => a.concat(b), [])\n  return unscoped.concat(scopes)\n}\n\nconst walk = (options, callback) => {\n  const p = new Promise((resolve, reject) => {\n    new BundleWalker(options).on('done', resolve).on('error', reject).start()\n  })\n  return callback ? p.then(res => callback(null, res), callback) : p\n}\n\nconst walkSync = options => {\n  return new BundleWalkerSync(options).start().result\n}\n\nmodule.exports = walk\nwalk.sync = walkSync\nwalk.BundleWalker = BundleWalker\nwalk.BundleWalkerSync = BundleWalkerSync\n\n\n//# sourceURL=webpack:///./node_modules/npm-bundled/index.js?");

/***/ }),

/***/ "./node_modules/npm-normalize-package-bin/index.js":
/*!*********************************************************!*\
  !*** ./node_modules/npm-normalize-package-bin/index.js ***!
  \*********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("// pass in a manifest with a 'bin' field here, and it'll turn it\n// into a properly santized bin object\nconst {join, basename} = __webpack_require__(/*! path */ \"path\")\n\nconst normalize = pkg =>\n  !pkg.bin ? removeBin(pkg)\n  : typeof pkg.bin === 'string' ? normalizeString(pkg)\n  : Array.isArray(pkg.bin) ? normalizeArray(pkg)\n  : typeof pkg.bin === 'object' ? normalizeObject(pkg)\n  : removeBin(pkg)\n\nconst normalizeString = pkg => {\n  if (!pkg.name)\n    return removeBin(pkg)\n  pkg.bin = { [pkg.name]: pkg.bin }\n  return normalizeObject(pkg)\n}\n\nconst normalizeArray = pkg => {\n  pkg.bin = pkg.bin.reduce((acc, k) => {\n    acc[basename(k)] = k\n    return acc\n  }, {})\n  return normalizeObject(pkg)\n}\n\nconst removeBin = pkg => {\n  delete pkg.bin\n  return pkg\n}\n\nconst normalizeObject = pkg => {\n  const orig = pkg.bin\n  const clean = {}\n  let hasBins = false\n  Object.keys(orig).forEach(binKey => {\n    const base = join('/', basename(binKey.replace(/\\\\|:/g, '/'))).substr(1)\n\n    if (typeof orig[binKey] !== 'string' || !base)\n      return\n\n    const binTarget = join('/', orig[binKey])\n      .replace(/\\\\/g, '/').substr(1)\n\n    if (!binTarget)\n      return\n\n    clean[base] = binTarget\n    hasBins = true\n  })\n\n  if (hasBins)\n    pkg.bin = clean\n  else\n    delete pkg.bin\n\n  return pkg\n}\n\nmodule.exports = normalize\n\n\n//# sourceURL=webpack:///./node_modules/npm-normalize-package-bin/index.js?");

/***/ }),

/***/ "./node_modules/npm-packlist/index.js":
/*!********************************************!*\
  !*** ./node_modules/npm-packlist/index.js ***!
  \********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\n// Do a two-pass walk, first to get the list of packages that need to be\n// bundled, then again to get the actual files and folders.\n// Keep a cache of node_modules content and package.json data, so that the\n// second walk doesn't have to re-do all the same work.\n\nconst bundleWalk = __webpack_require__(/*! npm-bundled */ \"./node_modules/npm-bundled/index.js\")\nconst BundleWalker = bundleWalk.BundleWalker\nconst BundleWalkerSync = bundleWalk.BundleWalkerSync\n\nconst ignoreWalk = __webpack_require__(/*! ignore-walk */ \"./node_modules/ignore-walk/index.js\")\nconst IgnoreWalker = ignoreWalk.Walker\nconst IgnoreWalkerSync = ignoreWalk.WalkerSync\n\nconst rootBuiltinRules = Symbol('root-builtin-rules')\nconst packageNecessaryRules = Symbol('package-necessary-rules')\nconst path = __webpack_require__(/*! path */ \"path\")\n\nconst normalizePackageBin = __webpack_require__(/*! npm-normalize-package-bin */ \"./node_modules/npm-normalize-package-bin/index.js\")\n\nconst defaultRules = [\n  '.npmignore',\n  '.gitignore',\n  '**/.git',\n  '**/.svn',\n  '**/.hg',\n  '**/CVS',\n  '**/.git/**',\n  '**/.svn/**',\n  '**/.hg/**',\n  '**/CVS/**',\n  '/.lock-wscript',\n  '/.wafpickle-*',\n  '/build/config.gypi',\n  'npm-debug.log',\n  '**/.npmrc',\n  '.*.swp',\n  '.DS_Store',\n  '**/.DS_Store/**',\n  '._*',\n  '**/._*/**',\n  '*.orig',\n  '/package-lock.json',\n  '/yarn.lock',\n  'archived-packages/**',\n  'core',\n  '!core/',\n  '!**/core/',\n  '*.core',\n  '*.vgcore',\n  'vgcore.*',\n  'core.+([0-9])',\n]\n\n// There may be others, but :?|<> are handled by node-tar\nconst nameIsBadForWindows = file => /\\*/.test(file)\n\n// a decorator that applies our custom rules to an ignore walker\nconst npmWalker = Class => class Walker extends Class {\n  constructor (opt) {\n    opt = opt || {}\n\n    // the order in which rules are applied.\n    opt.ignoreFiles = [\n      rootBuiltinRules,\n      'package.json',\n      '.npmignore',\n      '.gitignore',\n      packageNecessaryRules\n    ]\n\n    opt.includeEmpty = false\n    opt.path = opt.path || process.cwd()\n    const dirName = path.basename(opt.path)\n    const parentName = path.basename(path.dirname(opt.path))\n    opt.follow =\n      dirName === 'node_modules' ||\n      (parentName === 'node_modules' && /^@/.test(dirName))\n    super(opt)\n\n    // ignore a bunch of things by default at the root level.\n    // also ignore anything in node_modules, except bundled dependencies\n    if (!this.parent) {\n      this.bundled = opt.bundled || []\n      this.bundledScopes = Array.from(new Set(\n        this.bundled.filter(f => /^@/.test(f))\n        .map(f => f.split('/')[0])))\n      const rules = defaultRules.join('\\n') + '\\n'\n      this.packageJsonCache = opt.packageJsonCache || new Map()\n      super.onReadIgnoreFile(rootBuiltinRules, rules, _=>_)\n    } else {\n      this.bundled = []\n      this.bundledScopes = []\n      this.packageJsonCache = this.parent.packageJsonCache\n    }\n  }\n\n  onReaddir (entries) {\n    if (!this.parent) {\n      entries = entries.filter(e =>\n        e !== '.git' &&\n        !(e === 'node_modules' && this.bundled.length === 0)\n      )\n    }\n    return super.onReaddir(entries)\n  }\n\n  filterEntry (entry, partial) {\n    // get the partial path from the root of the walk\n    const p = this.path.substr(this.root.length + 1)\n    const pkgre = /^node_modules\\/(@[^\\/]+\\/?[^\\/]+|[^\\/]+)(\\/.*)?$/\n    const isRoot = !this.parent\n    const pkg = isRoot && pkgre.test(entry) ?\n      entry.replace(pkgre, '$1') : null\n    const rootNM = isRoot && entry === 'node_modules'\n    const rootPJ = isRoot && entry === 'package.json'\n\n    return (\n      // if we're in a bundled package, check with the parent.\n      /^node_modules($|\\/)/i.test(p) ? this.parent.filterEntry(\n          this.basename + '/' + entry, partial)\n\n      // if package is bundled, all files included\n      // also include @scope dirs for bundled scoped deps\n      // they'll be ignored if no files end up in them.\n      // However, this only matters if we're in the root.\n      // node_modules folders elsewhere, like lib/node_modules,\n      // should be included normally unless ignored.\n      : pkg ? -1 !== this.bundled.indexOf(pkg) ||\n        -1 !== this.bundledScopes.indexOf(pkg)\n\n      // only walk top node_modules if we want to bundle something\n      : rootNM ? !!this.bundled.length\n\n      // always include package.json at the root.\n      : rootPJ ? true\n\n      // otherwise, follow ignore-walk's logic\n      : super.filterEntry(entry, partial)\n    )\n  }\n\n  filterEntries () {\n    if (this.ignoreRules['package.json'])\n      this.ignoreRules['.gitignore'] = this.ignoreRules['.npmignore'] = null\n    else if (this.ignoreRules['.npmignore'])\n      this.ignoreRules['.gitignore'] = null\n    this.filterEntries = super.filterEntries\n    super.filterEntries()\n  }\n\n  addIgnoreFile (file, then) {\n    const ig = path.resolve(this.path, file)\n    if (this.packageJsonCache.has(ig))\n      this.onPackageJson(ig, this.packageJsonCache.get(ig), then)\n    else\n      super.addIgnoreFile(file, then)\n  }\n\n  onPackageJson (ig, pkg, then) {\n    this.packageJsonCache.set(ig, pkg)\n\n    // if there's a bin, browser or main, make sure we don't ignore it\n    // also, don't ignore the package.json itself!\n    //\n    // Weird side-effect of this: a readme (etc) file will be included\n    // if it exists anywhere within a folder with a package.json file.\n    // The original intent was only to include these files in the root,\n    // but now users in the wild are dependent on that behavior for\n    // localized documentation and other use cases.  Adding a `/` to\n    // these rules, while tempting and arguably more \"correct\", is a\n    // breaking change.\n    const rules = [\n      pkg.browser ? '!' + pkg.browser : '',\n      pkg.main ? '!' + pkg.main : '',\n      '!package.json',\n      '!npm-shrinkwrap.json',\n      '!@(readme|copying|license|licence|notice|changes|changelog|history){,.*[^~$]}'\n    ]\n    if (pkg.bin) {\n      // always an object, because normalized already\n      for (const key in pkg.bin)\n        rules.push('!' + pkg.bin[key])\n    }\n\n    const data = rules.filter(f => f).join('\\n') + '\\n'\n    super.onReadIgnoreFile(packageNecessaryRules, data, _=>_)\n\n    if (Array.isArray(pkg.files))\n      super.onReadIgnoreFile('package.json', '*\\n' + pkg.files.map(\n        f => '!' + f + '\\n!' + f.replace(/\\/+$/, '') + '/**'\n      ).join('\\n') + '\\n', then)\n    else\n      then()\n  }\n\n  // override parent stat function to completely skip any filenames\n  // that will break windows entirely.\n  // XXX(isaacs) Next major version should make this an error instead.\n  stat (entry, file, dir, then) {\n    if (nameIsBadForWindows(entry))\n      then()\n    else\n      super.stat(entry, file, dir, then)\n  }\n\n  // override parent onstat function to nix all symlinks\n  onstat (st, entry, file, dir, then) {\n    if (st.isSymbolicLink())\n      then()\n    else\n      super.onstat(st, entry, file, dir, then)\n  }\n\n  onReadIgnoreFile (file, data, then) {\n    if (file === 'package.json')\n      try {\n        const ig = path.resolve(this.path, file)\n        this.onPackageJson(ig, normalizePackageBin(JSON.parse(data)), then)\n      } catch (er) {\n        // ignore package.json files that are not json\n        then()\n      }\n    else\n      super.onReadIgnoreFile(file, data, then)\n  }\n\n  sort (a, b) {\n    return sort(a, b)\n  }\n}\n\nclass Walker extends npmWalker(IgnoreWalker) {\n  walker (entry, then) {\n    new Walker(this.walkerOpt(entry)).on('done', then).start()\n  }\n}\n\nclass WalkerSync extends npmWalker(IgnoreWalkerSync) {\n  walker (entry, then) {\n    new WalkerSync(this.walkerOpt(entry)).start()\n    then()\n  }\n}\n\nconst walk = (options, callback) => {\n  options = options || {}\n  const p = new Promise((resolve, reject) => {\n    const bw = new BundleWalker(options)\n    bw.on('done', bundled => {\n      options.bundled = bundled\n      options.packageJsonCache = bw.packageJsonCache\n      new Walker(options).on('done', resolve).on('error', reject).start()\n    })\n    bw.start()\n  })\n  return callback ? p.then(res => callback(null, res), callback) : p\n}\n\nconst walkSync = options => {\n  options = options || {}\n  const bw = new BundleWalkerSync(options).start()\n  options.bundled = bw.result\n  options.packageJsonCache = bw.packageJsonCache\n  const walker = new WalkerSync(options)\n  walker.start()\n  return walker.result\n}\n\n// optimize for compressibility\n// extname, then basename, then locale alphabetically\n// https://twitter.com/isntitvacant/status/1131094910923231232\nconst sort = (a, b) => {\n  const exta = path.extname(a).toLowerCase()\n  const extb = path.extname(b).toLowerCase()\n  const basea = path.basename(a).toLowerCase()\n  const baseb = path.basename(b).toLowerCase()\n\n  return exta.localeCompare(extb) ||\n    basea.localeCompare(baseb) ||\n    a.localeCompare(b)\n}\n\n\nmodule.exports = walk\nwalk.sync = walkSync\nwalk.Walker = Walker\nwalk.WalkerSync = WalkerSync\n\n\n//# sourceURL=webpack:///./node_modules/npm-packlist/index.js?");

/***/ }),

/***/ "./node_modules/npmlog/log.js":
/*!************************************!*\
  !*** ./node_modules/npmlog/log.js ***!
  \************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar Progress = __webpack_require__(/*! are-we-there-yet */ \"./node_modules/are-we-there-yet/index.js\")\nvar Gauge = __webpack_require__(/*! gauge */ \"./node_modules/gauge/index.js\")\nvar EE = __webpack_require__(/*! events */ \"events\").EventEmitter\nvar log = exports = module.exports = new EE()\nvar util = __webpack_require__(/*! util */ \"util\")\n\nvar setBlocking = __webpack_require__(/*! set-blocking */ \"set-blocking\")\nvar consoleControl = __webpack_require__(/*! console-control-strings */ \"./node_modules/console-control-strings/index.js\")\n\nsetBlocking(true)\nvar stream = process.stderr\nObject.defineProperty(log, 'stream', {\n  set: function (newStream) {\n    stream = newStream\n    if (this.gauge) this.gauge.setWriteTo(stream, stream)\n  },\n  get: function () {\n    return stream\n  }\n})\n\n// by default, decide based on tty-ness.\nvar colorEnabled\nlog.useColor = function () {\n  return colorEnabled != null ? colorEnabled : stream.isTTY\n}\n\nlog.enableColor = function () {\n  colorEnabled = true\n  this.gauge.setTheme({hasColor: colorEnabled, hasUnicode: unicodeEnabled})\n}\nlog.disableColor = function () {\n  colorEnabled = false\n  this.gauge.setTheme({hasColor: colorEnabled, hasUnicode: unicodeEnabled})\n}\n\n// default level\nlog.level = 'info'\n\nlog.gauge = new Gauge(stream, {\n  enabled: false, // no progress bars unless asked\n  theme: {hasColor: log.useColor()},\n  template: [\n    {type: 'progressbar', length: 20},\n    {type: 'activityIndicator', kerning: 1, length: 1},\n    {type: 'section', default: ''},\n    ':',\n    {type: 'logline', kerning: 1, default: ''}\n  ]\n})\n\nlog.tracker = new Progress.TrackerGroup()\n\n// we track this separately as we may need to temporarily disable the\n// display of the status bar for our own loggy purposes.\nlog.progressEnabled = log.gauge.isEnabled()\n\nvar unicodeEnabled\n\nlog.enableUnicode = function () {\n  unicodeEnabled = true\n  this.gauge.setTheme({hasColor: this.useColor(), hasUnicode: unicodeEnabled})\n}\n\nlog.disableUnicode = function () {\n  unicodeEnabled = false\n  this.gauge.setTheme({hasColor: this.useColor(), hasUnicode: unicodeEnabled})\n}\n\nlog.setGaugeThemeset = function (themes) {\n  this.gauge.setThemeset(themes)\n}\n\nlog.setGaugeTemplate = function (template) {\n  this.gauge.setTemplate(template)\n}\n\nlog.enableProgress = function () {\n  if (this.progressEnabled) return\n  this.progressEnabled = true\n  this.tracker.on('change', this.showProgress)\n  if (this._pause) return\n  this.gauge.enable()\n}\n\nlog.disableProgress = function () {\n  if (!this.progressEnabled) return\n  this.progressEnabled = false\n  this.tracker.removeListener('change', this.showProgress)\n  this.gauge.disable()\n}\n\nvar trackerConstructors = ['newGroup', 'newItem', 'newStream']\n\nvar mixinLog = function (tracker) {\n  // mixin the public methods from log into the tracker\n  // (except: conflicts and one's we handle specially)\n  Object.keys(log).forEach(function (P) {\n    if (P[0] === '_') return\n    if (trackerConstructors.filter(function (C) { return C === P }).length) return\n    if (tracker[P]) return\n    if (typeof log[P] !== 'function') return\n    var func = log[P]\n    tracker[P] = function () {\n      return func.apply(log, arguments)\n    }\n  })\n  // if the new tracker is a group, make sure any subtrackers get\n  // mixed in too\n  if (tracker instanceof Progress.TrackerGroup) {\n    trackerConstructors.forEach(function (C) {\n      var func = tracker[C]\n      tracker[C] = function () { return mixinLog(func.apply(tracker, arguments)) }\n    })\n  }\n  return tracker\n}\n\n// Add tracker constructors to the top level log object\ntrackerConstructors.forEach(function (C) {\n  log[C] = function () { return mixinLog(this.tracker[C].apply(this.tracker, arguments)) }\n})\n\nlog.clearProgress = function (cb) {\n  if (!this.progressEnabled) return cb && process.nextTick(cb)\n  this.gauge.hide(cb)\n}\n\nlog.showProgress = function (name, completed) {\n  if (!this.progressEnabled) return\n  var values = {}\n  if (name) values.section = name\n  var last = log.record[log.record.length - 1]\n  if (last) {\n    values.subsection = last.prefix\n    var disp = log.disp[last.level] || last.level\n    var logline = this._format(disp, log.style[last.level])\n    if (last.prefix) logline += ' ' + this._format(last.prefix, this.prefixStyle)\n    logline += ' ' + last.message.split(/\\r?\\n/)[0]\n    values.logline = logline\n  }\n  values.completed = completed || this.tracker.completed()\n  this.gauge.show(values)\n}.bind(log) // bind for use in tracker's on-change listener\n\n// temporarily stop emitting, but don't drop\nlog.pause = function () {\n  this._paused = true\n  if (this.progressEnabled) this.gauge.disable()\n}\n\nlog.resume = function () {\n  if (!this._paused) return\n  this._paused = false\n\n  var b = this._buffer\n  this._buffer = []\n  b.forEach(function (m) {\n    this.emitLog(m)\n  }, this)\n  if (this.progressEnabled) this.gauge.enable()\n}\n\nlog._buffer = []\n\nvar id = 0\nlog.record = []\nlog.maxRecordSize = 10000\nlog.log = function (lvl, prefix, message) {\n  var l = this.levels[lvl]\n  if (l === undefined) {\n    return this.emit('error', new Error(util.format(\n      'Undefined log level: %j', lvl)))\n  }\n\n  var a = new Array(arguments.length - 2)\n  var stack = null\n  for (var i = 2; i < arguments.length; i++) {\n    var arg = a[i - 2] = arguments[i]\n\n    // resolve stack traces to a plain string.\n    if (typeof arg === 'object' && arg &&\n        (arg instanceof Error) && arg.stack) {\n\n      Object.defineProperty(arg, 'stack', {\n        value: stack = arg.stack + '',\n        enumerable: true,\n        writable: true\n      })\n    }\n  }\n  if (stack) a.unshift(stack + '\\n')\n  message = util.format.apply(util, a)\n\n  var m = { id: id++,\n            level: lvl,\n            prefix: String(prefix || ''),\n            message: message,\n            messageRaw: a }\n\n  this.emit('log', m)\n  this.emit('log.' + lvl, m)\n  if (m.prefix) this.emit(m.prefix, m)\n\n  this.record.push(m)\n  var mrs = this.maxRecordSize\n  var n = this.record.length - mrs\n  if (n > mrs / 10) {\n    var newSize = Math.floor(mrs * 0.9)\n    this.record = this.record.slice(-1 * newSize)\n  }\n\n  this.emitLog(m)\n}.bind(log)\n\nlog.emitLog = function (m) {\n  if (this._paused) {\n    this._buffer.push(m)\n    return\n  }\n  if (this.progressEnabled) this.gauge.pulse(m.prefix)\n  var l = this.levels[m.level]\n  if (l === undefined) return\n  if (l < this.levels[this.level]) return\n  if (l > 0 && !isFinite(l)) return\n\n  // If 'disp' is null or undefined, use the lvl as a default\n  // Allows: '', 0 as valid disp\n  var disp = log.disp[m.level] != null ? log.disp[m.level] : m.level\n  this.clearProgress()\n  m.message.split(/\\r?\\n/).forEach(function (line) {\n    if (this.heading) {\n      this.write(this.heading, this.headingStyle)\n      this.write(' ')\n    }\n    this.write(disp, log.style[m.level])\n    var p = m.prefix || ''\n    if (p) this.write(' ')\n    this.write(p, this.prefixStyle)\n    this.write(' ' + line + '\\n')\n  }, this)\n  this.showProgress()\n}\n\nlog._format = function (msg, style) {\n  if (!stream) return\n\n  var output = ''\n  if (this.useColor()) {\n    style = style || {}\n    var settings = []\n    if (style.fg) settings.push(style.fg)\n    if (style.bg) settings.push('bg' + style.bg[0].toUpperCase() + style.bg.slice(1))\n    if (style.bold) settings.push('bold')\n    if (style.underline) settings.push('underline')\n    if (style.inverse) settings.push('inverse')\n    if (settings.length) output += consoleControl.color(settings)\n    if (style.beep) output += consoleControl.beep()\n  }\n  output += msg\n  if (this.useColor()) {\n    output += consoleControl.color('reset')\n  }\n  return output\n}\n\nlog.write = function (msg, style) {\n  if (!stream) return\n\n  stream.write(this._format(msg, style))\n}\n\nlog.addLevel = function (lvl, n, style, disp) {\n  // If 'disp' is null or undefined, use the lvl as a default\n  if (disp == null) disp = lvl\n  this.levels[lvl] = n\n  this.style[lvl] = style\n  if (!this[lvl]) {\n    this[lvl] = function () {\n      var a = new Array(arguments.length + 1)\n      a[0] = lvl\n      for (var i = 0; i < arguments.length; i++) {\n        a[i + 1] = arguments[i]\n      }\n      return this.log.apply(this, a)\n    }.bind(this)\n  }\n  this.disp[lvl] = disp\n}\n\nlog.prefixStyle = { fg: 'magenta' }\nlog.headingStyle = { fg: 'white', bg: 'black' }\n\nlog.style = {}\nlog.levels = {}\nlog.disp = {}\nlog.addLevel('silly', -Infinity, { inverse: true }, 'sill')\nlog.addLevel('verbose', 1000, { fg: 'blue', bg: 'black' }, 'verb')\nlog.addLevel('info', 2000, { fg: 'green' })\nlog.addLevel('timing', 2500, { fg: 'green', bg: 'black' })\nlog.addLevel('http', 3000, { fg: 'green', bg: 'black' })\nlog.addLevel('notice', 3500, { fg: 'blue', bg: 'black' })\nlog.addLevel('warn', 4000, { fg: 'black', bg: 'yellow' }, 'WARN')\nlog.addLevel('error', 5000, { fg: 'red', bg: 'black' }, 'ERR!')\nlog.addLevel('silent', Infinity)\n\n// allow 'error' prefix\nlog.on('error', function () {})\n\n\n//# sourceURL=webpack:///./node_modules/npmlog/log.js?");

/***/ }),

/***/ "./node_modules/os-homedir/index.js":
/*!******************************************!*\
  !*** ./node_modules/os-homedir/index.js ***!
  \******************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar os = __webpack_require__(/*! os */ \"os\");\n\nfunction homedir() {\n\tvar env = process.env;\n\tvar home = env.HOME;\n\tvar user = env.LOGNAME || env.USER || env.LNAME || env.USERNAME;\n\n\tif (process.platform === 'win32') {\n\t\treturn env.USERPROFILE || env.HOMEDRIVE + env.HOMEPATH || home || null;\n\t}\n\n\tif (process.platform === 'darwin') {\n\t\treturn home || (user ? '/Users/' + user : null);\n\t}\n\n\tif (process.platform === 'linux') {\n\t\treturn home || (process.getuid() === 0 ? '/root' : (user ? '/home/' + user : null));\n\t}\n\n\treturn home || null;\n}\n\nmodule.exports = typeof os.homedir === 'function' ? os.homedir : homedir;\n\n\n//# sourceURL=webpack:///./node_modules/os-homedir/index.js?");

/***/ }),

/***/ "./node_modules/os-tmpdir/index.js":
/*!*****************************************!*\
  !*** ./node_modules/os-tmpdir/index.js ***!
  \*****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar isWindows = process.platform === 'win32';\nvar trailingSlashRe = isWindows ? /[^:]\\\\$/ : /.\\/$/;\n\n// https://github.com/nodejs/node/blob/3e7a14381497a3b73dda68d05b5130563cdab420/lib/os.js#L25-L43\nmodule.exports = function () {\n\tvar path;\n\n\tif (isWindows) {\n\t\tpath = process.env.TEMP ||\n\t\t\tprocess.env.TMP ||\n\t\t\t(process.env.SystemRoot || process.env.windir) + '\\\\temp';\n\t} else {\n\t\tpath = process.env.TMPDIR ||\n\t\t\tprocess.env.TMP ||\n\t\t\tprocess.env.TEMP ||\n\t\t\t'/tmp';\n\t}\n\n\tif (trailingSlashRe.test(path)) {\n\t\tpath = path.slice(0, -1);\n\t}\n\n\treturn path;\n};\n\n\n//# sourceURL=webpack:///./node_modules/os-tmpdir/index.js?");

/***/ }),

/***/ "./node_modules/osenv/osenv.js":
/*!*************************************!*\
  !*** ./node_modules/osenv/osenv.js ***!
  \*************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("var isWindows = process.platform === 'win32'\nvar path = __webpack_require__(/*! path */ \"path\")\nvar exec = __webpack_require__(/*! child_process */ \"child_process\").exec\nvar osTmpdir = __webpack_require__(/*! os-tmpdir */ \"./node_modules/os-tmpdir/index.js\")\nvar osHomedir = __webpack_require__(/*! os-homedir */ \"./node_modules/os-homedir/index.js\")\n\n// looking up envs is a bit costly.\n// Also, sometimes we want to have a fallback\n// Pass in a callback to wait for the fallback on failures\n// After the first lookup, always returns the same thing.\nfunction memo (key, lookup, fallback) {\n  var fell = false\n  var falling = false\n  exports[key] = function (cb) {\n    var val = lookup()\n    if (!val && !fell && !falling && fallback) {\n      fell = true\n      falling = true\n      exec(fallback, function (er, output, stderr) {\n        falling = false\n        if (er) return // oh well, we tried\n        val = output.trim()\n      })\n    }\n    exports[key] = function (cb) {\n      if (cb) process.nextTick(cb.bind(null, null, val))\n      return val\n    }\n    if (cb && !falling) process.nextTick(cb.bind(null, null, val))\n    return val\n  }\n}\n\nmemo('user', function () {\n  return ( isWindows\n         ? process.env.USERDOMAIN + '\\\\' + process.env.USERNAME\n         : process.env.USER\n         )\n}, 'whoami')\n\nmemo('prompt', function () {\n  return isWindows ? process.env.PROMPT : process.env.PS1\n})\n\nmemo('hostname', function () {\n  return isWindows ? process.env.COMPUTERNAME : process.env.HOSTNAME\n}, 'hostname')\n\nmemo('tmpdir', function () {\n  return osTmpdir()\n})\n\nmemo('home', function () {\n  return osHomedir()\n})\n\nmemo('path', function () {\n  return (process.env.PATH ||\n          process.env.Path ||\n          process.env.path).split(isWindows ? ';' : ':')\n})\n\nmemo('editor', function () {\n  return process.env.EDITOR ||\n         process.env.VISUAL ||\n         (isWindows ? 'notepad.exe' : 'vi')\n})\n\nmemo('shell', function () {\n  return isWindows ? process.env.ComSpec || 'cmd'\n         : process.env.SHELL || 'bash'\n})\n\n\n//# sourceURL=webpack:///./node_modules/osenv/osenv.js?");

/***/ }),

/***/ "./node_modules/rc/index.js":
/*!**********************************!*\
  !*** ./node_modules/rc/index.js ***!
  \**********************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval("var cc   = __webpack_require__(/*! ./lib/utils */ \"./node_modules/rc/lib/utils.js\")\nvar join = __webpack_require__(/*! path */ \"path\").join\nvar deepExtend = __webpack_require__(/*! deep-extend */ \"./node_modules/deep-extend/lib/deep-extend.js\")\nvar etc = '/etc'\nvar win = process.platform === \"win32\"\nvar home = win\n           ? process.env.USERPROFILE\n           : process.env.HOME\n\nmodule.exports = function (name, defaults, argv, parse) {\n  if('string' !== typeof name)\n    throw new Error('rc(name): name *must* be string')\n  if(!argv)\n    argv = __webpack_require__(/*! minimist */ \"minimist\")(process.argv.slice(2))\n  defaults = (\n      'string' === typeof defaults\n    ? cc.json(defaults) : defaults\n    ) || {}\n\n  parse = parse || cc.parse\n\n  var env = cc.env(name + '_')\n\n  var configs = [defaults]\n  var configFiles = []\n  function addConfigFile (file) {\n    if (configFiles.indexOf(file) >= 0) return\n    var fileConfig = cc.file(file)\n    if (fileConfig) {\n      configs.push(parse(fileConfig))\n      configFiles.push(file)\n    }\n  }\n\n  // which files do we look at?\n  if (!win)\n   [join(etc, name, 'config'),\n    join(etc, name + 'rc')].forEach(addConfigFile)\n  if (home)\n   [join(home, '.config', name, 'config'),\n    join(home, '.config', name),\n    join(home, '.' + name, 'config'),\n    join(home, '.' + name + 'rc')].forEach(addConfigFile)\n  addConfigFile(cc.find('.'+name+'rc'))\n  if (env.config) addConfigFile(env.config)\n  if (argv.config) addConfigFile(argv.config)\n\n  return deepExtend.apply(null, configs.concat([\n    env,\n    argv,\n    configFiles.length ? {configs: configFiles, config: configFiles[configFiles.length - 1]} : undefined,\n  ]))\n}\n\n\n//# sourceURL=webpack:///./node_modules/rc/index.js?");

/***/ }),

/***/ "./node_modules/rc/lib/utils.js":
/*!**************************************!*\
  !*** ./node_modules/rc/lib/utils.js ***!
  \**************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar fs   = __webpack_require__(/*! fs */ \"fs\")\nvar ini  = __webpack_require__(/*! ini */ \"ini\")\nvar path = __webpack_require__(/*! path */ \"path\")\nvar stripJsonComments = __webpack_require__(/*! strip-json-comments */ \"./node_modules/strip-json-comments/index.js\")\n\nvar parse = exports.parse = function (content) {\n\n  //if it ends in .json or starts with { then it must be json.\n  //must be done this way, because ini accepts everything.\n  //can't just try and parse it and let it throw if it's not ini.\n  //everything is ini. even json with a syntax error.\n\n  if(/^\\s*{/.test(content))\n    return JSON.parse(stripJsonComments(content))\n  return ini.parse(content)\n\n}\n\nvar file = exports.file = function () {\n  var args = [].slice.call(arguments).filter(function (arg) { return arg != null })\n\n  //path.join breaks if it's a not a string, so just skip this.\n  for(var i in args)\n    if('string' !== typeof args[i])\n      return\n\n  var file = path.join.apply(null, args)\n  var content\n  try {\n    return fs.readFileSync(file,'utf-8')\n  } catch (err) {\n    return\n  }\n}\n\nvar json = exports.json = function () {\n  var content = file.apply(null, arguments)\n  return content ? parse(content) : null\n}\n\nvar env = exports.env = function (prefix, env) {\n  env = env || process.env\n  var obj = {}\n  var l = prefix.length\n  for(var k in env) {\n    if(k.toLowerCase().indexOf(prefix.toLowerCase()) === 0) {\n\n      var keypath = k.substring(l).split('__')\n\n      // Trim empty strings from keypath array\n      var _emptyStringIndex\n      while ((_emptyStringIndex=keypath.indexOf('')) > -1) {\n        keypath.splice(_emptyStringIndex, 1)\n      }\n\n      var cursor = obj\n      keypath.forEach(function _buildSubObj(_subkey,i){\n\n        // (check for _subkey first so we ignore empty strings)\n        // (check for cursor to avoid assignment to primitive objects)\n        if (!_subkey || typeof cursor !== 'object')\n          return\n\n        // If this is the last key, just stuff the value in there\n        // Assigns actual value from env variable to final key\n        // (unless it's just an empty string- in that case use the last valid key)\n        if (i === keypath.length-1)\n          cursor[_subkey] = env[k]\n\n\n        // Build sub-object if nothing already exists at the keypath\n        if (cursor[_subkey] === undefined)\n          cursor[_subkey] = {}\n\n        // Increment cursor used to track the object at the current depth\n        cursor = cursor[_subkey]\n\n      })\n\n    }\n\n  }\n\n  return obj\n}\n\nvar find = exports.find = function () {\n  var rel = path.join.apply(null, [].slice.call(arguments))\n\n  function find(start, rel) {\n    var file = path.join(start, rel)\n    try {\n      fs.statSync(file)\n      return file\n    } catch (err) {\n      if(path.dirname(start) !== start) // root\n        return find(path.dirname(start), rel)\n    }\n  }\n  return find(process.cwd(), rel)\n}\n\n\n\n\n//# sourceURL=webpack:///./node_modules/rc/lib/utils.js?");

/***/ }),

/***/ "./node_modules/sax/lib/sax.js":
/*!*************************************!*\
  !*** ./node_modules/sax/lib/sax.js ***!
  \*************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

eval(";(function (sax) { // wrapper for non-node envs\n  sax.parser = function (strict, opt) { return new SAXParser(strict, opt) }\n  sax.SAXParser = SAXParser\n  sax.SAXStream = SAXStream\n  sax.createStream = createStream\n\n  // When we pass the MAX_BUFFER_LENGTH position, start checking for buffer overruns.\n  // When we check, schedule the next check for MAX_BUFFER_LENGTH - (max(buffer lengths)),\n  // since that's the earliest that a buffer overrun could occur.  This way, checks are\n  // as rare as required, but as often as necessary to ensure never crossing this bound.\n  // Furthermore, buffers are only tested at most once per write(), so passing a very\n  // large string into write() might have undesirable effects, but this is manageable by\n  // the caller, so it is assumed to be safe.  Thus, a call to write() may, in the extreme\n  // edge case, result in creating at most one complete copy of the string passed in.\n  // Set to Infinity to have unlimited buffers.\n  sax.MAX_BUFFER_LENGTH = 64 * 1024\n\n  var buffers = [\n    'comment', 'sgmlDecl', 'textNode', 'tagName', 'doctype',\n    'procInstName', 'procInstBody', 'entity', 'attribName',\n    'attribValue', 'cdata', 'script'\n  ]\n\n  sax.EVENTS = [\n    'text',\n    'processinginstruction',\n    'sgmldeclaration',\n    'doctype',\n    'comment',\n    'opentagstart',\n    'attribute',\n    'opentag',\n    'closetag',\n    'opencdata',\n    'cdata',\n    'closecdata',\n    'error',\n    'end',\n    'ready',\n    'script',\n    'opennamespace',\n    'closenamespace'\n  ]\n\n  function SAXParser (strict, opt) {\n    if (!(this instanceof SAXParser)) {\n      return new SAXParser(strict, opt)\n    }\n\n    var parser = this\n    clearBuffers(parser)\n    parser.q = parser.c = ''\n    parser.bufferCheckPosition = sax.MAX_BUFFER_LENGTH\n    parser.opt = opt || {}\n    parser.opt.lowercase = parser.opt.lowercase || parser.opt.lowercasetags\n    parser.looseCase = parser.opt.lowercase ? 'toLowerCase' : 'toUpperCase'\n    parser.tags = []\n    parser.closed = parser.closedRoot = parser.sawRoot = false\n    parser.tag = parser.error = null\n    parser.strict = !!strict\n    parser.noscript = !!(strict || parser.opt.noscript)\n    parser.state = S.BEGIN\n    parser.strictEntities = parser.opt.strictEntities\n    parser.ENTITIES = parser.strictEntities ? Object.create(sax.XML_ENTITIES) : Object.create(sax.ENTITIES)\n    parser.attribList = []\n\n    // namespaces form a prototype chain.\n    // it always points at the current tag,\n    // which protos to its parent tag.\n    if (parser.opt.xmlns) {\n      parser.ns = Object.create(rootNS)\n    }\n\n    // mostly just for error reporting\n    parser.trackPosition = parser.opt.position !== false\n    if (parser.trackPosition) {\n      parser.position = parser.line = parser.column = 0\n    }\n    emit(parser, 'onready')\n  }\n\n  if (!Object.create) {\n    Object.create = function (o) {\n      function F () {}\n      F.prototype = o\n      var newf = new F()\n      return newf\n    }\n  }\n\n  if (!Object.keys) {\n    Object.keys = function (o) {\n      var a = []\n      for (var i in o) if (o.hasOwnProperty(i)) a.push(i)\n      return a\n    }\n  }\n\n  function checkBufferLength (parser) {\n    var maxAllowed = Math.max(sax.MAX_BUFFER_LENGTH, 10)\n    var maxActual = 0\n    for (var i = 0, l = buffers.length; i < l; i++) {\n      var len = parser[buffers[i]].length\n      if (len > maxAllowed) {\n        // Text/cdata nodes can get big, and since they're buffered,\n        // we can get here under normal conditions.\n        // Avoid issues by emitting the text node now,\n        // so at least it won't get any bigger.\n        switch (buffers[i]) {\n          case 'textNode':\n            closeText(parser)\n            break\n\n          case 'cdata':\n            emitNode(parser, 'oncdata', parser.cdata)\n            parser.cdata = ''\n            break\n\n          case 'script':\n            emitNode(parser, 'onscript', parser.script)\n            parser.script = ''\n            break\n\n          default:\n            error(parser, 'Max buffer length exceeded: ' + buffers[i])\n        }\n      }\n      maxActual = Math.max(maxActual, len)\n    }\n    // schedule the next check for the earliest possible buffer overrun.\n    var m = sax.MAX_BUFFER_LENGTH - maxActual\n    parser.bufferCheckPosition = m + parser.position\n  }\n\n  function clearBuffers (parser) {\n    for (var i = 0, l = buffers.length; i < l; i++) {\n      parser[buffers[i]] = ''\n    }\n  }\n\n  function flushBuffers (parser) {\n    closeText(parser)\n    if (parser.cdata !== '') {\n      emitNode(parser, 'oncdata', parser.cdata)\n      parser.cdata = ''\n    }\n    if (parser.script !== '') {\n      emitNode(parser, 'onscript', parser.script)\n      parser.script = ''\n    }\n  }\n\n  SAXParser.prototype = {\n    end: function () { end(this) },\n    write: write,\n    resume: function () { this.error = null; return this },\n    close: function () { return this.write(null) },\n    flush: function () { flushBuffers(this) }\n  }\n\n  var Stream\n  try {\n    Stream = __webpack_require__(/*! stream */ \"stream\").Stream\n  } catch (ex) {\n    Stream = function () {}\n  }\n\n  var streamWraps = sax.EVENTS.filter(function (ev) {\n    return ev !== 'error' && ev !== 'end'\n  })\n\n  function createStream (strict, opt) {\n    return new SAXStream(strict, opt)\n  }\n\n  function SAXStream (strict, opt) {\n    if (!(this instanceof SAXStream)) {\n      return new SAXStream(strict, opt)\n    }\n\n    Stream.apply(this)\n\n    this._parser = new SAXParser(strict, opt)\n    this.writable = true\n    this.readable = true\n\n    var me = this\n\n    this._parser.onend = function () {\n      me.emit('end')\n    }\n\n    this._parser.onerror = function (er) {\n      me.emit('error', er)\n\n      // if didn't throw, then means error was handled.\n      // go ahead and clear error, so we can write again.\n      me._parser.error = null\n    }\n\n    this._decoder = null\n\n    streamWraps.forEach(function (ev) {\n      Object.defineProperty(me, 'on' + ev, {\n        get: function () {\n          return me._parser['on' + ev]\n        },\n        set: function (h) {\n          if (!h) {\n            me.removeAllListeners(ev)\n            me._parser['on' + ev] = h\n            return h\n          }\n          me.on(ev, h)\n        },\n        enumerable: true,\n        configurable: false\n      })\n    })\n  }\n\n  SAXStream.prototype = Object.create(Stream.prototype, {\n    constructor: {\n      value: SAXStream\n    }\n  })\n\n  SAXStream.prototype.write = function (data) {\n    if (typeof Buffer === 'function' &&\n      typeof Buffer.isBuffer === 'function' &&\n      Buffer.isBuffer(data)) {\n      if (!this._decoder) {\n        var SD = __webpack_require__(/*! string_decoder */ \"string_decoder\").StringDecoder\n        this._decoder = new SD('utf8')\n      }\n      data = this._decoder.write(data)\n    }\n\n    this._parser.write(data.toString())\n    this.emit('data', data)\n    return true\n  }\n\n  SAXStream.prototype.end = function (chunk) {\n    if (chunk && chunk.length) {\n      this.write(chunk)\n    }\n    this._parser.end()\n    return true\n  }\n\n  SAXStream.prototype.on = function (ev, handler) {\n    var me = this\n    if (!me._parser['on' + ev] && streamWraps.indexOf(ev) !== -1) {\n      me._parser['on' + ev] = function () {\n        var args = arguments.length === 1 ? [arguments[0]] : Array.apply(null, arguments)\n        args.splice(0, 0, ev)\n        me.emit.apply(me, args)\n      }\n    }\n\n    return Stream.prototype.on.call(me, ev, handler)\n  }\n\n  // this really needs to be replaced with character classes.\n  // XML allows all manner of ridiculous numbers and digits.\n  var CDATA = '[CDATA['\n  var DOCTYPE = 'DOCTYPE'\n  var XML_NAMESPACE = 'http://www.w3.org/XML/1998/namespace'\n  var XMLNS_NAMESPACE = 'http://www.w3.org/2000/xmlns/'\n  var rootNS = { xml: XML_NAMESPACE, xmlns: XMLNS_NAMESPACE }\n\n  // http://www.w3.org/TR/REC-xml/#NT-NameStartChar\n  // This implementation works on strings, a single character at a time\n  // as such, it cannot ever support astral-plane characters (10000-EFFFF)\n  // without a significant breaking change to either this  parser, or the\n  // JavaScript language.  Implementation of an emoji-capable xml parser\n  // is left as an exercise for the reader.\n  var nameStart = /[:_A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD]/\n\n  var nameBody = /[:_A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD\\u00B7\\u0300-\\u036F\\u203F-\\u2040.\\d-]/\n\n  var entityStart = /[#:_A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD]/\n  var entityBody = /[#:_A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD\\u00B7\\u0300-\\u036F\\u203F-\\u2040.\\d-]/\n\n  function isWhitespace (c) {\n    return c === ' ' || c === '\\n' || c === '\\r' || c === '\\t'\n  }\n\n  function isQuote (c) {\n    return c === '\"' || c === '\\''\n  }\n\n  function isAttribEnd (c) {\n    return c === '>' || isWhitespace(c)\n  }\n\n  function isMatch (regex, c) {\n    return regex.test(c)\n  }\n\n  function notMatch (regex, c) {\n    return !isMatch(regex, c)\n  }\n\n  var S = 0\n  sax.STATE = {\n    BEGIN: S++, // leading byte order mark or whitespace\n    BEGIN_WHITESPACE: S++, // leading whitespace\n    TEXT: S++, // general stuff\n    TEXT_ENTITY: S++, // &amp and such.\n    OPEN_WAKA: S++, // <\n    SGML_DECL: S++, // <!BLARG\n    SGML_DECL_QUOTED: S++, // <!BLARG foo \"bar\n    DOCTYPE: S++, // <!DOCTYPE\n    DOCTYPE_QUOTED: S++, // <!DOCTYPE \"//blah\n    DOCTYPE_DTD: S++, // <!DOCTYPE \"//blah\" [ ...\n    DOCTYPE_DTD_QUOTED: S++, // <!DOCTYPE \"//blah\" [ \"foo\n    COMMENT_STARTING: S++, // <!-\n    COMMENT: S++, // <!--\n    COMMENT_ENDING: S++, // <!-- blah -\n    COMMENT_ENDED: S++, // <!-- blah --\n    CDATA: S++, // <![CDATA[ something\n    CDATA_ENDING: S++, // ]\n    CDATA_ENDING_2: S++, // ]]\n    PROC_INST: S++, // <?hi\n    PROC_INST_BODY: S++, // <?hi there\n    PROC_INST_ENDING: S++, // <?hi \"there\" ?\n    OPEN_TAG: S++, // <strong\n    OPEN_TAG_SLASH: S++, // <strong /\n    ATTRIB: S++, // <a\n    ATTRIB_NAME: S++, // <a foo\n    ATTRIB_NAME_SAW_WHITE: S++, // <a foo _\n    ATTRIB_VALUE: S++, // <a foo=\n    ATTRIB_VALUE_QUOTED: S++, // <a foo=\"bar\n    ATTRIB_VALUE_CLOSED: S++, // <a foo=\"bar\"\n    ATTRIB_VALUE_UNQUOTED: S++, // <a foo=bar\n    ATTRIB_VALUE_ENTITY_Q: S++, // <foo bar=\"&quot;\"\n    ATTRIB_VALUE_ENTITY_U: S++, // <foo bar=&quot\n    CLOSE_TAG: S++, // </a\n    CLOSE_TAG_SAW_WHITE: S++, // </a   >\n    SCRIPT: S++, // <script> ...\n    SCRIPT_ENDING: S++ // <script> ... <\n  }\n\n  sax.XML_ENTITIES = {\n    'amp': '&',\n    'gt': '>',\n    'lt': '<',\n    'quot': '\"',\n    'apos': \"'\"\n  }\n\n  sax.ENTITIES = {\n    'amp': '&',\n    'gt': '>',\n    'lt': '<',\n    'quot': '\"',\n    'apos': \"'\",\n    'AElig': 198,\n    'Aacute': 193,\n    'Acirc': 194,\n    'Agrave': 192,\n    'Aring': 197,\n    'Atilde': 195,\n    'Auml': 196,\n    'Ccedil': 199,\n    'ETH': 208,\n    'Eacute': 201,\n    'Ecirc': 202,\n    'Egrave': 200,\n    'Euml': 203,\n    'Iacute': 205,\n    'Icirc': 206,\n    'Igrave': 204,\n    'Iuml': 207,\n    'Ntilde': 209,\n    'Oacute': 211,\n    'Ocirc': 212,\n    'Ograve': 210,\n    'Oslash': 216,\n    'Otilde': 213,\n    'Ouml': 214,\n    'THORN': 222,\n    'Uacute': 218,\n    'Ucirc': 219,\n    'Ugrave': 217,\n    'Uuml': 220,\n    'Yacute': 221,\n    'aacute': 225,\n    'acirc': 226,\n    'aelig': 230,\n    'agrave': 224,\n    'aring': 229,\n    'atilde': 227,\n    'auml': 228,\n    'ccedil': 231,\n    'eacute': 233,\n    'ecirc': 234,\n    'egrave': 232,\n    'eth': 240,\n    'euml': 235,\n    'iacute': 237,\n    'icirc': 238,\n    'igrave': 236,\n    'iuml': 239,\n    'ntilde': 241,\n    'oacute': 243,\n    'ocirc': 244,\n    'ograve': 242,\n    'oslash': 248,\n    'otilde': 245,\n    'ouml': 246,\n    'szlig': 223,\n    'thorn': 254,\n    'uacute': 250,\n    'ucirc': 251,\n    'ugrave': 249,\n    'uuml': 252,\n    'yacute': 253,\n    'yuml': 255,\n    'copy': 169,\n    'reg': 174,\n    'nbsp': 160,\n    'iexcl': 161,\n    'cent': 162,\n    'pound': 163,\n    'curren': 164,\n    'yen': 165,\n    'brvbar': 166,\n    'sect': 167,\n    'uml': 168,\n    'ordf': 170,\n    'laquo': 171,\n    'not': 172,\n    'shy': 173,\n    'macr': 175,\n    'deg': 176,\n    'plusmn': 177,\n    'sup1': 185,\n    'sup2': 178,\n    'sup3': 179,\n    'acute': 180,\n    'micro': 181,\n    'para': 182,\n    'middot': 183,\n    'cedil': 184,\n    'ordm': 186,\n    'raquo': 187,\n    'frac14': 188,\n    'frac12': 189,\n    'frac34': 190,\n    'iquest': 191,\n    'times': 215,\n    'divide': 247,\n    'OElig': 338,\n    'oelig': 339,\n    'Scaron': 352,\n    'scaron': 353,\n    'Yuml': 376,\n    'fnof': 402,\n    'circ': 710,\n    'tilde': 732,\n    'Alpha': 913,\n    'Beta': 914,\n    'Gamma': 915,\n    'Delta': 916,\n    'Epsilon': 917,\n    'Zeta': 918,\n    'Eta': 919,\n    'Theta': 920,\n    'Iota': 921,\n    'Kappa': 922,\n    'Lambda': 923,\n    'Mu': 924,\n    'Nu': 925,\n    'Xi': 926,\n    'Omicron': 927,\n    'Pi': 928,\n    'Rho': 929,\n    'Sigma': 931,\n    'Tau': 932,\n    'Upsilon': 933,\n    'Phi': 934,\n    'Chi': 935,\n    'Psi': 936,\n    'Omega': 937,\n    'alpha': 945,\n    'beta': 946,\n    'gamma': 947,\n    'delta': 948,\n    'epsilon': 949,\n    'zeta': 950,\n    'eta': 951,\n    'theta': 952,\n    'iota': 953,\n    'kappa': 954,\n    'lambda': 955,\n    'mu': 956,\n    'nu': 957,\n    'xi': 958,\n    'omicron': 959,\n    'pi': 960,\n    'rho': 961,\n    'sigmaf': 962,\n    'sigma': 963,\n    'tau': 964,\n    'upsilon': 965,\n    'phi': 966,\n    'chi': 967,\n    'psi': 968,\n    'omega': 969,\n    'thetasym': 977,\n    'upsih': 978,\n    'piv': 982,\n    'ensp': 8194,\n    'emsp': 8195,\n    'thinsp': 8201,\n    'zwnj': 8204,\n    'zwj': 8205,\n    'lrm': 8206,\n    'rlm': 8207,\n    'ndash': 8211,\n    'mdash': 8212,\n    'lsquo': 8216,\n    'rsquo': 8217,\n    'sbquo': 8218,\n    'ldquo': 8220,\n    'rdquo': 8221,\n    'bdquo': 8222,\n    'dagger': 8224,\n    'Dagger': 8225,\n    'bull': 8226,\n    'hellip': 8230,\n    'permil': 8240,\n    'prime': 8242,\n    'Prime': 8243,\n    'lsaquo': 8249,\n    'rsaquo': 8250,\n    'oline': 8254,\n    'frasl': 8260,\n    'euro': 8364,\n    'image': 8465,\n    'weierp': 8472,\n    'real': 8476,\n    'trade': 8482,\n    'alefsym': 8501,\n    'larr': 8592,\n    'uarr': 8593,\n    'rarr': 8594,\n    'darr': 8595,\n    'harr': 8596,\n    'crarr': 8629,\n    'lArr': 8656,\n    'uArr': 8657,\n    'rArr': 8658,\n    'dArr': 8659,\n    'hArr': 8660,\n    'forall': 8704,\n    'part': 8706,\n    'exist': 8707,\n    'empty': 8709,\n    'nabla': 8711,\n    'isin': 8712,\n    'notin': 8713,\n    'ni': 8715,\n    'prod': 8719,\n    'sum': 8721,\n    'minus': 8722,\n    'lowast': 8727,\n    'radic': 8730,\n    'prop': 8733,\n    'infin': 8734,\n    'ang': 8736,\n    'and': 8743,\n    'or': 8744,\n    'cap': 8745,\n    'cup': 8746,\n    'int': 8747,\n    'there4': 8756,\n    'sim': 8764,\n    'cong': 8773,\n    'asymp': 8776,\n    'ne': 8800,\n    'equiv': 8801,\n    'le': 8804,\n    'ge': 8805,\n    'sub': 8834,\n    'sup': 8835,\n    'nsub': 8836,\n    'sube': 8838,\n    'supe': 8839,\n    'oplus': 8853,\n    'otimes': 8855,\n    'perp': 8869,\n    'sdot': 8901,\n    'lceil': 8968,\n    'rceil': 8969,\n    'lfloor': 8970,\n    'rfloor': 8971,\n    'lang': 9001,\n    'rang': 9002,\n    'loz': 9674,\n    'spades': 9824,\n    'clubs': 9827,\n    'hearts': 9829,\n    'diams': 9830\n  }\n\n  Object.keys(sax.ENTITIES).forEach(function (key) {\n    var e = sax.ENTITIES[key]\n    var s = typeof e === 'number' ? String.fromCharCode(e) : e\n    sax.ENTITIES[key] = s\n  })\n\n  for (var s in sax.STATE) {\n    sax.STATE[sax.STATE[s]] = s\n  }\n\n  // shorthand\n  S = sax.STATE\n\n  function emit (parser, event, data) {\n    parser[event] && parser[event](data)\n  }\n\n  function emitNode (parser, nodeType, data) {\n    if (parser.textNode) closeText(parser)\n    emit(parser, nodeType, data)\n  }\n\n  function closeText (parser) {\n    parser.textNode = textopts(parser.opt, parser.textNode)\n    if (parser.textNode) emit(parser, 'ontext', parser.textNode)\n    parser.textNode = ''\n  }\n\n  function textopts (opt, text) {\n    if (opt.trim) text = text.trim()\n    if (opt.normalize) text = text.replace(/\\s+/g, ' ')\n    return text\n  }\n\n  function error (parser, er) {\n    closeText(parser)\n    if (parser.trackPosition) {\n      er += '\\nLine: ' + parser.line +\n        '\\nColumn: ' + parser.column +\n        '\\nChar: ' + parser.c\n    }\n    er = new Error(er)\n    parser.error = er\n    emit(parser, 'onerror', er)\n    return parser\n  }\n\n  function end (parser) {\n    if (parser.sawRoot && !parser.closedRoot) strictFail(parser, 'Unclosed root tag')\n    if ((parser.state !== S.BEGIN) &&\n      (parser.state !== S.BEGIN_WHITESPACE) &&\n      (parser.state !== S.TEXT)) {\n      error(parser, 'Unexpected end')\n    }\n    closeText(parser)\n    parser.c = ''\n    parser.closed = true\n    emit(parser, 'onend')\n    SAXParser.call(parser, parser.strict, parser.opt)\n    return parser\n  }\n\n  function strictFail (parser, message) {\n    if (typeof parser !== 'object' || !(parser instanceof SAXParser)) {\n      throw new Error('bad call to strictFail')\n    }\n    if (parser.strict) {\n      error(parser, message)\n    }\n  }\n\n  function newTag (parser) {\n    if (!parser.strict) parser.tagName = parser.tagName[parser.looseCase]()\n    var parent = parser.tags[parser.tags.length - 1] || parser\n    var tag = parser.tag = { name: parser.tagName, attributes: {} }\n\n    // will be overridden if tag contails an xmlns=\"foo\" or xmlns:foo=\"bar\"\n    if (parser.opt.xmlns) {\n      tag.ns = parent.ns\n    }\n    parser.attribList.length = 0\n    emitNode(parser, 'onopentagstart', tag)\n  }\n\n  function qname (name, attribute) {\n    var i = name.indexOf(':')\n    var qualName = i < 0 ? [ '', name ] : name.split(':')\n    var prefix = qualName[0]\n    var local = qualName[1]\n\n    // <x \"xmlns\"=\"http://foo\">\n    if (attribute && name === 'xmlns') {\n      prefix = 'xmlns'\n      local = ''\n    }\n\n    return { prefix: prefix, local: local }\n  }\n\n  function attrib (parser) {\n    if (!parser.strict) {\n      parser.attribName = parser.attribName[parser.looseCase]()\n    }\n\n    if (parser.attribList.indexOf(parser.attribName) !== -1 ||\n      parser.tag.attributes.hasOwnProperty(parser.attribName)) {\n      parser.attribName = parser.attribValue = ''\n      return\n    }\n\n    if (parser.opt.xmlns) {\n      var qn = qname(parser.attribName, true)\n      var prefix = qn.prefix\n      var local = qn.local\n\n      if (prefix === 'xmlns') {\n        // namespace binding attribute. push the binding into scope\n        if (local === 'xml' && parser.attribValue !== XML_NAMESPACE) {\n          strictFail(parser,\n            'xml: prefix must be bound to ' + XML_NAMESPACE + '\\n' +\n            'Actual: ' + parser.attribValue)\n        } else if (local === 'xmlns' && parser.attribValue !== XMLNS_NAMESPACE) {\n          strictFail(parser,\n            'xmlns: prefix must be bound to ' + XMLNS_NAMESPACE + '\\n' +\n            'Actual: ' + parser.attribValue)\n        } else {\n          var tag = parser.tag\n          var parent = parser.tags[parser.tags.length - 1] || parser\n          if (tag.ns === parent.ns) {\n            tag.ns = Object.create(parent.ns)\n          }\n          tag.ns[local] = parser.attribValue\n        }\n      }\n\n      // defer onattribute events until all attributes have been seen\n      // so any new bindings can take effect. preserve attribute order\n      // so deferred events can be emitted in document order\n      parser.attribList.push([parser.attribName, parser.attribValue])\n    } else {\n      // in non-xmlns mode, we can emit the event right away\n      parser.tag.attributes[parser.attribName] = parser.attribValue\n      emitNode(parser, 'onattribute', {\n        name: parser.attribName,\n        value: parser.attribValue\n      })\n    }\n\n    parser.attribName = parser.attribValue = ''\n  }\n\n  function openTag (parser, selfClosing) {\n    if (parser.opt.xmlns) {\n      // emit namespace binding events\n      var tag = parser.tag\n\n      // add namespace info to tag\n      var qn = qname(parser.tagName)\n      tag.prefix = qn.prefix\n      tag.local = qn.local\n      tag.uri = tag.ns[qn.prefix] || ''\n\n      if (tag.prefix && !tag.uri) {\n        strictFail(parser, 'Unbound namespace prefix: ' +\n          JSON.stringify(parser.tagName))\n        tag.uri = qn.prefix\n      }\n\n      var parent = parser.tags[parser.tags.length - 1] || parser\n      if (tag.ns && parent.ns !== tag.ns) {\n        Object.keys(tag.ns).forEach(function (p) {\n          emitNode(parser, 'onopennamespace', {\n            prefix: p,\n            uri: tag.ns[p]\n          })\n        })\n      }\n\n      // handle deferred onattribute events\n      // Note: do not apply default ns to attributes:\n      //   http://www.w3.org/TR/REC-xml-names/#defaulting\n      for (var i = 0, l = parser.attribList.length; i < l; i++) {\n        var nv = parser.attribList[i]\n        var name = nv[0]\n        var value = nv[1]\n        var qualName = qname(name, true)\n        var prefix = qualName.prefix\n        var local = qualName.local\n        var uri = prefix === '' ? '' : (tag.ns[prefix] || '')\n        var a = {\n          name: name,\n          value: value,\n          prefix: prefix,\n          local: local,\n          uri: uri\n        }\n\n        // if there's any attributes with an undefined namespace,\n        // then fail on them now.\n        if (prefix && prefix !== 'xmlns' && !uri) {\n          strictFail(parser, 'Unbound namespace prefix: ' +\n            JSON.stringify(prefix))\n          a.uri = prefix\n        }\n        parser.tag.attributes[name] = a\n        emitNode(parser, 'onattribute', a)\n      }\n      parser.attribList.length = 0\n    }\n\n    parser.tag.isSelfClosing = !!selfClosing\n\n    // process the tag\n    parser.sawRoot = true\n    parser.tags.push(parser.tag)\n    emitNode(parser, 'onopentag', parser.tag)\n    if (!selfClosing) {\n      // special case for <script> in non-strict mode.\n      if (!parser.noscript && parser.tagName.toLowerCase() === 'script') {\n        parser.state = S.SCRIPT\n      } else {\n        parser.state = S.TEXT\n      }\n      parser.tag = null\n      parser.tagName = ''\n    }\n    parser.attribName = parser.attribValue = ''\n    parser.attribList.length = 0\n  }\n\n  function closeTag (parser) {\n    if (!parser.tagName) {\n      strictFail(parser, 'Weird empty close tag.')\n      parser.textNode += '</>'\n      parser.state = S.TEXT\n      return\n    }\n\n    if (parser.script) {\n      if (parser.tagName !== 'script') {\n        parser.script += '</' + parser.tagName + '>'\n        parser.tagName = ''\n        parser.state = S.SCRIPT\n        return\n      }\n      emitNode(parser, 'onscript', parser.script)\n      parser.script = ''\n    }\n\n    // first make sure that the closing tag actually exists.\n    // <a><b></c></b></a> will close everything, otherwise.\n    var t = parser.tags.length\n    var tagName = parser.tagName\n    if (!parser.strict) {\n      tagName = tagName[parser.looseCase]()\n    }\n    var closeTo = tagName\n    while (t--) {\n      var close = parser.tags[t]\n      if (close.name !== closeTo) {\n        // fail the first time in strict mode\n        strictFail(parser, 'Unexpected close tag')\n      } else {\n        break\n      }\n    }\n\n    // didn't find it.  we already failed for strict, so just abort.\n    if (t < 0) {\n      strictFail(parser, 'Unmatched closing tag: ' + parser.tagName)\n      parser.textNode += '</' + parser.tagName + '>'\n      parser.state = S.TEXT\n      return\n    }\n    parser.tagName = tagName\n    var s = parser.tags.length\n    while (s-- > t) {\n      var tag = parser.tag = parser.tags.pop()\n      parser.tagName = parser.tag.name\n      emitNode(parser, 'onclosetag', parser.tagName)\n\n      var x = {}\n      for (var i in tag.ns) {\n        x[i] = tag.ns[i]\n      }\n\n      var parent = parser.tags[parser.tags.length - 1] || parser\n      if (parser.opt.xmlns && tag.ns !== parent.ns) {\n        // remove namespace bindings introduced by tag\n        Object.keys(tag.ns).forEach(function (p) {\n          var n = tag.ns[p]\n          emitNode(parser, 'onclosenamespace', { prefix: p, uri: n })\n        })\n      }\n    }\n    if (t === 0) parser.closedRoot = true\n    parser.tagName = parser.attribValue = parser.attribName = ''\n    parser.attribList.length = 0\n    parser.state = S.TEXT\n  }\n\n  function parseEntity (parser) {\n    var entity = parser.entity\n    var entityLC = entity.toLowerCase()\n    var num\n    var numStr = ''\n\n    if (parser.ENTITIES[entity]) {\n      return parser.ENTITIES[entity]\n    }\n    if (parser.ENTITIES[entityLC]) {\n      return parser.ENTITIES[entityLC]\n    }\n    entity = entityLC\n    if (entity.charAt(0) === '#') {\n      if (entity.charAt(1) === 'x') {\n        entity = entity.slice(2)\n        num = parseInt(entity, 16)\n        numStr = num.toString(16)\n      } else {\n        entity = entity.slice(1)\n        num = parseInt(entity, 10)\n        numStr = num.toString(10)\n      }\n    }\n    entity = entity.replace(/^0+/, '')\n    if (isNaN(num) || numStr.toLowerCase() !== entity) {\n      strictFail(parser, 'Invalid character entity')\n      return '&' + parser.entity + ';'\n    }\n\n    return String.fromCodePoint(num)\n  }\n\n  function beginWhiteSpace (parser, c) {\n    if (c === '<') {\n      parser.state = S.OPEN_WAKA\n      parser.startTagPosition = parser.position\n    } else if (!isWhitespace(c)) {\n      // have to process this as a text node.\n      // weird, but happens.\n      strictFail(parser, 'Non-whitespace before first tag.')\n      parser.textNode = c\n      parser.state = S.TEXT\n    }\n  }\n\n  function charAt (chunk, i) {\n    var result = ''\n    if (i < chunk.length) {\n      result = chunk.charAt(i)\n    }\n    return result\n  }\n\n  function write (chunk) {\n    var parser = this\n    if (this.error) {\n      throw this.error\n    }\n    if (parser.closed) {\n      return error(parser,\n        'Cannot write after close. Assign an onready handler.')\n    }\n    if (chunk === null) {\n      return end(parser)\n    }\n    if (typeof chunk === 'object') {\n      chunk = chunk.toString()\n    }\n    var i = 0\n    var c = ''\n    while (true) {\n      c = charAt(chunk, i++)\n      parser.c = c\n\n      if (!c) {\n        break\n      }\n\n      if (parser.trackPosition) {\n        parser.position++\n        if (c === '\\n') {\n          parser.line++\n          parser.column = 0\n        } else {\n          parser.column++\n        }\n      }\n\n      switch (parser.state) {\n        case S.BEGIN:\n          parser.state = S.BEGIN_WHITESPACE\n          if (c === '\\uFEFF') {\n            continue\n          }\n          beginWhiteSpace(parser, c)\n          continue\n\n        case S.BEGIN_WHITESPACE:\n          beginWhiteSpace(parser, c)\n          continue\n\n        case S.TEXT:\n          if (parser.sawRoot && !parser.closedRoot) {\n            var starti = i - 1\n            while (c && c !== '<' && c !== '&') {\n              c = charAt(chunk, i++)\n              if (c && parser.trackPosition) {\n                parser.position++\n                if (c === '\\n') {\n                  parser.line++\n                  parser.column = 0\n                } else {\n                  parser.column++\n                }\n              }\n            }\n            parser.textNode += chunk.substring(starti, i - 1)\n          }\n          if (c === '<' && !(parser.sawRoot && parser.closedRoot && !parser.strict)) {\n            parser.state = S.OPEN_WAKA\n            parser.startTagPosition = parser.position\n          } else {\n            if (!isWhitespace(c) && (!parser.sawRoot || parser.closedRoot)) {\n              strictFail(parser, 'Text data outside of root node.')\n            }\n            if (c === '&') {\n              parser.state = S.TEXT_ENTITY\n            } else {\n              parser.textNode += c\n            }\n          }\n          continue\n\n        case S.SCRIPT:\n          // only non-strict\n          if (c === '<') {\n            parser.state = S.SCRIPT_ENDING\n          } else {\n            parser.script += c\n          }\n          continue\n\n        case S.SCRIPT_ENDING:\n          if (c === '/') {\n            parser.state = S.CLOSE_TAG\n          } else {\n            parser.script += '<' + c\n            parser.state = S.SCRIPT\n          }\n          continue\n\n        case S.OPEN_WAKA:\n          // either a /, ?, !, or text is coming next.\n          if (c === '!') {\n            parser.state = S.SGML_DECL\n            parser.sgmlDecl = ''\n          } else if (isWhitespace(c)) {\n            // wait for it...\n          } else if (isMatch(nameStart, c)) {\n            parser.state = S.OPEN_TAG\n            parser.tagName = c\n          } else if (c === '/') {\n            parser.state = S.CLOSE_TAG\n            parser.tagName = ''\n          } else if (c === '?') {\n            parser.state = S.PROC_INST\n            parser.procInstName = parser.procInstBody = ''\n          } else {\n            strictFail(parser, 'Unencoded <')\n            // if there was some whitespace, then add that in.\n            if (parser.startTagPosition + 1 < parser.position) {\n              var pad = parser.position - parser.startTagPosition\n              c = new Array(pad).join(' ') + c\n            }\n            parser.textNode += '<' + c\n            parser.state = S.TEXT\n          }\n          continue\n\n        case S.SGML_DECL:\n          if ((parser.sgmlDecl + c).toUpperCase() === CDATA) {\n            emitNode(parser, 'onopencdata')\n            parser.state = S.CDATA\n            parser.sgmlDecl = ''\n            parser.cdata = ''\n          } else if (parser.sgmlDecl + c === '--') {\n            parser.state = S.COMMENT\n            parser.comment = ''\n            parser.sgmlDecl = ''\n          } else if ((parser.sgmlDecl + c).toUpperCase() === DOCTYPE) {\n            parser.state = S.DOCTYPE\n            if (parser.doctype || parser.sawRoot) {\n              strictFail(parser,\n                'Inappropriately located doctype declaration')\n            }\n            parser.doctype = ''\n            parser.sgmlDecl = ''\n          } else if (c === '>') {\n            emitNode(parser, 'onsgmldeclaration', parser.sgmlDecl)\n            parser.sgmlDecl = ''\n            parser.state = S.TEXT\n          } else if (isQuote(c)) {\n            parser.state = S.SGML_DECL_QUOTED\n            parser.sgmlDecl += c\n          } else {\n            parser.sgmlDecl += c\n          }\n          continue\n\n        case S.SGML_DECL_QUOTED:\n          if (c === parser.q) {\n            parser.state = S.SGML_DECL\n            parser.q = ''\n          }\n          parser.sgmlDecl += c\n          continue\n\n        case S.DOCTYPE:\n          if (c === '>') {\n            parser.state = S.TEXT\n            emitNode(parser, 'ondoctype', parser.doctype)\n            parser.doctype = true // just remember that we saw it.\n          } else {\n            parser.doctype += c\n            if (c === '[') {\n              parser.state = S.DOCTYPE_DTD\n            } else if (isQuote(c)) {\n              parser.state = S.DOCTYPE_QUOTED\n              parser.q = c\n            }\n          }\n          continue\n\n        case S.DOCTYPE_QUOTED:\n          parser.doctype += c\n          if (c === parser.q) {\n            parser.q = ''\n            parser.state = S.DOCTYPE\n          }\n          continue\n\n        case S.DOCTYPE_DTD:\n          parser.doctype += c\n          if (c === ']') {\n            parser.state = S.DOCTYPE\n          } else if (isQuote(c)) {\n            parser.state = S.DOCTYPE_DTD_QUOTED\n            parser.q = c\n          }\n          continue\n\n        case S.DOCTYPE_DTD_QUOTED:\n          parser.doctype += c\n          if (c === parser.q) {\n            parser.state = S.DOCTYPE_DTD\n            parser.q = ''\n          }\n          continue\n\n        case S.COMMENT:\n          if (c === '-') {\n            parser.state = S.COMMENT_ENDING\n          } else {\n            parser.comment += c\n          }\n          continue\n\n        case S.COMMENT_ENDING:\n          if (c === '-') {\n            parser.state = S.COMMENT_ENDED\n            parser.comment = textopts(parser.opt, parser.comment)\n            if (parser.comment) {\n              emitNode(parser, 'oncomment', parser.comment)\n            }\n            parser.comment = ''\n          } else {\n            parser.comment += '-' + c\n            parser.state = S.COMMENT\n          }\n          continue\n\n        case S.COMMENT_ENDED:\n          if (c !== '>') {\n            strictFail(parser, 'Malformed comment')\n            // allow <!-- blah -- bloo --> in non-strict mode,\n            // which is a comment of \" blah -- bloo \"\n            parser.comment += '--' + c\n            parser.state = S.COMMENT\n          } else {\n            parser.state = S.TEXT\n          }\n          continue\n\n        case S.CDATA:\n          if (c === ']') {\n            parser.state = S.CDATA_ENDING\n          } else {\n            parser.cdata += c\n          }\n          continue\n\n        case S.CDATA_ENDING:\n          if (c === ']') {\n            parser.state = S.CDATA_ENDING_2\n          } else {\n            parser.cdata += ']' + c\n            parser.state = S.CDATA\n          }\n          continue\n\n        case S.CDATA_ENDING_2:\n          if (c === '>') {\n            if (parser.cdata) {\n              emitNode(parser, 'oncdata', parser.cdata)\n            }\n            emitNode(parser, 'onclosecdata')\n            parser.cdata = ''\n            parser.state = S.TEXT\n          } else if (c === ']') {\n            parser.cdata += ']'\n          } else {\n            parser.cdata += ']]' + c\n            parser.state = S.CDATA\n          }\n          continue\n\n        case S.PROC_INST:\n          if (c === '?') {\n            parser.state = S.PROC_INST_ENDING\n          } else if (isWhitespace(c)) {\n            parser.state = S.PROC_INST_BODY\n          } else {\n            parser.procInstName += c\n          }\n          continue\n\n        case S.PROC_INST_BODY:\n          if (!parser.procInstBody && isWhitespace(c)) {\n            continue\n          } else if (c === '?') {\n            parser.state = S.PROC_INST_ENDING\n          } else {\n            parser.procInstBody += c\n          }\n          continue\n\n        case S.PROC_INST_ENDING:\n          if (c === '>') {\n            emitNode(parser, 'onprocessinginstruction', {\n              name: parser.procInstName,\n              body: parser.procInstBody\n            })\n            parser.procInstName = parser.procInstBody = ''\n            parser.state = S.TEXT\n          } else {\n            parser.procInstBody += '?' + c\n            parser.state = S.PROC_INST_BODY\n          }\n          continue\n\n        case S.OPEN_TAG:\n          if (isMatch(nameBody, c)) {\n            parser.tagName += c\n          } else {\n            newTag(parser)\n            if (c === '>') {\n              openTag(parser)\n            } else if (c === '/') {\n              parser.state = S.OPEN_TAG_SLASH\n            } else {\n              if (!isWhitespace(c)) {\n                strictFail(parser, 'Invalid character in tag name')\n              }\n              parser.state = S.ATTRIB\n            }\n          }\n          continue\n\n        case S.OPEN_TAG_SLASH:\n          if (c === '>') {\n            openTag(parser, true)\n            closeTag(parser)\n          } else {\n            strictFail(parser, 'Forward-slash in opening tag not followed by >')\n            parser.state = S.ATTRIB\n          }\n          continue\n\n        case S.ATTRIB:\n          // haven't read the attribute name yet.\n          if (isWhitespace(c)) {\n            continue\n          } else if (c === '>') {\n            openTag(parser)\n          } else if (c === '/') {\n            parser.state = S.OPEN_TAG_SLASH\n          } else if (isMatch(nameStart, c)) {\n            parser.attribName = c\n            parser.attribValue = ''\n            parser.state = S.ATTRIB_NAME\n          } else {\n            strictFail(parser, 'Invalid attribute name')\n          }\n          continue\n\n        case S.ATTRIB_NAME:\n          if (c === '=') {\n            parser.state = S.ATTRIB_VALUE\n          } else if (c === '>') {\n            strictFail(parser, 'Attribute without value')\n            parser.attribValue = parser.attribName\n            attrib(parser)\n            openTag(parser)\n          } else if (isWhitespace(c)) {\n            parser.state = S.ATTRIB_NAME_SAW_WHITE\n          } else if (isMatch(nameBody, c)) {\n            parser.attribName += c\n          } else {\n            strictFail(parser, 'Invalid attribute name')\n          }\n          continue\n\n        case S.ATTRIB_NAME_SAW_WHITE:\n          if (c === '=') {\n            parser.state = S.ATTRIB_VALUE\n          } else if (isWhitespace(c)) {\n            continue\n          } else {\n            strictFail(parser, 'Attribute without value')\n            parser.tag.attributes[parser.attribName] = ''\n            parser.attribValue = ''\n            emitNode(parser, 'onattribute', {\n              name: parser.attribName,\n              value: ''\n            })\n            parser.attribName = ''\n            if (c === '>') {\n              openTag(parser)\n            } else if (isMatch(nameStart, c)) {\n              parser.attribName = c\n              parser.state = S.ATTRIB_NAME\n            } else {\n              strictFail(parser, 'Invalid attribute name')\n              parser.state = S.ATTRIB\n            }\n          }\n          continue\n\n        case S.ATTRIB_VALUE:\n          if (isWhitespace(c)) {\n            continue\n          } else if (isQuote(c)) {\n            parser.q = c\n            parser.state = S.ATTRIB_VALUE_QUOTED\n          } else {\n            strictFail(parser, 'Unquoted attribute value')\n            parser.state = S.ATTRIB_VALUE_UNQUOTED\n            parser.attribValue = c\n          }\n          continue\n\n        case S.ATTRIB_VALUE_QUOTED:\n          if (c !== parser.q) {\n            if (c === '&') {\n              parser.state = S.ATTRIB_VALUE_ENTITY_Q\n            } else {\n              parser.attribValue += c\n            }\n            continue\n          }\n          attrib(parser)\n          parser.q = ''\n          parser.state = S.ATTRIB_VALUE_CLOSED\n          continue\n\n        case S.ATTRIB_VALUE_CLOSED:\n          if (isWhitespace(c)) {\n            parser.state = S.ATTRIB\n          } else if (c === '>') {\n            openTag(parser)\n          } else if (c === '/') {\n            parser.state = S.OPEN_TAG_SLASH\n          } else if (isMatch(nameStart, c)) {\n            strictFail(parser, 'No whitespace between attributes')\n            parser.attribName = c\n            parser.attribValue = ''\n            parser.state = S.ATTRIB_NAME\n          } else {\n            strictFail(parser, 'Invalid attribute name')\n          }\n          continue\n\n        case S.ATTRIB_VALUE_UNQUOTED:\n          if (!isAttribEnd(c)) {\n            if (c === '&') {\n              parser.state = S.ATTRIB_VALUE_ENTITY_U\n            } else {\n              parser.attribValue += c\n            }\n            continue\n          }\n          attrib(parser)\n          if (c === '>') {\n            openTag(parser)\n          } else {\n            parser.state = S.ATTRIB\n          }\n          continue\n\n        case S.CLOSE_TAG:\n          if (!parser.tagName) {\n            if (isWhitespace(c)) {\n              continue\n            } else if (notMatch(nameStart, c)) {\n              if (parser.script) {\n                parser.script += '</' + c\n                parser.state = S.SCRIPT\n              } else {\n                strictFail(parser, 'Invalid tagname in closing tag.')\n              }\n            } else {\n              parser.tagName = c\n            }\n          } else if (c === '>') {\n            closeTag(parser)\n          } else if (isMatch(nameBody, c)) {\n            parser.tagName += c\n          } else if (parser.script) {\n            parser.script += '</' + parser.tagName\n            parser.tagName = ''\n            parser.state = S.SCRIPT\n          } else {\n            if (!isWhitespace(c)) {\n              strictFail(parser, 'Invalid tagname in closing tag')\n            }\n            parser.state = S.CLOSE_TAG_SAW_WHITE\n          }\n          continue\n\n        case S.CLOSE_TAG_SAW_WHITE:\n          if (isWhitespace(c)) {\n            continue\n          }\n          if (c === '>') {\n            closeTag(parser)\n          } else {\n            strictFail(parser, 'Invalid characters in closing tag')\n          }\n          continue\n\n        case S.TEXT_ENTITY:\n        case S.ATTRIB_VALUE_ENTITY_Q:\n        case S.ATTRIB_VALUE_ENTITY_U:\n          var returnState\n          var buffer\n          switch (parser.state) {\n            case S.TEXT_ENTITY:\n              returnState = S.TEXT\n              buffer = 'textNode'\n              break\n\n            case S.ATTRIB_VALUE_ENTITY_Q:\n              returnState = S.ATTRIB_VALUE_QUOTED\n              buffer = 'attribValue'\n              break\n\n            case S.ATTRIB_VALUE_ENTITY_U:\n              returnState = S.ATTRIB_VALUE_UNQUOTED\n              buffer = 'attribValue'\n              break\n          }\n\n          if (c === ';') {\n            parser[buffer] += parseEntity(parser)\n            parser.entity = ''\n            parser.state = returnState\n          } else if (isMatch(parser.entity.length ? entityBody : entityStart, c)) {\n            parser.entity += c\n          } else {\n            strictFail(parser, 'Invalid character in entity name')\n            parser[buffer] += '&' + parser.entity + c\n            parser.entity = ''\n            parser.state = returnState\n          }\n\n          continue\n\n        default:\n          throw new Error(parser, 'Unknown state: ' + parser.state)\n      }\n    } // while\n\n    if (parser.position >= parser.bufferCheckPosition) {\n      checkBufferLength(parser)\n    }\n    return parser\n  }\n\n  /*! http://mths.be/fromcodepoint v0.1.0 by @mathias */\n  /* istanbul ignore next */\n  if (!String.fromCodePoint) {\n    (function () {\n      var stringFromCharCode = String.fromCharCode\n      var floor = Math.floor\n      var fromCodePoint = function () {\n        var MAX_SIZE = 0x4000\n        var codeUnits = []\n        var highSurrogate\n        var lowSurrogate\n        var index = -1\n        var length = arguments.length\n        if (!length) {\n          return ''\n        }\n        var result = ''\n        while (++index < length) {\n          var codePoint = Number(arguments[index])\n          if (\n            !isFinite(codePoint) || // `NaN`, `+Infinity`, or `-Infinity`\n            codePoint < 0 || // not a valid Unicode code point\n            codePoint > 0x10FFFF || // not a valid Unicode code point\n            floor(codePoint) !== codePoint // not an integer\n          ) {\n            throw RangeError('Invalid code point: ' + codePoint)\n          }\n          if (codePoint <= 0xFFFF) { // BMP code point\n            codeUnits.push(codePoint)\n          } else { // Astral code point; split in surrogate halves\n            // http://mathiasbynens.be/notes/javascript-encoding#surrogate-formulae\n            codePoint -= 0x10000\n            highSurrogate = (codePoint >> 10) + 0xD800\n            lowSurrogate = (codePoint % 0x400) + 0xDC00\n            codeUnits.push(highSurrogate, lowSurrogate)\n          }\n          if (index + 1 === length || codeUnits.length > MAX_SIZE) {\n            result += stringFromCharCode.apply(null, codeUnits)\n            codeUnits.length = 0\n          }\n        }\n        return result\n      }\n      /* istanbul ignore next */\n      if (Object.defineProperty) {\n        Object.defineProperty(String, 'fromCodePoint', {\n          value: fromCodePoint,\n          configurable: true,\n          writable: true\n        })\n      } else {\n        String.fromCodePoint = fromCodePoint\n      }\n    }())\n  }\n})( false ? undefined : exports)\n\n\n//# sourceURL=webpack:///./node_modules/sax/lib/sax.js?");

/***/ }),

/***/ "./node_modules/strip-json-comments/index.js":
/*!***************************************************!*\
  !*** ./node_modules/strip-json-comments/index.js ***!
  \***************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar singleComment = 1;\nvar multiComment = 2;\n\nfunction stripWithoutWhitespace() {\n\treturn '';\n}\n\nfunction stripWithWhitespace(str, start, end) {\n\treturn str.slice(start, end).replace(/\\S/g, ' ');\n}\n\nmodule.exports = function (str, opts) {\n\topts = opts || {};\n\n\tvar currentChar;\n\tvar nextChar;\n\tvar insideString = false;\n\tvar insideComment = false;\n\tvar offset = 0;\n\tvar ret = '';\n\tvar strip = opts.whitespace === false ? stripWithoutWhitespace : stripWithWhitespace;\n\n\tfor (var i = 0; i < str.length; i++) {\n\t\tcurrentChar = str[i];\n\t\tnextChar = str[i + 1];\n\n\t\tif (!insideComment && currentChar === '\"') {\n\t\t\tvar escaped = str[i - 1] === '\\\\' && str[i - 2] !== '\\\\';\n\t\t\tif (!escaped) {\n\t\t\t\tinsideString = !insideString;\n\t\t\t}\n\t\t}\n\n\t\tif (insideString) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!insideComment && currentChar + nextChar === '//') {\n\t\t\tret += str.slice(offset, i);\n\t\t\toffset = i;\n\t\t\tinsideComment = singleComment;\n\t\t\ti++;\n\t\t} else if (insideComment === singleComment && currentChar + nextChar === '\\r\\n') {\n\t\t\ti++;\n\t\t\tinsideComment = false;\n\t\t\tret += strip(str, offset, i);\n\t\t\toffset = i;\n\t\t\tcontinue;\n\t\t} else if (insideComment === singleComment && currentChar === '\\n') {\n\t\t\tinsideComment = false;\n\t\t\tret += strip(str, offset, i);\n\t\t\toffset = i;\n\t\t} else if (!insideComment && currentChar + nextChar === '/*') {\n\t\t\tret += str.slice(offset, i);\n\t\t\toffset = i;\n\t\t\tinsideComment = multiComment;\n\t\t\ti++;\n\t\t\tcontinue;\n\t\t} else if (insideComment === multiComment && currentChar + nextChar === '*/') {\n\t\t\ti++;\n\t\t\tinsideComment = false;\n\t\t\tret += strip(str, offset, i + 1);\n\t\t\toffset = i + 1;\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\treturn ret + (insideComment ? strip(str.substr(offset)) : str.substr(offset));\n};\n\n\n//# sourceURL=webpack:///./node_modules/strip-json-comments/index.js?");

/***/ }),

/***/ "./node_modules/tar/index.js":
/*!***********************************!*\
  !*** ./node_modules/tar/index.js ***!
  \***********************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\n// high-level commands\nexports.c = exports.create = __webpack_require__(/*! ./lib/create.js */ \"./node_modules/tar/lib/create.js\")\nexports.r = exports.replace = __webpack_require__(/*! ./lib/replace.js */ \"./node_modules/tar/lib/replace.js\")\nexports.t = exports.list = __webpack_require__(/*! ./lib/list.js */ \"./node_modules/tar/lib/list.js\")\nexports.u = exports.update = __webpack_require__(/*! ./lib/update.js */ \"./node_modules/tar/lib/update.js\")\nexports.x = exports.extract = __webpack_require__(/*! ./lib/extract.js */ \"./node_modules/tar/lib/extract.js\")\n\n// classes\nexports.Pack = __webpack_require__(/*! ./lib/pack.js */ \"./node_modules/tar/lib/pack.js\")\nexports.Unpack = __webpack_require__(/*! ./lib/unpack.js */ \"./node_modules/tar/lib/unpack.js\")\nexports.Parse = __webpack_require__(/*! ./lib/parse.js */ \"./node_modules/tar/lib/parse.js\")\nexports.ReadEntry = __webpack_require__(/*! ./lib/read-entry.js */ \"./node_modules/tar/lib/read-entry.js\")\nexports.WriteEntry = __webpack_require__(/*! ./lib/write-entry.js */ \"./node_modules/tar/lib/write-entry.js\")\nexports.Header = __webpack_require__(/*! ./lib/header.js */ \"./node_modules/tar/lib/header.js\")\nexports.Pax = __webpack_require__(/*! ./lib/pax.js */ \"./node_modules/tar/lib/pax.js\")\nexports.types = __webpack_require__(/*! ./lib/types.js */ \"./node_modules/tar/lib/types.js\")\n\n\n//# sourceURL=webpack:///./node_modules/tar/index.js?");

/***/ }),

/***/ "./node_modules/tar/lib/buffer.js":
/*!****************************************!*\
  !*** ./node_modules/tar/lib/buffer.js ***!
  \****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\n// Buffer in node 4.x < 4.5.0 doesn't have working Buffer.from\n// or Buffer.alloc, and Buffer in node 10 deprecated the ctor.\n// .M, this is fine .\\^/M..\nlet B = Buffer\n/* istanbul ignore next */\nif (!B.alloc) {\n  B = __webpack_require__(/*! safe-buffer */ \"safe-buffer\").Buffer\n}\nmodule.exports = B\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/buffer.js?");

/***/ }),

/***/ "./node_modules/tar/lib/create.js":
/*!****************************************!*\
  !*** ./node_modules/tar/lib/create.js ***!
  \****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\n// tar -c\nconst hlo = __webpack_require__(/*! ./high-level-opt.js */ \"./node_modules/tar/lib/high-level-opt.js\")\n\nconst Pack = __webpack_require__(/*! ./pack.js */ \"./node_modules/tar/lib/pack.js\")\nconst fs = __webpack_require__(/*! fs */ \"fs\")\nconst fsm = __webpack_require__(/*! fs-minipass */ \"./node_modules/fs-minipass/index.js\")\nconst t = __webpack_require__(/*! ./list.js */ \"./node_modules/tar/lib/list.js\")\nconst path = __webpack_require__(/*! path */ \"path\")\n\nconst c = module.exports = (opt_, files, cb) => {\n  if (typeof files === 'function')\n    cb = files\n\n  if (Array.isArray(opt_))\n    files = opt_, opt_ = {}\n\n  if (!files || !Array.isArray(files) || !files.length)\n    throw new TypeError('no files or directories specified')\n\n  files = Array.from(files)\n\n  const opt = hlo(opt_)\n\n  if (opt.sync && typeof cb === 'function')\n    throw new TypeError('callback not supported for sync tar functions')\n\n  if (!opt.file && typeof cb === 'function')\n    throw new TypeError('callback only supported with file option')\n\n  return opt.file && opt.sync ? createFileSync(opt, files)\n    : opt.file ? createFile(opt, files, cb)\n    : opt.sync ? createSync(opt, files)\n    : create(opt, files)\n}\n\nconst createFileSync = (opt, files) => {\n  const p = new Pack.Sync(opt)\n  const stream = new fsm.WriteStreamSync(opt.file, {\n    mode: opt.mode || 0o666\n  })\n  p.pipe(stream)\n  addFilesSync(p, files)\n}\n\nconst createFile = (opt, files, cb) => {\n  const p = new Pack(opt)\n  const stream = new fsm.WriteStream(opt.file, {\n    mode: opt.mode || 0o666\n  })\n  p.pipe(stream)\n\n  const promise = new Promise((res, rej) => {\n    stream.on('error', rej)\n    stream.on('close', res)\n    p.on('error', rej)\n  })\n\n  addFilesAsync(p, files)\n\n  return cb ? promise.then(cb, cb) : promise\n}\n\nconst addFilesSync = (p, files) => {\n  files.forEach(file => {\n    if (file.charAt(0) === '@')\n      t({\n        file: path.resolve(p.cwd, file.substr(1)),\n        sync: true,\n        noResume: true,\n        onentry: entry => p.add(entry)\n      })\n    else\n      p.add(file)\n  })\n  p.end()\n}\n\nconst addFilesAsync = (p, files) => {\n  while (files.length) {\n    const file = files.shift()\n    if (file.charAt(0) === '@')\n      return t({\n        file: path.resolve(p.cwd, file.substr(1)),\n        noResume: true,\n        onentry: entry => p.add(entry)\n      }).then(_ => addFilesAsync(p, files))\n    else\n      p.add(file)\n  }\n  p.end()\n}\n\nconst createSync = (opt, files) => {\n  const p = new Pack.Sync(opt)\n  addFilesSync(p, files)\n  return p\n}\n\nconst create = (opt, files) => {\n  const p = new Pack(opt)\n  addFilesAsync(p, files)\n  return p\n}\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/create.js?");

/***/ }),

/***/ "./node_modules/tar/lib/extract.js":
/*!*****************************************!*\
  !*** ./node_modules/tar/lib/extract.js ***!
  \*****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\n// tar -x\nconst hlo = __webpack_require__(/*! ./high-level-opt.js */ \"./node_modules/tar/lib/high-level-opt.js\")\nconst Unpack = __webpack_require__(/*! ./unpack.js */ \"./node_modules/tar/lib/unpack.js\")\nconst fs = __webpack_require__(/*! fs */ \"fs\")\nconst fsm = __webpack_require__(/*! fs-minipass */ \"./node_modules/fs-minipass/index.js\")\nconst path = __webpack_require__(/*! path */ \"path\")\n\nconst x = module.exports = (opt_, files, cb) => {\n  if (typeof opt_ === 'function')\n    cb = opt_, files = null, opt_ = {}\n  else if (Array.isArray(opt_))\n    files = opt_, opt_ = {}\n\n  if (typeof files === 'function')\n    cb = files, files = null\n\n  if (!files)\n    files = []\n  else\n    files = Array.from(files)\n\n  const opt = hlo(opt_)\n\n  if (opt.sync && typeof cb === 'function')\n    throw new TypeError('callback not supported for sync tar functions')\n\n  if (!opt.file && typeof cb === 'function')\n    throw new TypeError('callback only supported with file option')\n\n  if (files.length)\n    filesFilter(opt, files)\n\n  return opt.file && opt.sync ? extractFileSync(opt)\n    : opt.file ? extractFile(opt, cb)\n    : opt.sync ? extractSync(opt)\n    : extract(opt)\n}\n\n// construct a filter that limits the file entries listed\n// include child entries if a dir is included\nconst filesFilter = (opt, files) => {\n  const map = new Map(files.map(f => [f.replace(/\\/+$/, ''), true]))\n  const filter = opt.filter\n\n  const mapHas = (file, r) => {\n    const root = r || path.parse(file).root || '.'\n    const ret = file === root ? false\n      : map.has(file) ? map.get(file)\n      : mapHas(path.dirname(file), root)\n\n    map.set(file, ret)\n    return ret\n  }\n\n  opt.filter = filter\n    ? (file, entry) => filter(file, entry) && mapHas(file.replace(/\\/+$/, ''))\n    : file => mapHas(file.replace(/\\/+$/, ''))\n}\n\nconst extractFileSync = opt => {\n  const u = new Unpack.Sync(opt)\n\n  const file = opt.file\n  let threw = true\n  let fd\n  const stat = fs.statSync(file)\n  // This trades a zero-byte read() syscall for a stat\n  // However, it will usually result in less memory allocation\n  const readSize = opt.maxReadSize || 16*1024*1024\n  const stream = new fsm.ReadStreamSync(file, {\n    readSize: readSize,\n    size: stat.size\n  })\n  stream.pipe(u)\n}\n\nconst extractFile = (opt, cb) => {\n  const u = new Unpack(opt)\n  const readSize = opt.maxReadSize || 16*1024*1024\n\n  const file = opt.file\n  const p = new Promise((resolve, reject) => {\n    u.on('error', reject)\n    u.on('close', resolve)\n\n    // This trades a zero-byte read() syscall for a stat\n    // However, it will usually result in less memory allocation\n    fs.stat(file, (er, stat) => {\n      if (er)\n        reject(er)\n      else {\n        const stream = new fsm.ReadStream(file, {\n          readSize: readSize,\n          size: stat.size\n        })\n        stream.on('error', reject)\n        stream.pipe(u)\n      }\n    })\n  })\n  return cb ? p.then(cb, cb) : p\n}\n\nconst extractSync = opt => {\n  return new Unpack.Sync(opt)\n}\n\nconst extract = opt => {\n  return new Unpack(opt)\n}\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/extract.js?");

/***/ }),

/***/ "./node_modules/tar/lib/header.js":
/*!****************************************!*\
  !*** ./node_modules/tar/lib/header.js ***!
  \****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n// parse a 512-byte header block to a data object, or vice-versa\n// encode returns `true` if a pax extended header is needed, because\n// the data could not be faithfully encoded in a simple header.\n// (Also, check header.needPax to see if it needs a pax header.)\n\nconst Buffer = __webpack_require__(/*! ./buffer.js */ \"./node_modules/tar/lib/buffer.js\")\nconst types = __webpack_require__(/*! ./types.js */ \"./node_modules/tar/lib/types.js\")\nconst pathModule = __webpack_require__(/*! path */ \"path\").posix\nconst large = __webpack_require__(/*! ./large-numbers.js */ \"./node_modules/tar/lib/large-numbers.js\")\n\nconst SLURP = Symbol('slurp')\nconst TYPE = Symbol('type')\n\nclass Header {\n  constructor (data, off, ex, gex) {\n    this.cksumValid = false\n    this.needPax = false\n    this.nullBlock = false\n\n    this.block = null\n    this.path = null\n    this.mode = null\n    this.uid = null\n    this.gid = null\n    this.size = null\n    this.mtime = null\n    this.cksum = null\n    this[TYPE] = '0'\n    this.linkpath = null\n    this.uname = null\n    this.gname = null\n    this.devmaj = 0\n    this.devmin = 0\n    this.atime = null\n    this.ctime = null\n\n    if (Buffer.isBuffer(data))\n      this.decode(data, off || 0, ex, gex)\n    else if (data)\n      this.set(data)\n  }\n\n  decode (buf, off, ex, gex) {\n    if (!off)\n      off = 0\n\n    if (!buf || !(buf.length >= off + 512))\n      throw new Error('need 512 bytes for header')\n\n    this.path = decString(buf, off, 100)\n    this.mode = decNumber(buf, off + 100, 8)\n    this.uid = decNumber(buf, off + 108, 8)\n    this.gid = decNumber(buf, off + 116, 8)\n    this.size = decNumber(buf, off + 124, 12)\n    this.mtime = decDate(buf, off + 136, 12)\n    this.cksum = decNumber(buf, off + 148, 12)\n\n    // if we have extended or global extended headers, apply them now\n    // See https://github.com/npm/node-tar/pull/187\n    this[SLURP](ex)\n    this[SLURP](gex, true)\n\n    // old tar versions marked dirs as a file with a trailing /\n    this[TYPE] = decString(buf, off + 156, 1)\n    if (this[TYPE] === '')\n      this[TYPE] = '0'\n    if (this[TYPE] === '0' && this.path.substr(-1) === '/')\n      this[TYPE] = '5'\n\n    // tar implementations sometimes incorrectly put the stat(dir).size\n    // as the size in the tarball, even though Directory entries are\n    // not able to have any body at all.  In the very rare chance that\n    // it actually DOES have a body, we weren't going to do anything with\n    // it anyway, and it'll just be a warning about an invalid header.\n    if (this[TYPE] === '5')\n      this.size = 0\n\n    this.linkpath = decString(buf, off + 157, 100)\n    if (buf.slice(off + 257, off + 265).toString() === 'ustar\\u000000') {\n      this.uname = decString(buf, off + 265, 32)\n      this.gname = decString(buf, off + 297, 32)\n      this.devmaj = decNumber(buf, off + 329, 8)\n      this.devmin = decNumber(buf, off + 337, 8)\n      if (buf[off + 475] !== 0) {\n        // definitely a prefix, definitely >130 chars.\n        const prefix = decString(buf, off + 345, 155)\n        this.path = prefix + '/' + this.path\n      } else {\n        const prefix = decString(buf, off + 345, 130)\n        if (prefix)\n          this.path = prefix + '/' + this.path\n        this.atime = decDate(buf, off + 476, 12)\n        this.ctime = decDate(buf, off + 488, 12)\n      }\n    }\n\n    let sum = 8 * 0x20\n    for (let i = off; i < off + 148; i++) {\n      sum += buf[i]\n    }\n    for (let i = off + 156; i < off + 512; i++) {\n      sum += buf[i]\n    }\n    this.cksumValid = sum === this.cksum\n    if (this.cksum === null && sum === 8 * 0x20)\n      this.nullBlock = true\n  }\n\n  [SLURP] (ex, global) {\n    for (let k in ex) {\n      // we slurp in everything except for the path attribute in\n      // a global extended header, because that's weird.\n      if (ex[k] !== null && ex[k] !== undefined &&\n          !(global && k === 'path'))\n        this[k] = ex[k]\n    }\n  }\n\n  encode (buf, off) {\n    if (!buf) {\n      buf = this.block = Buffer.alloc(512)\n      off = 0\n    }\n\n    if (!off)\n      off = 0\n\n    if (!(buf.length >= off + 512))\n      throw new Error('need 512 bytes for header')\n\n    const prefixSize = this.ctime || this.atime ? 130 : 155\n    const split = splitPrefix(this.path || '', prefixSize)\n    const path = split[0]\n    const prefix = split[1]\n    this.needPax = split[2]\n\n    this.needPax = encString(buf, off, 100, path) || this.needPax\n    this.needPax = encNumber(buf, off + 100, 8, this.mode) || this.needPax\n    this.needPax = encNumber(buf, off + 108, 8, this.uid) || this.needPax\n    this.needPax = encNumber(buf, off + 116, 8, this.gid) || this.needPax\n    this.needPax = encNumber(buf, off + 124, 12, this.size) || this.needPax\n    this.needPax = encDate(buf, off + 136, 12, this.mtime) || this.needPax\n    buf[off + 156] = this[TYPE].charCodeAt(0)\n    this.needPax = encString(buf, off + 157, 100, this.linkpath) || this.needPax\n    buf.write('ustar\\u000000', off + 257, 8)\n    this.needPax = encString(buf, off + 265, 32, this.uname) || this.needPax\n    this.needPax = encString(buf, off + 297, 32, this.gname) || this.needPax\n    this.needPax = encNumber(buf, off + 329, 8, this.devmaj) || this.needPax\n    this.needPax = encNumber(buf, off + 337, 8, this.devmin) || this.needPax\n    this.needPax = encString(buf, off + 345, prefixSize, prefix) || this.needPax\n    if (buf[off + 475] !== 0)\n      this.needPax = encString(buf, off + 345, 155, prefix) || this.needPax\n    else {\n      this.needPax = encString(buf, off + 345, 130, prefix) || this.needPax\n      this.needPax = encDate(buf, off + 476, 12, this.atime) || this.needPax\n      this.needPax = encDate(buf, off + 488, 12, this.ctime) || this.needPax\n    }\n\n    let sum = 8 * 0x20\n    for (let i = off; i < off + 148; i++) {\n      sum += buf[i]\n    }\n    for (let i = off + 156; i < off + 512; i++) {\n      sum += buf[i]\n    }\n    this.cksum = sum\n    encNumber(buf, off + 148, 8, this.cksum)\n    this.cksumValid = true\n\n    return this.needPax\n  }\n\n  set (data) {\n    for (let i in data) {\n      if (data[i] !== null && data[i] !== undefined)\n        this[i] = data[i]\n    }\n  }\n\n  get type () {\n    return types.name.get(this[TYPE]) || this[TYPE]\n  }\n\n  get typeKey () {\n    return this[TYPE]\n  }\n\n  set type (type) {\n    if (types.code.has(type))\n      this[TYPE] = types.code.get(type)\n    else\n      this[TYPE] = type\n  }\n}\n\nconst splitPrefix = (p, prefixSize) => {\n  const pathSize = 100\n  let pp = p\n  let prefix = ''\n  let ret\n  const root = pathModule.parse(p).root || '.'\n\n  if (Buffer.byteLength(pp) < pathSize)\n    ret = [pp, prefix, false]\n  else {\n    // first set prefix to the dir, and path to the base\n    prefix = pathModule.dirname(pp)\n    pp = pathModule.basename(pp)\n\n    do {\n      // both fit!\n      if (Buffer.byteLength(pp) <= pathSize &&\n          Buffer.byteLength(prefix) <= prefixSize)\n        ret = [pp, prefix, false]\n\n      // prefix fits in prefix, but path doesn't fit in path\n      else if (Buffer.byteLength(pp) > pathSize &&\n          Buffer.byteLength(prefix) <= prefixSize)\n        ret = [pp.substr(0, pathSize - 1), prefix, true]\n\n      else {\n        // make path take a bit from prefix\n        pp = pathModule.join(pathModule.basename(prefix), pp)\n        prefix = pathModule.dirname(prefix)\n      }\n    } while (prefix !== root && !ret)\n\n    // at this point, found no resolution, just truncate\n    if (!ret)\n      ret = [p.substr(0, pathSize - 1), '', true]\n  }\n  return ret\n}\n\nconst decString = (buf, off, size) =>\n  buf.slice(off, off + size).toString('utf8').replace(/\\0.*/, '')\n\nconst decDate = (buf, off, size) =>\n  numToDate(decNumber(buf, off, size))\n\nconst numToDate = num => num === null ? null : new Date(num * 1000)\n\nconst decNumber = (buf, off, size) =>\n  buf[off] & 0x80 ? large.parse(buf.slice(off, off + size))\n    : decSmallNumber(buf, off, size)\n\nconst nanNull = value => isNaN(value) ? null : value\n\nconst decSmallNumber = (buf, off, size) =>\n  nanNull(parseInt(\n    buf.slice(off, off + size)\n      .toString('utf8').replace(/\\0.*$/, '').trim(), 8))\n\n// the maximum encodable as a null-terminated octal, by field size\nconst MAXNUM = {\n  12: 0o77777777777,\n  8 : 0o7777777\n}\n\nconst encNumber = (buf, off, size, number) =>\n  number === null ? false :\n  number > MAXNUM[size] || number < 0\n    ? (large.encode(number, buf.slice(off, off + size)), true)\n    : (encSmallNumber(buf, off, size, number), false)\n\nconst encSmallNumber = (buf, off, size, number) =>\n  buf.write(octalString(number, size), off, size, 'ascii')\n\nconst octalString = (number, size) =>\n  padOctal(Math.floor(number).toString(8), size)\n\nconst padOctal = (string, size) =>\n  (string.length === size - 1 ? string\n  : new Array(size - string.length - 1).join('0') + string + ' ') + '\\0'\n\nconst encDate = (buf, off, size, date) =>\n  date === null ? false :\n  encNumber(buf, off, size, date.getTime() / 1000)\n\n// enough to fill the longest string we've got\nconst NULLS = new Array(156).join('\\0')\n// pad with nulls, return true if it's longer or non-ascii\nconst encString = (buf, off, size, string) =>\n  string === null ? false :\n  (buf.write(string + NULLS, off, size, 'utf8'),\n   string.length !== Buffer.byteLength(string) || string.length > size)\n\nmodule.exports = Header\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/header.js?");

/***/ }),

/***/ "./node_modules/tar/lib/high-level-opt.js":
/*!************************************************!*\
  !*** ./node_modules/tar/lib/high-level-opt.js ***!
  \************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\n// turn tar(1) style args like `C` into the more verbose things like `cwd`\n\nconst argmap = new Map([\n  ['C', 'cwd'],\n  ['f', 'file'],\n  ['z', 'gzip'],\n  ['P', 'preservePaths'],\n  ['U', 'unlink'],\n  ['strip-components', 'strip'],\n  ['stripComponents', 'strip'],\n  ['keep-newer', 'newer'],\n  ['keepNewer', 'newer'],\n  ['keep-newer-files', 'newer'],\n  ['keepNewerFiles', 'newer'],\n  ['k', 'keep'],\n  ['keep-existing', 'keep'],\n  ['keepExisting', 'keep'],\n  ['m', 'noMtime'],\n  ['no-mtime', 'noMtime'],\n  ['p', 'preserveOwner'],\n  ['L', 'follow'],\n  ['h', 'follow']\n])\n\nconst parse = module.exports = opt => opt ? Object.keys(opt).map(k => [\n  argmap.has(k) ? argmap.get(k) : k, opt[k]\n]).reduce((set, kv) => (set[kv[0]] = kv[1], set), Object.create(null)) : {}\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/high-level-opt.js?");

/***/ }),

/***/ "./node_modules/tar/lib/large-numbers.js":
/*!***********************************************!*\
  !*** ./node_modules/tar/lib/large-numbers.js ***!
  \***********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n// Tar can encode large and negative numbers using a leading byte of\n// 0xff for negative, and 0x80 for positive.\n\nconst encode = exports.encode = (num, buf) => {\n  if (!Number.isSafeInteger(num))\n    // The number is so large that javascript cannot represent it with integer\n    // precision.\n    throw TypeError('cannot encode number outside of javascript safe integer range')\n  else if (num < 0)\n    encodeNegative(num, buf)\n  else\n    encodePositive(num, buf)\n  return buf\n}\n\nconst encodePositive = (num, buf) => {\n  buf[0] = 0x80\n\n  for (var i = buf.length; i > 1; i--) {\n    buf[i-1] = num & 0xff\n    num = Math.floor(num / 0x100)\n  }\n}\n\nconst encodeNegative = (num, buf) => {\n  buf[0] = 0xff\n  var flipped = false\n  num = num * -1\n  for (var i = buf.length; i > 1; i--) {\n    var byte = num & 0xff\n    num = Math.floor(num / 0x100)\n    if (flipped)\n      buf[i-1] = onesComp(byte)\n    else if (byte === 0)\n      buf[i-1] = 0\n    else {\n      flipped = true\n      buf[i-1] = twosComp(byte)\n    }\n  }\n}\n\nconst parse = exports.parse = (buf) => {\n  var post = buf[buf.length - 1]\n  var pre = buf[0]\n  var value;\n  if (pre === 0x80)\n    value = pos(buf.slice(1, buf.length))\n  else if (pre === 0xff)\n    value = twos(buf)\n  else\n    throw TypeError('invalid base256 encoding')\n\n  if (!Number.isSafeInteger(value))\n    // The number is so large that javascript cannot represent it with integer\n    // precision.\n    throw TypeError('parsed number outside of javascript safe integer range')\n\n  return value\n}\n\nconst twos = (buf) => {\n  var len = buf.length\n  var sum = 0\n  var flipped = false\n  for (var i = len - 1; i > -1; i--) {\n    var byte = buf[i]\n    var f\n    if (flipped)\n      f = onesComp(byte)\n    else if (byte === 0)\n      f = byte\n    else {\n      flipped = true\n      f = twosComp(byte)\n    }\n    if (f !== 0)\n      sum -= f * Math.pow(256, len - i - 1)\n  }\n  return sum\n}\n\nconst pos = (buf) => {\n  var len = buf.length\n  var sum = 0\n  for (var i = len - 1; i > -1; i--) {\n    var byte = buf[i]\n    if (byte !== 0)\n      sum += byte * Math.pow(256, len - i - 1)\n  }\n  return sum\n}\n\nconst onesComp = byte => (0xff ^ byte) & 0xff\n\nconst twosComp = byte => ((0xff ^ byte) + 1) & 0xff\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/large-numbers.js?");

/***/ }),

/***/ "./node_modules/tar/lib/list.js":
/*!**************************************!*\
  !*** ./node_modules/tar/lib/list.js ***!
  \**************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nconst Buffer = __webpack_require__(/*! ./buffer.js */ \"./node_modules/tar/lib/buffer.js\")\n\n// XXX: This shares a lot in common with extract.js\n// maybe some DRY opportunity here?\n\n// tar -t\nconst hlo = __webpack_require__(/*! ./high-level-opt.js */ \"./node_modules/tar/lib/high-level-opt.js\")\nconst Parser = __webpack_require__(/*! ./parse.js */ \"./node_modules/tar/lib/parse.js\")\nconst fs = __webpack_require__(/*! fs */ \"fs\")\nconst fsm = __webpack_require__(/*! fs-minipass */ \"./node_modules/fs-minipass/index.js\")\nconst path = __webpack_require__(/*! path */ \"path\")\n\nconst t = module.exports = (opt_, files, cb) => {\n  if (typeof opt_ === 'function')\n    cb = opt_, files = null, opt_ = {}\n  else if (Array.isArray(opt_))\n    files = opt_, opt_ = {}\n\n  if (typeof files === 'function')\n    cb = files, files = null\n\n  if (!files)\n    files = []\n  else\n    files = Array.from(files)\n\n  const opt = hlo(opt_)\n\n  if (opt.sync && typeof cb === 'function')\n    throw new TypeError('callback not supported for sync tar functions')\n\n  if (!opt.file && typeof cb === 'function')\n    throw new TypeError('callback only supported with file option')\n\n  if (files.length)\n    filesFilter(opt, files)\n\n  if (!opt.noResume)\n    onentryFunction(opt)\n\n  return opt.file && opt.sync ? listFileSync(opt)\n    : opt.file ? listFile(opt, cb)\n    : list(opt)\n}\n\nconst onentryFunction = opt => {\n  const onentry = opt.onentry\n  opt.onentry = onentry ? e => {\n    onentry(e)\n    e.resume()\n  } : e => e.resume()\n}\n\n// construct a filter that limits the file entries listed\n// include child entries if a dir is included\nconst filesFilter = (opt, files) => {\n  const map = new Map(files.map(f => [f.replace(/\\/+$/, ''), true]))\n  const filter = opt.filter\n\n  const mapHas = (file, r) => {\n    const root = r || path.parse(file).root || '.'\n    const ret = file === root ? false\n      : map.has(file) ? map.get(file)\n      : mapHas(path.dirname(file), root)\n\n    map.set(file, ret)\n    return ret\n  }\n\n  opt.filter = filter\n    ? (file, entry) => filter(file, entry) && mapHas(file.replace(/\\/+$/, ''))\n    : file => mapHas(file.replace(/\\/+$/, ''))\n}\n\nconst listFileSync = opt => {\n  const p = list(opt)\n  const file = opt.file\n  let threw = true\n  let fd\n  try {\n    const stat = fs.statSync(file)\n    const readSize = opt.maxReadSize || 16*1024*1024\n    if (stat.size < readSize) {\n      p.end(fs.readFileSync(file))\n    } else {\n      let pos = 0\n      const buf = Buffer.allocUnsafe(readSize)\n      fd = fs.openSync(file, 'r')\n      while (pos < stat.size) {\n        let bytesRead = fs.readSync(fd, buf, 0, readSize, pos)\n        pos += bytesRead\n        p.write(buf.slice(0, bytesRead))\n      }\n      p.end()\n    }\n    threw = false\n  } finally {\n    if (threw && fd)\n      try { fs.closeSync(fd) } catch (er) {}\n  }\n}\n\nconst listFile = (opt, cb) => {\n  const parse = new Parser(opt)\n  const readSize = opt.maxReadSize || 16*1024*1024\n\n  const file = opt.file\n  const p = new Promise((resolve, reject) => {\n    parse.on('error', reject)\n    parse.on('end', resolve)\n\n    fs.stat(file, (er, stat) => {\n      if (er)\n        reject(er)\n      else {\n        const stream = new fsm.ReadStream(file, {\n          readSize: readSize,\n          size: stat.size\n        })\n        stream.on('error', reject)\n        stream.pipe(parse)\n      }\n    })\n  })\n  return cb ? p.then(cb, cb) : p\n}\n\nconst list = opt => new Parser(opt)\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/list.js?");

/***/ }),

/***/ "./node_modules/tar/lib/mkdir.js":
/*!***************************************!*\
  !*** ./node_modules/tar/lib/mkdir.js ***!
  \***************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n// wrapper around mkdirp for tar's needs.\n\n// TODO: This should probably be a class, not functionally\n// passing around state in a gazillion args.\n\nconst mkdirp = __webpack_require__(/*! mkdirp */ \"mkdirp\")\nconst fs = __webpack_require__(/*! fs */ \"fs\")\nconst path = __webpack_require__(/*! path */ \"path\")\nconst chownr = __webpack_require__(/*! chownr */ \"chownr\")\n\nclass SymlinkError extends Error {\n  constructor (symlink, path) {\n    super('Cannot extract through symbolic link')\n    this.path = path\n    this.symlink = symlink\n  }\n\n  get name () {\n    return 'SylinkError'\n  }\n}\n\nclass CwdError extends Error {\n  constructor (path, code) {\n    super(code + ': Cannot cd into \\'' + path + '\\'')\n    this.path = path\n    this.code = code\n  }\n\n  get name () {\n    return 'CwdError'\n  }\n}\n\nconst mkdir = module.exports = (dir, opt, cb) => {\n  // if there's any overlap between mask and mode,\n  // then we'll need an explicit chmod\n  const umask = opt.umask\n  const mode = opt.mode | 0o0700\n  const needChmod = (mode & umask) !== 0\n\n  const uid = opt.uid\n  const gid = opt.gid\n  const doChown = typeof uid === 'number' &&\n    typeof gid === 'number' &&\n    ( uid !== opt.processUid || gid !== opt.processGid )\n\n  const preserve = opt.preserve\n  const unlink = opt.unlink\n  const cache = opt.cache\n  const cwd = opt.cwd\n\n  const done = (er, created) => {\n    if (er)\n      cb(er)\n    else {\n      cache.set(dir, true)\n      if (created && doChown)\n        chownr(created, uid, gid, er => done(er))\n      else if (needChmod)\n        fs.chmod(dir, mode, cb)\n      else\n        cb()\n    }\n  }\n\n  if (cache && cache.get(dir) === true)\n    return done()\n\n  if (dir === cwd)\n    return fs.stat(dir, (er, st) => {\n      if (er || !st.isDirectory())\n        er = new CwdError(dir, er && er.code || 'ENOTDIR')\n      done(er)\n    })\n\n  if (preserve)\n    return mkdirp(dir, mode, done)\n\n  const sub = path.relative(cwd, dir)\n  const parts = sub.split(/\\/|\\\\/)\n  mkdir_(cwd, parts, mode, cache, unlink, cwd, null, done)\n}\n\nconst mkdir_ = (base, parts, mode, cache, unlink, cwd, created, cb) => {\n  if (!parts.length)\n    return cb(null, created)\n  const p = parts.shift()\n  const part = base + '/' + p\n  if (cache.get(part))\n    return mkdir_(part, parts, mode, cache, unlink, cwd, created, cb)\n  fs.mkdir(part, mode, onmkdir(part, parts, mode, cache, unlink, cwd, created, cb))\n}\n\nconst onmkdir = (part, parts, mode, cache, unlink, cwd, created, cb) => er => {\n  if (er) {\n    if (er.path && path.dirname(er.path) === cwd &&\n        (er.code === 'ENOTDIR' || er.code === 'ENOENT'))\n      return cb(new CwdError(cwd, er.code))\n\n    fs.lstat(part, (statEr, st) => {\n      if (statEr)\n        cb(statEr)\n      else if (st.isDirectory())\n        mkdir_(part, parts, mode, cache, unlink, cwd, created, cb)\n      else if (unlink)\n        fs.unlink(part, er => {\n          if (er)\n            return cb(er)\n          fs.mkdir(part, mode, onmkdir(part, parts, mode, cache, unlink, cwd, created, cb))\n        })\n      else if (st.isSymbolicLink())\n        return cb(new SymlinkError(part, part + '/' + parts.join('/')))\n      else\n        cb(er)\n    })\n  } else {\n    created = created || part\n    mkdir_(part, parts, mode, cache, unlink, cwd, created, cb)\n  }\n}\n\nconst mkdirSync = module.exports.sync = (dir, opt) => {\n  // if there's any overlap between mask and mode,\n  // then we'll need an explicit chmod\n  const umask = opt.umask\n  const mode = opt.mode | 0o0700\n  const needChmod = (mode & umask) !== 0\n\n  const uid = opt.uid\n  const gid = opt.gid\n  const doChown = typeof uid === 'number' &&\n    typeof gid === 'number' &&\n    ( uid !== opt.processUid || gid !== opt.processGid )\n\n  const preserve = opt.preserve\n  const unlink = opt.unlink\n  const cache = opt.cache\n  const cwd = opt.cwd\n\n  const done = (created) => {\n    cache.set(dir, true)\n    if (created && doChown)\n      chownr.sync(created, uid, gid)\n    if (needChmod)\n      fs.chmodSync(dir, mode)\n  }\n\n  if (cache && cache.get(dir) === true)\n    return done()\n\n  if (dir === cwd) {\n    let ok = false\n    let code = 'ENOTDIR'\n    try {\n      ok = fs.statSync(dir).isDirectory()\n    } catch (er) {\n      code = er.code\n    } finally {\n      if (!ok)\n        throw new CwdError(dir, code)\n    }\n    done()\n    return\n  }\n\n  if (preserve)\n    return done(mkdirp.sync(dir, mode))\n\n  const sub = path.relative(cwd, dir)\n  const parts = sub.split(/\\/|\\\\/)\n  let created = null\n  for (let p = parts.shift(), part = cwd;\n       p && (part += '/' + p);\n       p = parts.shift()) {\n\n    if (cache.get(part))\n      continue\n\n    try {\n      fs.mkdirSync(part, mode)\n      created = created || part\n      cache.set(part, true)\n    } catch (er) {\n      if (er.path && path.dirname(er.path) === cwd &&\n          (er.code === 'ENOTDIR' || er.code === 'ENOENT'))\n        return new CwdError(cwd, er.code)\n\n      const st = fs.lstatSync(part)\n      if (st.isDirectory()) {\n        cache.set(part, true)\n        continue\n      } else if (unlink) {\n        fs.unlinkSync(part)\n        fs.mkdirSync(part, mode)\n        created = created || part\n        cache.set(part, true)\n        continue\n      } else if (st.isSymbolicLink())\n        return new SymlinkError(part, part + '/' + parts.join('/'))\n    }\n  }\n\n  return done(created)\n}\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/mkdir.js?");

/***/ }),

/***/ "./node_modules/tar/lib/mode-fix.js":
/*!******************************************!*\
  !*** ./node_modules/tar/lib/mode-fix.js ***!
  \******************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nmodule.exports = (mode, isDir) => {\n  mode &= 0o7777\n  // if dirs are readable, then they should be listable\n  if (isDir) {\n    if (mode & 0o400)\n      mode |= 0o100\n    if (mode & 0o40)\n      mode |= 0o10\n    if (mode & 0o4)\n      mode |= 0o1\n  }\n  return mode\n}\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/mode-fix.js?");

/***/ }),

/***/ "./node_modules/tar/lib/pack.js":
/*!**************************************!*\
  !*** ./node_modules/tar/lib/pack.js ***!
  \**************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nconst Buffer = __webpack_require__(/*! ./buffer.js */ \"./node_modules/tar/lib/buffer.js\")\n\n// A readable tar stream creator\n// Technically, this is a transform stream that you write paths into,\n// and tar format comes out of.\n// The `add()` method is like `write()` but returns this,\n// and end() return `this` as well, so you can\n// do `new Pack(opt).add('files').add('dir').end().pipe(output)\n// You could also do something like:\n// streamOfPaths().pipe(new Pack()).pipe(new fs.WriteStream('out.tar'))\n\nclass PackJob {\n  constructor (path, absolute) {\n    this.path = path || './'\n    this.absolute = absolute\n    this.entry = null\n    this.stat = null\n    this.readdir = null\n    this.pending = false\n    this.ignore = false\n    this.piped = false\n  }\n}\n\nconst MiniPass = __webpack_require__(/*! minipass */ \"./node_modules/minipass/index.js\")\nconst zlib = __webpack_require__(/*! minizlib */ \"./node_modules/minizlib/index.js\")\nconst ReadEntry = __webpack_require__(/*! ./read-entry.js */ \"./node_modules/tar/lib/read-entry.js\")\nconst WriteEntry = __webpack_require__(/*! ./write-entry.js */ \"./node_modules/tar/lib/write-entry.js\")\nconst WriteEntrySync = WriteEntry.Sync\nconst WriteEntryTar = WriteEntry.Tar\nconst Yallist = __webpack_require__(/*! yallist */ \"yallist\")\nconst EOF = Buffer.alloc(1024)\nconst ONSTAT = Symbol('onStat')\nconst ENDED = Symbol('ended')\nconst QUEUE = Symbol('queue')\nconst CURRENT = Symbol('current')\nconst PROCESS = Symbol('process')\nconst PROCESSING = Symbol('processing')\nconst PROCESSJOB = Symbol('processJob')\nconst JOBS = Symbol('jobs')\nconst JOBDONE = Symbol('jobDone')\nconst ADDFSENTRY = Symbol('addFSEntry')\nconst ADDTARENTRY = Symbol('addTarEntry')\nconst STAT = Symbol('stat')\nconst READDIR = Symbol('readdir')\nconst ONREADDIR = Symbol('onreaddir')\nconst PIPE = Symbol('pipe')\nconst ENTRY = Symbol('entry')\nconst ENTRYOPT = Symbol('entryOpt')\nconst WRITEENTRYCLASS = Symbol('writeEntryClass')\nconst WRITE = Symbol('write')\nconst ONDRAIN = Symbol('ondrain')\n\nconst fs = __webpack_require__(/*! fs */ \"fs\")\nconst path = __webpack_require__(/*! path */ \"path\")\nconst warner = __webpack_require__(/*! ./warn-mixin.js */ \"./node_modules/tar/lib/warn-mixin.js\")\n\nconst Pack = warner(class Pack extends MiniPass {\n  constructor (opt) {\n    super(opt)\n    opt = opt || Object.create(null)\n    this.opt = opt\n    this.cwd = opt.cwd || process.cwd()\n    this.maxReadSize = opt.maxReadSize\n    this.preservePaths = !!opt.preservePaths\n    this.strict = !!opt.strict\n    this.noPax = !!opt.noPax\n    this.prefix = (opt.prefix || '').replace(/(\\\\|\\/)+$/, '')\n    this.linkCache = opt.linkCache || new Map()\n    this.statCache = opt.statCache || new Map()\n    this.readdirCache = opt.readdirCache || new Map()\n\n    this[WRITEENTRYCLASS] = WriteEntry\n    if (typeof opt.onwarn === 'function')\n      this.on('warn', opt.onwarn)\n\n    this.zip = null\n    if (opt.gzip) {\n      if (typeof opt.gzip !== 'object')\n        opt.gzip = {}\n      this.zip = new zlib.Gzip(opt.gzip)\n      this.zip.on('data', chunk => super.write(chunk))\n      this.zip.on('end', _ => super.end())\n      this.zip.on('drain', _ => this[ONDRAIN]())\n      this.on('resume', _ => this.zip.resume())\n    } else\n      this.on('drain', this[ONDRAIN])\n\n    this.portable = !!opt.portable\n    this.noDirRecurse = !!opt.noDirRecurse\n    this.follow = !!opt.follow\n    this.noMtime = !!opt.noMtime\n    this.mtime = opt.mtime || null\n\n    this.filter = typeof opt.filter === 'function' ? opt.filter : _ => true\n\n    this[QUEUE] = new Yallist\n    this[JOBS] = 0\n    this.jobs = +opt.jobs || 4\n    this[PROCESSING] = false\n    this[ENDED] = false\n  }\n\n  [WRITE] (chunk) {\n    return super.write(chunk)\n  }\n\n  add (path) {\n    this.write(path)\n    return this\n  }\n\n  end (path) {\n    if (path)\n      this.write(path)\n    this[ENDED] = true\n    this[PROCESS]()\n    return this\n  }\n\n  write (path) {\n    if (this[ENDED])\n      throw new Error('write after end')\n\n    if (path instanceof ReadEntry)\n      this[ADDTARENTRY](path)\n    else\n      this[ADDFSENTRY](path)\n    return this.flowing\n  }\n\n  [ADDTARENTRY] (p) {\n    const absolute = path.resolve(this.cwd, p.path)\n    if (this.prefix)\n      p.path = this.prefix + '/' + p.path.replace(/^\\.(\\/+|$)/, '')\n\n    // in this case, we don't have to wait for the stat\n    if (!this.filter(p.path, p))\n      p.resume()\n    else {\n      const job = new PackJob(p.path, absolute, false)\n      job.entry = new WriteEntryTar(p, this[ENTRYOPT](job))\n      job.entry.on('end', _ => this[JOBDONE](job))\n      this[JOBS] += 1\n      this[QUEUE].push(job)\n    }\n\n    this[PROCESS]()\n  }\n\n  [ADDFSENTRY] (p) {\n    const absolute = path.resolve(this.cwd, p)\n    if (this.prefix)\n      p = this.prefix + '/' + p.replace(/^\\.(\\/+|$)/, '')\n\n    this[QUEUE].push(new PackJob(p, absolute))\n    this[PROCESS]()\n  }\n\n  [STAT] (job) {\n    job.pending = true\n    this[JOBS] += 1\n    const stat = this.follow ? 'stat' : 'lstat'\n    fs[stat](job.absolute, (er, stat) => {\n      job.pending = false\n      this[JOBS] -= 1\n      if (er)\n        this.emit('error', er)\n      else\n        this[ONSTAT](job, stat)\n    })\n  }\n\n  [ONSTAT] (job, stat) {\n    this.statCache.set(job.absolute, stat)\n    job.stat = stat\n\n    // now we have the stat, we can filter it.\n    if (!this.filter(job.path, stat))\n      job.ignore = true\n\n    this[PROCESS]()\n  }\n\n  [READDIR] (job) {\n    job.pending = true\n    this[JOBS] += 1\n    fs.readdir(job.absolute, (er, entries) => {\n      job.pending = false\n      this[JOBS] -= 1\n      if (er)\n        return this.emit('error', er)\n      this[ONREADDIR](job, entries)\n    })\n  }\n\n  [ONREADDIR] (job, entries) {\n    this.readdirCache.set(job.absolute, entries)\n    job.readdir = entries\n    this[PROCESS]()\n  }\n\n  [PROCESS] () {\n    if (this[PROCESSING])\n      return\n\n    this[PROCESSING] = true\n    for (let w = this[QUEUE].head;\n         w !== null && this[JOBS] < this.jobs;\n         w = w.next) {\n      this[PROCESSJOB](w.value)\n      if (w.value.ignore) {\n        const p = w.next\n        this[QUEUE].removeNode(w)\n        w.next = p\n      }\n    }\n\n    this[PROCESSING] = false\n\n    if (this[ENDED] && !this[QUEUE].length && this[JOBS] === 0) {\n      if (this.zip)\n        this.zip.end(EOF)\n      else {\n        super.write(EOF)\n        super.end()\n      }\n    }\n  }\n\n  get [CURRENT] () {\n    return this[QUEUE] && this[QUEUE].head && this[QUEUE].head.value\n  }\n\n  [JOBDONE] (job) {\n    this[QUEUE].shift()\n    this[JOBS] -= 1\n    this[PROCESS]()\n  }\n\n  [PROCESSJOB] (job) {\n    if (job.pending)\n      return\n\n    if (job.entry) {\n      if (job === this[CURRENT] && !job.piped)\n        this[PIPE](job)\n      return\n    }\n\n    if (!job.stat) {\n      if (this.statCache.has(job.absolute))\n        this[ONSTAT](job, this.statCache.get(job.absolute))\n      else\n        this[STAT](job)\n    }\n    if (!job.stat)\n      return\n\n    // filtered out!\n    if (job.ignore)\n      return\n\n    if (!this.noDirRecurse && job.stat.isDirectory() && !job.readdir) {\n      if (this.readdirCache.has(job.absolute))\n        this[ONREADDIR](job, this.readdirCache.get(job.absolute))\n      else\n        this[READDIR](job)\n      if (!job.readdir)\n        return\n    }\n\n    // we know it doesn't have an entry, because that got checked above\n    job.entry = this[ENTRY](job)\n    if (!job.entry) {\n      job.ignore = true\n      return\n    }\n\n    if (job === this[CURRENT] && !job.piped)\n      this[PIPE](job)\n  }\n\n  [ENTRYOPT] (job) {\n    return {\n      onwarn: (msg, data) => {\n        this.warn(msg, data)\n      },\n      noPax: this.noPax,\n      cwd: this.cwd,\n      absolute: job.absolute,\n      preservePaths: this.preservePaths,\n      maxReadSize: this.maxReadSize,\n      strict: this.strict,\n      portable: this.portable,\n      linkCache: this.linkCache,\n      statCache: this.statCache,\n      noMtime: this.noMtime,\n      mtime: this.mtime\n    }\n  }\n\n  [ENTRY] (job) {\n    this[JOBS] += 1\n    try {\n      return new this[WRITEENTRYCLASS](job.path, this[ENTRYOPT](job))\n        .on('end', () => this[JOBDONE](job))\n        .on('error', er => this.emit('error', er))\n    } catch (er) {\n      this.emit('error', er)\n    }\n  }\n\n  [ONDRAIN] () {\n    if (this[CURRENT] && this[CURRENT].entry)\n      this[CURRENT].entry.resume()\n  }\n\n  // like .pipe() but using super, because our write() is special\n  [PIPE] (job) {\n    job.piped = true\n\n    if (job.readdir)\n      job.readdir.forEach(entry => {\n        const p = this.prefix ?\n          job.path.slice(this.prefix.length + 1) || './'\n          : job.path\n\n        const base = p === './' ? '' : p.replace(/\\/*$/, '/')\n        this[ADDFSENTRY](base + entry)\n      })\n\n    const source = job.entry\n    const zip = this.zip\n\n    if (zip)\n      source.on('data', chunk => {\n        if (!zip.write(chunk))\n          source.pause()\n      })\n    else\n      source.on('data', chunk => {\n        if (!super.write(chunk))\n          source.pause()\n      })\n  }\n\n  pause () {\n    if (this.zip)\n      this.zip.pause()\n    return super.pause()\n  }\n})\n\nclass PackSync extends Pack {\n  constructor (opt) {\n    super(opt)\n    this[WRITEENTRYCLASS] = WriteEntrySync\n  }\n\n  // pause/resume are no-ops in sync streams.\n  pause () {}\n  resume () {}\n\n  [STAT] (job) {\n    const stat = this.follow ? 'statSync' : 'lstatSync'\n    this[ONSTAT](job, fs[stat](job.absolute))\n  }\n\n  [READDIR] (job, stat) {\n    this[ONREADDIR](job, fs.readdirSync(job.absolute))\n  }\n\n  // gotta get it all in this tick\n  [PIPE] (job) {\n    const source = job.entry\n    const zip = this.zip\n\n    if (job.readdir)\n      job.readdir.forEach(entry => {\n        const p = this.prefix ?\n          job.path.slice(this.prefix.length + 1) || './'\n          : job.path\n\n        const base = p === './' ? '' : p.replace(/\\/*$/, '/')\n        this[ADDFSENTRY](base + entry)\n      })\n\n    if (zip)\n      source.on('data', chunk => {\n        zip.write(chunk)\n      })\n    else\n      source.on('data', chunk => {\n        super[WRITE](chunk)\n      })\n  }\n}\n\nPack.Sync = PackSync\n\nmodule.exports = Pack\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/pack.js?");

/***/ }),

/***/ "./node_modules/tar/lib/parse.js":
/*!***************************************!*\
  !*** ./node_modules/tar/lib/parse.js ***!
  \***************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\n// this[BUFFER] is the remainder of a chunk if we're waiting for\n// the full 512 bytes of a header to come in.  We will Buffer.concat()\n// it to the next write(), which is a mem copy, but a small one.\n//\n// this[QUEUE] is a Yallist of entries that haven't been emitted\n// yet this can only get filled up if the user keeps write()ing after\n// a write() returns false, or does a write() with more than one entry\n//\n// We don't buffer chunks, we always parse them and either create an\n// entry, or push it into the active entry.  The ReadEntry class knows\n// to throw data away if .ignore=true\n//\n// Shift entry off the buffer when it emits 'end', and emit 'entry' for\n// the next one in the list.\n//\n// At any time, we're pushing body chunks into the entry at WRITEENTRY,\n// and waiting for 'end' on the entry at READENTRY\n//\n// ignored entries get .resume() called on them straight away\n\nconst warner = __webpack_require__(/*! ./warn-mixin.js */ \"./node_modules/tar/lib/warn-mixin.js\")\nconst path = __webpack_require__(/*! path */ \"path\")\nconst Header = __webpack_require__(/*! ./header.js */ \"./node_modules/tar/lib/header.js\")\nconst EE = __webpack_require__(/*! events */ \"events\")\nconst Yallist = __webpack_require__(/*! yallist */ \"yallist\")\nconst maxMetaEntrySize = 1024 * 1024\nconst Entry = __webpack_require__(/*! ./read-entry.js */ \"./node_modules/tar/lib/read-entry.js\")\nconst Pax = __webpack_require__(/*! ./pax.js */ \"./node_modules/tar/lib/pax.js\")\nconst zlib = __webpack_require__(/*! minizlib */ \"./node_modules/minizlib/index.js\")\nconst Buffer = __webpack_require__(/*! ./buffer.js */ \"./node_modules/tar/lib/buffer.js\")\n\nconst gzipHeader = Buffer.from([0x1f, 0x8b])\nconst STATE = Symbol('state')\nconst WRITEENTRY = Symbol('writeEntry')\nconst READENTRY = Symbol('readEntry')\nconst NEXTENTRY = Symbol('nextEntry')\nconst PROCESSENTRY = Symbol('processEntry')\nconst EX = Symbol('extendedHeader')\nconst GEX = Symbol('globalExtendedHeader')\nconst META = Symbol('meta')\nconst EMITMETA = Symbol('emitMeta')\nconst BUFFER = Symbol('buffer')\nconst QUEUE = Symbol('queue')\nconst ENDED = Symbol('ended')\nconst EMITTEDEND = Symbol('emittedEnd')\nconst EMIT = Symbol('emit')\nconst UNZIP = Symbol('unzip')\nconst CONSUMECHUNK = Symbol('consumeChunk')\nconst CONSUMECHUNKSUB = Symbol('consumeChunkSub')\nconst CONSUMEBODY = Symbol('consumeBody')\nconst CONSUMEMETA = Symbol('consumeMeta')\nconst CONSUMEHEADER = Symbol('consumeHeader')\nconst CONSUMING = Symbol('consuming')\nconst BUFFERCONCAT = Symbol('bufferConcat')\nconst MAYBEEND = Symbol('maybeEnd')\nconst WRITING = Symbol('writing')\nconst ABORTED = Symbol('aborted')\nconst DONE = Symbol('onDone')\n\nconst noop = _ => true\n\nmodule.exports = warner(class Parser extends EE {\n  constructor (opt) {\n    opt = opt || {}\n    super(opt)\n\n    if (opt.ondone)\n      this.on(DONE, opt.ondone)\n    else\n      this.on(DONE, _ => {\n        this.emit('prefinish')\n        this.emit('finish')\n        this.emit('end')\n        this.emit('close')\n      })\n\n    this.strict = !!opt.strict\n    this.maxMetaEntrySize = opt.maxMetaEntrySize || maxMetaEntrySize\n    this.filter = typeof opt.filter === 'function' ? opt.filter : noop\n\n    // have to set this so that streams are ok piping into it\n    this.writable = true\n    this.readable = false\n\n    this[QUEUE] = new Yallist()\n    this[BUFFER] = null\n    this[READENTRY] = null\n    this[WRITEENTRY] = null\n    this[STATE] = 'begin'\n    this[META] = ''\n    this[EX] = null\n    this[GEX] = null\n    this[ENDED] = false\n    this[UNZIP] = null\n    this[ABORTED] = false\n    if (typeof opt.onwarn === 'function')\n      this.on('warn', opt.onwarn)\n    if (typeof opt.onentry === 'function')\n      this.on('entry', opt.onentry)\n  }\n\n  [CONSUMEHEADER] (chunk, position) {\n    let header\n    try {\n      header = new Header(chunk, position, this[EX], this[GEX])\n    } catch (er) {\n      return this.warn('invalid entry', er)\n    }\n\n    if (header.nullBlock)\n      this[EMIT]('nullBlock')\n    else if (!header.cksumValid)\n      this.warn('invalid entry', header)\n    else if (!header.path)\n      this.warn('invalid: path is required', header)\n    else {\n      const type = header.type\n      if (/^(Symbolic)?Link$/.test(type) && !header.linkpath)\n        this.warn('invalid: linkpath required', header)\n      else if (!/^(Symbolic)?Link$/.test(type) && header.linkpath)\n        this.warn('invalid: linkpath forbidden', header)\n      else {\n        const entry = this[WRITEENTRY] = new Entry(header, this[EX], this[GEX])\n\n        if (entry.meta) {\n          if (entry.size > this.maxMetaEntrySize) {\n            entry.ignore = true\n            this[EMIT]('ignoredEntry', entry)\n            this[STATE] = 'ignore'\n          } else if (entry.size > 0) {\n            this[META] = ''\n            entry.on('data', c => this[META] += c)\n            this[STATE] = 'meta'\n          }\n        } else {\n\n          this[EX] = null\n          entry.ignore = entry.ignore || !this.filter(entry.path, entry)\n          if (entry.ignore) {\n            this[EMIT]('ignoredEntry', entry)\n            this[STATE] = entry.remain ? 'ignore' : 'begin'\n          } else {\n            if (entry.remain)\n              this[STATE] = 'body'\n            else {\n              this[STATE] = 'begin'\n              entry.end()\n            }\n\n            if (!this[READENTRY]) {\n              this[QUEUE].push(entry)\n              this[NEXTENTRY]()\n            } else\n              this[QUEUE].push(entry)\n          }\n        }\n      }\n    }\n  }\n\n  [PROCESSENTRY] (entry) {\n    let go = true\n\n    if (!entry) {\n      this[READENTRY] = null\n      go = false\n    } else if (Array.isArray(entry))\n      this.emit.apply(this, entry)\n    else {\n      this[READENTRY] = entry\n      this.emit('entry', entry)\n      if (!entry.emittedEnd) {\n        entry.on('end', _ => this[NEXTENTRY]())\n        go = false\n      }\n    }\n\n    return go\n  }\n\n  [NEXTENTRY] () {\n    do {} while (this[PROCESSENTRY](this[QUEUE].shift()))\n\n    if (!this[QUEUE].length) {\n      // At this point, there's nothing in the queue, but we may have an\n      // entry which is being consumed (readEntry).\n      // If we don't, then we definitely can handle more data.\n      // If we do, and either it's flowing, or it has never had any data\n      // written to it, then it needs more.\n      // The only other possibility is that it has returned false from a\n      // write() call, so we wait for the next drain to continue.\n      const re = this[READENTRY]\n      const drainNow = !re || re.flowing || re.size === re.remain\n      if (drainNow) {\n        if (!this[WRITING])\n          this.emit('drain')\n      } else\n        re.once('drain', _ => this.emit('drain'))\n     }\n  }\n\n  [CONSUMEBODY] (chunk, position) {\n    // write up to but no  more than writeEntry.blockRemain\n    const entry = this[WRITEENTRY]\n    const br = entry.blockRemain\n    const c = (br >= chunk.length && position === 0) ? chunk\n      : chunk.slice(position, position + br)\n\n    entry.write(c)\n\n    if (!entry.blockRemain) {\n      this[STATE] = 'begin'\n      this[WRITEENTRY] = null\n      entry.end()\n    }\n\n    return c.length\n  }\n\n  [CONSUMEMETA] (chunk, position) {\n    const entry = this[WRITEENTRY]\n    const ret = this[CONSUMEBODY](chunk, position)\n\n    // if we finished, then the entry is reset\n    if (!this[WRITEENTRY])\n      this[EMITMETA](entry)\n\n    return ret\n  }\n\n  [EMIT] (ev, data, extra) {\n    if (!this[QUEUE].length && !this[READENTRY])\n      this.emit(ev, data, extra)\n    else\n      this[QUEUE].push([ev, data, extra])\n  }\n\n  [EMITMETA] (entry) {\n    this[EMIT]('meta', this[META])\n    switch (entry.type) {\n      case 'ExtendedHeader':\n      case 'OldExtendedHeader':\n        this[EX] = Pax.parse(this[META], this[EX], false)\n        break\n\n      case 'GlobalExtendedHeader':\n        this[GEX] = Pax.parse(this[META], this[GEX], true)\n        break\n\n      case 'NextFileHasLongPath':\n      case 'OldGnuLongPath':\n        this[EX] = this[EX] || Object.create(null)\n        this[EX].path = this[META].replace(/\\0.*/, '')\n        break\n\n      case 'NextFileHasLongLinkpath':\n        this[EX] = this[EX] || Object.create(null)\n        this[EX].linkpath = this[META].replace(/\\0.*/, '')\n        break\n\n      /* istanbul ignore next */\n      default: throw new Error('unknown meta: ' + entry.type)\n    }\n  }\n\n  abort (msg, error) {\n    this[ABORTED] = true\n    this.warn(msg, error)\n    this.emit('abort', error)\n    this.emit('error', error)\n  }\n\n  write (chunk) {\n    if (this[ABORTED])\n      return\n\n    // first write, might be gzipped\n    if (this[UNZIP] === null && chunk) {\n      if (this[BUFFER]) {\n        chunk = Buffer.concat([this[BUFFER], chunk])\n        this[BUFFER] = null\n      }\n      if (chunk.length < gzipHeader.length) {\n        this[BUFFER] = chunk\n        return true\n      }\n      for (let i = 0; this[UNZIP] === null && i < gzipHeader.length; i++) {\n        if (chunk[i] !== gzipHeader[i])\n          this[UNZIP] = false\n      }\n      if (this[UNZIP] === null) {\n        const ended = this[ENDED]\n        this[ENDED] = false\n        this[UNZIP] = new zlib.Unzip()\n        this[UNZIP].on('data', chunk => this[CONSUMECHUNK](chunk))\n        this[UNZIP].on('error', er =>\n          this.abort(er.message, er))\n        this[UNZIP].on('end', _ => {\n          this[ENDED] = true\n          this[CONSUMECHUNK]()\n        })\n        this[WRITING] = true\n        const ret = this[UNZIP][ended ? 'end' : 'write' ](chunk)\n        this[WRITING] = false\n        return ret\n      }\n    }\n\n    this[WRITING] = true\n    if (this[UNZIP])\n      this[UNZIP].write(chunk)\n    else\n      this[CONSUMECHUNK](chunk)\n    this[WRITING] = false\n\n    // return false if there's a queue, or if the current entry isn't flowing\n    const ret =\n      this[QUEUE].length ? false :\n      this[READENTRY] ? this[READENTRY].flowing :\n      true\n\n    // if we have no queue, then that means a clogged READENTRY\n    if (!ret && !this[QUEUE].length)\n      this[READENTRY].once('drain', _ => this.emit('drain'))\n\n    return ret\n  }\n\n  [BUFFERCONCAT] (c) {\n    if (c && !this[ABORTED])\n      this[BUFFER] = this[BUFFER] ? Buffer.concat([this[BUFFER], c]) : c\n  }\n\n  [MAYBEEND] () {\n    if (this[ENDED] &&\n        !this[EMITTEDEND] &&\n        !this[ABORTED] &&\n        !this[CONSUMING]) {\n      this[EMITTEDEND] = true\n      const entry = this[WRITEENTRY]\n      if (entry && entry.blockRemain) {\n        const have = this[BUFFER] ? this[BUFFER].length : 0\n        this.warn('Truncated input (needed ' + entry.blockRemain +\n                  ' more bytes, only ' + have + ' available)', entry)\n        if (this[BUFFER])\n          entry.write(this[BUFFER])\n        entry.end()\n      }\n      this[EMIT](DONE)\n    }\n  }\n\n  [CONSUMECHUNK] (chunk) {\n    if (this[CONSUMING]) {\n      this[BUFFERCONCAT](chunk)\n    } else if (!chunk && !this[BUFFER]) {\n      this[MAYBEEND]()\n    } else {\n      this[CONSUMING] = true\n      if (this[BUFFER]) {\n        this[BUFFERCONCAT](chunk)\n        const c = this[BUFFER]\n        this[BUFFER] = null\n        this[CONSUMECHUNKSUB](c)\n      } else {\n        this[CONSUMECHUNKSUB](chunk)\n      }\n\n      while (this[BUFFER] && this[BUFFER].length >= 512 && !this[ABORTED]) {\n        const c = this[BUFFER]\n        this[BUFFER] = null\n        this[CONSUMECHUNKSUB](c)\n      }\n      this[CONSUMING] = false\n    }\n\n    if (!this[BUFFER] || this[ENDED])\n      this[MAYBEEND]()\n  }\n\n  [CONSUMECHUNKSUB] (chunk) {\n    // we know that we are in CONSUMING mode, so anything written goes into\n    // the buffer.  Advance the position and put any remainder in the buffer.\n    let position = 0\n    let length = chunk.length\n    while (position + 512 <= length && !this[ABORTED]) {\n      switch (this[STATE]) {\n        case 'begin':\n          this[CONSUMEHEADER](chunk, position)\n          position += 512\n          break\n\n        case 'ignore':\n        case 'body':\n          position += this[CONSUMEBODY](chunk, position)\n          break\n\n        case 'meta':\n          position += this[CONSUMEMETA](chunk, position)\n          break\n\n        /* istanbul ignore next */\n        default:\n          throw new Error('invalid state: ' + this[STATE])\n      }\n    }\n\n    if (position < length) {\n      if (this[BUFFER])\n        this[BUFFER] = Buffer.concat([chunk.slice(position), this[BUFFER]])\n      else\n        this[BUFFER] = chunk.slice(position)\n    }\n  }\n\n  end (chunk) {\n    if (!this[ABORTED]) {\n      if (this[UNZIP])\n        this[UNZIP].end(chunk)\n      else {\n        this[ENDED] = true\n        this.write(chunk)\n      }\n    }\n  }\n})\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/parse.js?");

/***/ }),

/***/ "./node_modules/tar/lib/pax.js":
/*!*************************************!*\
  !*** ./node_modules/tar/lib/pax.js ***!
  \*************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nconst Buffer = __webpack_require__(/*! ./buffer.js */ \"./node_modules/tar/lib/buffer.js\")\nconst Header = __webpack_require__(/*! ./header.js */ \"./node_modules/tar/lib/header.js\")\nconst path = __webpack_require__(/*! path */ \"path\")\n\nclass Pax {\n  constructor (obj, global) {\n    this.atime = obj.atime || null\n    this.charset = obj.charset || null\n    this.comment = obj.comment || null\n    this.ctime = obj.ctime || null\n    this.gid = obj.gid || null\n    this.gname = obj.gname || null\n    this.linkpath = obj.linkpath || null\n    this.mtime = obj.mtime || null\n    this.path = obj.path || null\n    this.size = obj.size || null\n    this.uid = obj.uid || null\n    this.uname = obj.uname || null\n    this.dev = obj.dev || null\n    this.ino = obj.ino || null\n    this.nlink = obj.nlink || null\n    this.global = global || false\n  }\n\n  encode () {\n    const body = this.encodeBody()\n    if (body === '')\n      return null\n\n    const bodyLen = Buffer.byteLength(body)\n    // round up to 512 bytes\n    // add 512 for header\n    const bufLen = 512 * Math.ceil(1 + bodyLen / 512)\n    const buf = Buffer.allocUnsafe(bufLen)\n\n    // 0-fill the header section, it might not hit every field\n    for (let i = 0; i < 512; i++) {\n      buf[i] = 0\n    }\n\n    new Header({\n      // XXX split the path\n      // then the path should be PaxHeader + basename, but less than 99,\n      // prepend with the dirname\n      path: ('PaxHeader/' + path.basename(this.path)).slice(0, 99),\n      mode: this.mode || 0o644,\n      uid: this.uid || null,\n      gid: this.gid || null,\n      size: bodyLen,\n      mtime: this.mtime || null,\n      type: this.global ? 'GlobalExtendedHeader' : 'ExtendedHeader',\n      linkpath: '',\n      uname: this.uname || '',\n      gname: this.gname || '',\n      devmaj: 0,\n      devmin: 0,\n      atime: this.atime || null,\n      ctime: this.ctime || null\n    }).encode(buf)\n\n    buf.write(body, 512, bodyLen, 'utf8')\n\n    // null pad after the body\n    for (let i = bodyLen + 512; i < buf.length; i++) {\n      buf[i] = 0\n    }\n\n    return buf\n  }\n\n  encodeBody () {\n    return (\n      this.encodeField('path') +\n      this.encodeField('ctime') +\n      this.encodeField('atime') +\n      this.encodeField('dev') +\n      this.encodeField('ino') +\n      this.encodeField('nlink') +\n      this.encodeField('charset') +\n      this.encodeField('comment') +\n      this.encodeField('gid') +\n      this.encodeField('gname') +\n      this.encodeField('linkpath') +\n      this.encodeField('mtime') +\n      this.encodeField('size') +\n      this.encodeField('uid') +\n      this.encodeField('uname')\n    )\n  }\n\n  encodeField (field) {\n    if (this[field] === null || this[field] === undefined)\n      return ''\n    const v = this[field] instanceof Date ? this[field].getTime() / 1000\n      : this[field]\n    const s = ' ' +\n      (field === 'dev' || field === 'ino' || field === 'nlink'\n       ? 'SCHILY.' : '') +\n      field + '=' + v + '\\n'\n    const byteLen = Buffer.byteLength(s)\n    // the digits includes the length of the digits in ascii base-10\n    // so if it's 9 characters, then adding 1 for the 9 makes it 10\n    // which makes it 11 chars.\n    let digits = Math.floor(Math.log(byteLen) / Math.log(10)) + 1\n    if (byteLen + digits >= Math.pow(10, digits))\n      digits += 1\n    const len = digits + byteLen\n    return len + s\n  }\n}\n\nPax.parse = (string, ex, g) => new Pax(merge(parseKV(string), ex), g)\n\nconst merge = (a, b) =>\n  b ? Object.keys(a).reduce((s, k) => (s[k] = a[k], s), b) : a\n\nconst parseKV = string =>\n  string\n    .replace(/\\n$/, '')\n    .split('\\n')\n    .reduce(parseKVLine, Object.create(null))\n\nconst parseKVLine = (set, line) => {\n  const n = parseInt(line, 10)\n\n  // XXX Values with \\n in them will fail this.\n  // Refactor to not be a naive line-by-line parse.\n  if (n !== Buffer.byteLength(line) + 1)\n    return set\n\n  line = line.substr((n + ' ').length)\n  const kv = line.split('=')\n  const k = kv.shift().replace(/^SCHILY\\.(dev|ino|nlink)/, '$1')\n  if (!k)\n    return set\n\n  const v = kv.join('=')\n  set[k] = /^([A-Z]+\\.)?([mac]|birth|creation)time$/.test(k)\n    ?  new Date(v * 1000)\n    : /^[0-9]+$/.test(v) ? +v\n    : v\n  return set\n}\n\nmodule.exports = Pax\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/pax.js?");

/***/ }),

/***/ "./node_modules/tar/lib/read-entry.js":
/*!********************************************!*\
  !*** ./node_modules/tar/lib/read-entry.js ***!
  \********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nconst types = __webpack_require__(/*! ./types.js */ \"./node_modules/tar/lib/types.js\")\nconst MiniPass = __webpack_require__(/*! minipass */ \"./node_modules/minipass/index.js\")\n\nconst SLURP = Symbol('slurp')\nmodule.exports = class ReadEntry extends MiniPass {\n  constructor (header, ex, gex) {\n    super()\n    // read entries always start life paused.  this is to avoid the\n    // situation where Minipass's auto-ending empty streams results\n    // in an entry ending before we're ready for it.\n    this.pause()\n    this.extended = ex\n    this.globalExtended = gex\n    this.header = header\n    this.startBlockSize = 512 * Math.ceil(header.size / 512)\n    this.blockRemain = this.startBlockSize\n    this.remain = header.size\n    this.type = header.type\n    this.meta = false\n    this.ignore = false\n    switch (this.type) {\n      case 'File':\n      case 'OldFile':\n      case 'Link':\n      case 'SymbolicLink':\n      case 'CharacterDevice':\n      case 'BlockDevice':\n      case 'Directory':\n      case 'FIFO':\n      case 'ContiguousFile':\n      case 'GNUDumpDir':\n        break\n\n      case 'NextFileHasLongLinkpath':\n      case 'NextFileHasLongPath':\n      case 'OldGnuLongPath':\n      case 'GlobalExtendedHeader':\n      case 'ExtendedHeader':\n      case 'OldExtendedHeader':\n        this.meta = true\n        break\n\n      // NOTE: gnutar and bsdtar treat unrecognized types as 'File'\n      // it may be worth doing the same, but with a warning.\n      default:\n        this.ignore = true\n    }\n\n    this.path = header.path\n    this.mode = header.mode\n    if (this.mode)\n      this.mode = this.mode & 0o7777\n    this.uid = header.uid\n    this.gid = header.gid\n    this.uname = header.uname\n    this.gname = header.gname\n    this.size = header.size\n    this.mtime = header.mtime\n    this.atime = header.atime\n    this.ctime = header.ctime\n    this.linkpath = header.linkpath\n    this.uname = header.uname\n    this.gname = header.gname\n\n    if (ex) this[SLURP](ex)\n    if (gex) this[SLURP](gex, true)\n  }\n\n  write (data) {\n    const writeLen = data.length\n    if (writeLen > this.blockRemain)\n      throw new Error('writing more to entry than is appropriate')\n\n    const r = this.remain\n    const br = this.blockRemain\n    this.remain = Math.max(0, r - writeLen)\n    this.blockRemain = Math.max(0, br - writeLen)\n    if (this.ignore)\n      return true\n\n    if (r >= writeLen)\n      return super.write(data)\n\n    // r < writeLen\n    return super.write(data.slice(0, r))\n  }\n\n  [SLURP] (ex, global) {\n    for (let k in ex) {\n      // we slurp in everything except for the path attribute in\n      // a global extended header, because that's weird.\n      if (ex[k] !== null && ex[k] !== undefined &&\n          !(global && k === 'path'))\n        this[k] = ex[k]\n    }\n  }\n}\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/read-entry.js?");

/***/ }),

/***/ "./node_modules/tar/lib/replace.js":
/*!*****************************************!*\
  !*** ./node_modules/tar/lib/replace.js ***!
  \*****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nconst Buffer = __webpack_require__(/*! ./buffer.js */ \"./node_modules/tar/lib/buffer.js\")\n\n// tar -r\nconst hlo = __webpack_require__(/*! ./high-level-opt.js */ \"./node_modules/tar/lib/high-level-opt.js\")\nconst Pack = __webpack_require__(/*! ./pack.js */ \"./node_modules/tar/lib/pack.js\")\nconst Parse = __webpack_require__(/*! ./parse.js */ \"./node_modules/tar/lib/parse.js\")\nconst fs = __webpack_require__(/*! fs */ \"fs\")\nconst fsm = __webpack_require__(/*! fs-minipass */ \"./node_modules/fs-minipass/index.js\")\nconst t = __webpack_require__(/*! ./list.js */ \"./node_modules/tar/lib/list.js\")\nconst path = __webpack_require__(/*! path */ \"path\")\n\n// starting at the head of the file, read a Header\n// If the checksum is invalid, that's our position to start writing\n// If it is, jump forward by the specified size (round up to 512)\n// and try again.\n// Write the new Pack stream starting there.\n\nconst Header = __webpack_require__(/*! ./header.js */ \"./node_modules/tar/lib/header.js\")\n\nconst r = module.exports = (opt_, files, cb) => {\n  const opt = hlo(opt_)\n\n  if (!opt.file)\n    throw new TypeError('file is required')\n\n  if (opt.gzip)\n    throw new TypeError('cannot append to compressed archives')\n\n  if (!files || !Array.isArray(files) || !files.length)\n    throw new TypeError('no files or directories specified')\n\n  files = Array.from(files)\n\n  return opt.sync ? replaceSync(opt, files)\n    : replace(opt, files, cb)\n}\n\nconst replaceSync = (opt, files) => {\n  const p = new Pack.Sync(opt)\n\n  let threw = true\n  let fd\n  let position\n\n  try {\n    try {\n      fd = fs.openSync(opt.file, 'r+')\n    } catch (er) {\n      if (er.code === 'ENOENT')\n        fd = fs.openSync(opt.file, 'w+')\n      else\n        throw er\n    }\n\n    const st = fs.fstatSync(fd)\n    const headBuf = Buffer.alloc(512)\n\n    POSITION: for (position = 0; position < st.size; position += 512) {\n      for (let bufPos = 0, bytes = 0; bufPos < 512; bufPos += bytes) {\n        bytes = fs.readSync(\n          fd, headBuf, bufPos, headBuf.length - bufPos, position + bufPos\n        )\n\n        if (position === 0 && headBuf[0] === 0x1f && headBuf[1] === 0x8b)\n          throw new Error('cannot append to compressed archives')\n\n        if (!bytes)\n          break POSITION\n      }\n\n      let h = new Header(headBuf)\n      if (!h.cksumValid)\n        break\n      let entryBlockSize = 512 * Math.ceil(h.size / 512)\n      if (position + entryBlockSize + 512 > st.size)\n        break\n      // the 512 for the header we just parsed will be added as well\n      // also jump ahead all the blocks for the body\n      position += entryBlockSize\n      if (opt.mtimeCache)\n        opt.mtimeCache.set(h.path, h.mtime)\n    }\n    threw = false\n\n    streamSync(opt, p, position, fd, files)\n  } finally {\n    if (threw)\n      try { fs.closeSync(fd) } catch (er) {}\n  }\n}\n\nconst streamSync = (opt, p, position, fd, files) => {\n  const stream = new fsm.WriteStreamSync(opt.file, {\n    fd: fd,\n    start: position\n  })\n  p.pipe(stream)\n  addFilesSync(p, files)\n}\n\nconst replace = (opt, files, cb) => {\n  files = Array.from(files)\n  const p = new Pack(opt)\n\n  const getPos = (fd, size, cb_) => {\n    const cb = (er, pos) => {\n      if (er)\n        fs.close(fd, _ => cb_(er))\n      else\n        cb_(null, pos)\n    }\n\n    let position = 0\n    if (size === 0)\n      return cb(null, 0)\n\n    let bufPos = 0\n    const headBuf = Buffer.alloc(512)\n    const onread = (er, bytes) => {\n      if (er)\n        return cb(er)\n      bufPos += bytes\n      if (bufPos < 512 && bytes)\n        return fs.read(\n          fd, headBuf, bufPos, headBuf.length - bufPos,\n          position + bufPos, onread\n        )\n\n      if (position === 0 && headBuf[0] === 0x1f && headBuf[1] === 0x8b)\n        return cb(new Error('cannot append to compressed archives'))\n\n      // truncated header\n      if (bufPos < 512)\n        return cb(null, position)\n\n      const h = new Header(headBuf)\n      if (!h.cksumValid)\n        return cb(null, position)\n\n      const entryBlockSize = 512 * Math.ceil(h.size / 512)\n      if (position + entryBlockSize + 512 > size)\n        return cb(null, position)\n\n      position += entryBlockSize + 512\n      if (position >= size)\n        return cb(null, position)\n\n      if (opt.mtimeCache)\n        opt.mtimeCache.set(h.path, h.mtime)\n      bufPos = 0\n      fs.read(fd, headBuf, 0, 512, position, onread)\n    }\n    fs.read(fd, headBuf, 0, 512, position, onread)\n  }\n\n  const promise = new Promise((resolve, reject) => {\n    p.on('error', reject)\n    let flag = 'r+'\n    const onopen = (er, fd) => {\n      if (er && er.code === 'ENOENT' && flag === 'r+') {\n        flag = 'w+'\n        return fs.open(opt.file, flag, onopen)\n      }\n\n      if (er)\n        return reject(er)\n\n      fs.fstat(fd, (er, st) => {\n        if (er)\n          return reject(er)\n        getPos(fd, st.size, (er, position) => {\n          if (er)\n            return reject(er)\n          const stream = new fsm.WriteStream(opt.file, {\n            fd: fd,\n            start: position\n          })\n          p.pipe(stream)\n          stream.on('error', reject)\n          stream.on('close', resolve)\n          addFilesAsync(p, files)\n        })\n      })\n    }\n    fs.open(opt.file, flag, onopen)\n  })\n\n  return cb ? promise.then(cb, cb) : promise\n}\n\nconst addFilesSync = (p, files) => {\n  files.forEach(file => {\n    if (file.charAt(0) === '@')\n      t({\n        file: path.resolve(p.cwd, file.substr(1)),\n        sync: true,\n        noResume: true,\n        onentry: entry => p.add(entry)\n      })\n    else\n      p.add(file)\n  })\n  p.end()\n}\n\nconst addFilesAsync = (p, files) => {\n  while (files.length) {\n    const file = files.shift()\n    if (file.charAt(0) === '@')\n      return t({\n        file: path.resolve(p.cwd, file.substr(1)),\n        noResume: true,\n        onentry: entry => p.add(entry)\n      }).then(_ => addFilesAsync(p, files))\n    else\n      p.add(file)\n  }\n  p.end()\n}\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/replace.js?");

/***/ }),

/***/ "./node_modules/tar/lib/types.js":
/*!***************************************!*\
  !*** ./node_modules/tar/lib/types.js ***!
  \***************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n// map types from key to human-friendly name\nexports.name = new Map([\n  ['0', 'File'],\n  // same as File\n  ['', 'OldFile'],\n  ['1', 'Link'],\n  ['2', 'SymbolicLink'],\n  // Devices and FIFOs aren't fully supported\n  // they are parsed, but skipped when unpacking\n  ['3', 'CharacterDevice'],\n  ['4', 'BlockDevice'],\n  ['5', 'Directory'],\n  ['6', 'FIFO'],\n  // same as File\n  ['7', 'ContiguousFile'],\n  // pax headers\n  ['g', 'GlobalExtendedHeader'],\n  ['x', 'ExtendedHeader'],\n  // vendor-specific stuff\n  // skip\n  ['A', 'SolarisACL'],\n  // like 5, but with data, which should be skipped\n  ['D', 'GNUDumpDir'],\n  // metadata only, skip\n  ['I', 'Inode'],\n  // data = link path of next file\n  ['K', 'NextFileHasLongLinkpath'],\n  // data = path of next file\n  ['L', 'NextFileHasLongPath'],\n  // skip\n  ['M', 'ContinuationFile'],\n  // like L\n  ['N', 'OldGnuLongPath'],\n  // skip\n  ['S', 'SparseFile'],\n  // skip\n  ['V', 'TapeVolumeHeader'],\n  // like x\n  ['X', 'OldExtendedHeader']\n])\n\n// map the other direction\nexports.code = new Map(Array.from(exports.name).map(kv => [kv[1], kv[0]]))\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/types.js?");

/***/ }),

/***/ "./node_modules/tar/lib/unpack.js":
/*!****************************************!*\
  !*** ./node_modules/tar/lib/unpack.js ***!
  \****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\nconst assert = __webpack_require__(/*! assert */ \"assert\")\nconst EE = __webpack_require__(/*! events */ \"events\").EventEmitter\nconst Parser = __webpack_require__(/*! ./parse.js */ \"./node_modules/tar/lib/parse.js\")\nconst fs = __webpack_require__(/*! fs */ \"fs\")\nconst fsm = __webpack_require__(/*! fs-minipass */ \"./node_modules/fs-minipass/index.js\")\nconst path = __webpack_require__(/*! path */ \"path\")\nconst mkdir = __webpack_require__(/*! ./mkdir.js */ \"./node_modules/tar/lib/mkdir.js\")\nconst mkdirSync = mkdir.sync\nconst wc = __webpack_require__(/*! ./winchars.js */ \"./node_modules/tar/lib/winchars.js\")\n\nconst ONENTRY = Symbol('onEntry')\nconst CHECKFS = Symbol('checkFs')\nconst ISREUSABLE = Symbol('isReusable')\nconst MAKEFS = Symbol('makeFs')\nconst FILE = Symbol('file')\nconst DIRECTORY = Symbol('directory')\nconst LINK = Symbol('link')\nconst SYMLINK = Symbol('symlink')\nconst HARDLINK = Symbol('hardlink')\nconst UNSUPPORTED = Symbol('unsupported')\nconst UNKNOWN = Symbol('unknown')\nconst CHECKPATH = Symbol('checkPath')\nconst MKDIR = Symbol('mkdir')\nconst ONERROR = Symbol('onError')\nconst PENDING = Symbol('pending')\nconst PEND = Symbol('pend')\nconst UNPEND = Symbol('unpend')\nconst ENDED = Symbol('ended')\nconst MAYBECLOSE = Symbol('maybeClose')\nconst SKIP = Symbol('skip')\nconst DOCHOWN = Symbol('doChown')\nconst UID = Symbol('uid')\nconst GID = Symbol('gid')\nconst crypto = __webpack_require__(/*! crypto */ \"crypto\")\n\n// Unlinks on Windows are not atomic.\n//\n// This means that if you have a file entry, followed by another\n// file entry with an identical name, and you cannot re-use the file\n// (because it's a hardlink, or because unlink:true is set, or it's\n// Windows, which does not have useful nlink values), then the unlink\n// will be committed to the disk AFTER the new file has been written\n// over the old one, deleting the new file.\n//\n// To work around this, on Windows systems, we rename the file and then\n// delete the renamed file.  It's a sloppy kludge, but frankly, I do not\n// know of a better way to do this, given windows' non-atomic unlink\n// semantics.\n//\n// See: https://github.com/npm/node-tar/issues/183\n/* istanbul ignore next */\nconst unlinkFile = (path, cb) => {\n  if (process.platform !== 'win32')\n    return fs.unlink(path, cb)\n\n  const name = path + '.DELETE.' + crypto.randomBytes(16).toString('hex')\n  fs.rename(path, name, er => {\n    if (er)\n      return cb(er)\n    fs.unlink(name, cb)\n  })\n}\n\n/* istanbul ignore next */\nconst unlinkFileSync = path => {\n  if (process.platform !== 'win32')\n    return fs.unlinkSync(path)\n\n  const name = path + '.DELETE.' + crypto.randomBytes(16).toString('hex')\n  fs.renameSync(path, name)\n  fs.unlinkSync(name)\n}\n\n// this.gid, entry.gid, this.processUid\nconst uint32 = (a, b, c) =>\n  a === a >>> 0 ? a\n  : b === b >>> 0 ? b\n  : c\n\nclass Unpack extends Parser {\n  constructor (opt) {\n    if (!opt)\n      opt = {}\n\n    opt.ondone = _ => {\n      this[ENDED] = true\n      this[MAYBECLOSE]()\n    }\n\n    super(opt)\n\n    this.transform = typeof opt.transform === 'function' ? opt.transform : null\n\n    this.writable = true\n    this.readable = false\n\n    this[PENDING] = 0\n    this[ENDED] = false\n\n    this.dirCache = opt.dirCache || new Map()\n\n    if (typeof opt.uid === 'number' || typeof opt.gid === 'number') {\n      // need both or neither\n      if (typeof opt.uid !== 'number' || typeof opt.gid !== 'number')\n        throw new TypeError('cannot set owner without number uid and gid')\n      if (opt.preserveOwner)\n        throw new TypeError(\n          'cannot preserve owner in archive and also set owner explicitly')\n      this.uid = opt.uid\n      this.gid = opt.gid\n      this.setOwner = true\n    } else {\n      this.uid = null\n      this.gid = null\n      this.setOwner = false\n    }\n\n    // default true for root\n    if (opt.preserveOwner === undefined && typeof opt.uid !== 'number')\n      this.preserveOwner = process.getuid && process.getuid() === 0\n    else\n      this.preserveOwner = !!opt.preserveOwner\n\n    this.processUid = (this.preserveOwner || this.setOwner) && process.getuid ?\n      process.getuid() : null\n    this.processGid = (this.preserveOwner || this.setOwner) && process.getgid ?\n      process.getgid() : null\n\n    // mostly just for testing, but useful in some cases.\n    // Forcibly trigger a chown on every entry, no matter what\n    this.forceChown = opt.forceChown === true\n\n    // turn ><?| in filenames into 0xf000-higher encoded forms\n    this.win32 = !!opt.win32 || process.platform === 'win32'\n\n    // do not unpack over files that are newer than what's in the archive\n    this.newer = !!opt.newer\n\n    // do not unpack over ANY files\n    this.keep = !!opt.keep\n\n    // do not set mtime/atime of extracted entries\n    this.noMtime = !!opt.noMtime\n\n    // allow .., absolute path entries, and unpacking through symlinks\n    // without this, warn and skip .., relativize absolutes, and error\n    // on symlinks in extraction path\n    this.preservePaths = !!opt.preservePaths\n\n    // unlink files and links before writing. This breaks existing hard\n    // links, and removes symlink directories rather than erroring\n    this.unlink = !!opt.unlink\n\n    this.cwd = path.resolve(opt.cwd || process.cwd())\n    this.strip = +opt.strip || 0\n    this.processUmask = process.umask()\n    this.umask = typeof opt.umask === 'number' ? opt.umask : this.processUmask\n    // default mode for dirs created as parents\n    this.dmode = opt.dmode || (0o0777 & (~this.umask))\n    this.fmode = opt.fmode || (0o0666 & (~this.umask))\n    this.on('entry', entry => this[ONENTRY](entry))\n  }\n\n  [MAYBECLOSE] () {\n    if (this[ENDED] && this[PENDING] === 0) {\n      this.emit('prefinish')\n      this.emit('finish')\n      this.emit('end')\n      this.emit('close')\n    }\n  }\n\n  [CHECKPATH] (entry) {\n    if (this.strip) {\n      const parts = entry.path.split(/\\/|\\\\/)\n      if (parts.length < this.strip)\n        return false\n      entry.path = parts.slice(this.strip).join('/')\n\n      if (entry.type === 'Link') {\n        const linkparts = entry.linkpath.split(/\\/|\\\\/)\n        if (linkparts.length >= this.strip)\n          entry.linkpath = linkparts.slice(this.strip).join('/')\n      }\n    }\n\n    if (!this.preservePaths) {\n      const p = entry.path\n      if (p.match(/(^|\\/|\\\\)\\.\\.(\\\\|\\/|$)/)) {\n        this.warn('path contains \\'..\\'', p)\n        return false\n      }\n\n      // absolutes on posix are also absolutes on win32\n      // so we only need to test this one to get both\n      if (path.win32.isAbsolute(p)) {\n        const parsed = path.win32.parse(p)\n        this.warn('stripping ' + parsed.root + ' from absolute path', p)\n        entry.path = p.substr(parsed.root.length)\n      }\n    }\n\n    // only encode : chars that aren't drive letter indicators\n    if (this.win32) {\n      const parsed = path.win32.parse(entry.path)\n      entry.path = parsed.root === '' ? wc.encode(entry.path)\n        : parsed.root + wc.encode(entry.path.substr(parsed.root.length))\n    }\n\n    if (path.isAbsolute(entry.path))\n      entry.absolute = entry.path\n    else\n      entry.absolute = path.resolve(this.cwd, entry.path)\n\n    return true\n  }\n\n  [ONENTRY] (entry) {\n    if (!this[CHECKPATH](entry))\n      return entry.resume()\n\n    assert.equal(typeof entry.absolute, 'string')\n\n    switch (entry.type) {\n      case 'Directory':\n      case 'GNUDumpDir':\n        if (entry.mode)\n          entry.mode = entry.mode | 0o700\n\n      case 'File':\n      case 'OldFile':\n      case 'ContiguousFile':\n      case 'Link':\n      case 'SymbolicLink':\n        return this[CHECKFS](entry)\n\n      case 'CharacterDevice':\n      case 'BlockDevice':\n      case 'FIFO':\n        return this[UNSUPPORTED](entry)\n    }\n  }\n\n  [ONERROR] (er, entry) {\n    // Cwd has to exist, or else nothing works. That's serious.\n    // Other errors are warnings, which raise the error in strict\n    // mode, but otherwise continue on.\n    if (er.name === 'CwdError')\n      this.emit('error', er)\n    else {\n      this.warn(er.message, er)\n      this[UNPEND]()\n      entry.resume()\n    }\n  }\n\n  [MKDIR] (dir, mode, cb) {\n    mkdir(dir, {\n      uid: this.uid,\n      gid: this.gid,\n      processUid: this.processUid,\n      processGid: this.processGid,\n      umask: this.processUmask,\n      preserve: this.preservePaths,\n      unlink: this.unlink,\n      cache: this.dirCache,\n      cwd: this.cwd,\n      mode: mode\n    }, cb)\n  }\n\n  [DOCHOWN] (entry) {\n    // in preserve owner mode, chown if the entry doesn't match process\n    // in set owner mode, chown if setting doesn't match process\n    return this.forceChown ||\n      this.preserveOwner &&\n      ( typeof entry.uid === 'number' && entry.uid !== this.processUid ||\n        typeof entry.gid === 'number' && entry.gid !== this.processGid )\n      ||\n      ( typeof this.uid === 'number' && this.uid !== this.processUid ||\n        typeof this.gid === 'number' && this.gid !== this.processGid )\n  }\n\n  [UID] (entry) {\n    return uint32(this.uid, entry.uid, this.processUid)\n  }\n\n  [GID] (entry) {\n    return uint32(this.gid, entry.gid, this.processGid)\n  }\n\n  [FILE] (entry) {\n    const mode = entry.mode & 0o7777 || this.fmode\n    const stream = new fsm.WriteStream(entry.absolute, {\n      mode: mode,\n      autoClose: false\n    })\n    stream.on('error', er => this[ONERROR](er, entry))\n\n    let actions = 1\n    const done = er => {\n      if (er)\n        return this[ONERROR](er, entry)\n\n      if (--actions === 0)\n        fs.close(stream.fd, _ => this[UNPEND]())\n    }\n\n    stream.on('finish', _ => {\n      // if futimes fails, try utimes\n      // if utimes fails, fail with the original error\n      // same for fchown/chown\n      const abs = entry.absolute\n      const fd = stream.fd\n\n      if (entry.mtime && !this.noMtime) {\n        actions++\n        const atime = entry.atime || new Date()\n        const mtime = entry.mtime\n        fs.futimes(fd, atime, mtime, er =>\n          er ? fs.utimes(abs, atime, mtime, er2 => done(er2 && er))\n          : done())\n      }\n\n      if (this[DOCHOWN](entry)) {\n        actions++\n        const uid = this[UID](entry)\n        const gid = this[GID](entry)\n        fs.fchown(fd, uid, gid, er =>\n          er ? fs.chown(abs, uid, gid, er2 => done(er2 && er))\n          : done())\n      }\n\n      done()\n    })\n\n    const tx = this.transform ? this.transform(entry) || entry : entry\n    if (tx !== entry) {\n      tx.on('error', er => this[ONERROR](er, entry))\n      entry.pipe(tx)\n    }\n    tx.pipe(stream)\n  }\n\n  [DIRECTORY] (entry) {\n    const mode = entry.mode & 0o7777 || this.dmode\n    this[MKDIR](entry.absolute, mode, er => {\n      if (er)\n        return this[ONERROR](er, entry)\n\n      let actions = 1\n      const done = _ => {\n        if (--actions === 0) {\n          this[UNPEND]()\n          entry.resume()\n        }\n      }\n\n      if (entry.mtime && !this.noMtime) {\n        actions++\n        fs.utimes(entry.absolute, entry.atime || new Date(), entry.mtime, done)\n      }\n\n      if (this[DOCHOWN](entry)) {\n        actions++\n        fs.chown(entry.absolute, this[UID](entry), this[GID](entry), done)\n      }\n\n      done()\n    })\n  }\n\n  [UNSUPPORTED] (entry) {\n    this.warn('unsupported entry type: ' + entry.type, entry)\n    entry.resume()\n  }\n\n  [SYMLINK] (entry) {\n    this[LINK](entry, entry.linkpath, 'symlink')\n  }\n\n  [HARDLINK] (entry) {\n    this[LINK](entry, path.resolve(this.cwd, entry.linkpath), 'link')\n  }\n\n  [PEND] () {\n    this[PENDING]++\n  }\n\n  [UNPEND] () {\n    this[PENDING]--\n    this[MAYBECLOSE]()\n  }\n\n  [SKIP] (entry) {\n    this[UNPEND]()\n    entry.resume()\n  }\n\n  // Check if we can reuse an existing filesystem entry safely and\n  // overwrite it, rather than unlinking and recreating\n  // Windows doesn't report a useful nlink, so we just never reuse entries\n  [ISREUSABLE] (entry, st) {\n    return entry.type === 'File' &&\n      !this.unlink &&\n      st.isFile() &&\n      st.nlink <= 1 &&\n      process.platform !== 'win32'\n  }\n\n  // check if a thing is there, and if so, try to clobber it\n  [CHECKFS] (entry) {\n    this[PEND]()\n    this[MKDIR](path.dirname(entry.absolute), this.dmode, er => {\n      if (er)\n        return this[ONERROR](er, entry)\n      fs.lstat(entry.absolute, (er, st) => {\n        if (st && (this.keep || this.newer && st.mtime > entry.mtime))\n          this[SKIP](entry)\n        else if (er || this[ISREUSABLE](entry, st))\n          this[MAKEFS](null, entry)\n        else if (st.isDirectory()) {\n          if (entry.type === 'Directory') {\n            if (!entry.mode || (st.mode & 0o7777) === entry.mode)\n              this[MAKEFS](null, entry)\n            else\n              fs.chmod(entry.absolute, entry.mode, er => this[MAKEFS](er, entry))\n          } else\n            fs.rmdir(entry.absolute, er => this[MAKEFS](er, entry))\n        } else\n          unlinkFile(entry.absolute, er => this[MAKEFS](er, entry))\n      })\n    })\n  }\n\n  [MAKEFS] (er, entry) {\n    if (er)\n      return this[ONERROR](er, entry)\n\n    switch (entry.type) {\n      case 'File':\n      case 'OldFile':\n      case 'ContiguousFile':\n        return this[FILE](entry)\n\n      case 'Link':\n        return this[HARDLINK](entry)\n\n      case 'SymbolicLink':\n        return this[SYMLINK](entry)\n\n      case 'Directory':\n      case 'GNUDumpDir':\n        return this[DIRECTORY](entry)\n    }\n  }\n\n  [LINK] (entry, linkpath, link) {\n    // XXX: get the type ('file' or 'dir') for windows\n    fs[link](linkpath, entry.absolute, er => {\n      if (er)\n        return this[ONERROR](er, entry)\n      this[UNPEND]()\n      entry.resume()\n    })\n  }\n}\n\nclass UnpackSync extends Unpack {\n  constructor (opt) {\n    super(opt)\n  }\n\n  [CHECKFS] (entry) {\n    const er = this[MKDIR](path.dirname(entry.absolute), this.dmode)\n    if (er)\n      return this[ONERROR](er, entry)\n    try {\n      const st = fs.lstatSync(entry.absolute)\n      if (this.keep || this.newer && st.mtime > entry.mtime)\n        return this[SKIP](entry)\n      else if (this[ISREUSABLE](entry, st))\n        return this[MAKEFS](null, entry)\n      else {\n        try {\n          if (st.isDirectory()) {\n            if (entry.type === 'Directory') {\n              if (entry.mode && (st.mode & 0o7777) !== entry.mode)\n                fs.chmodSync(entry.absolute, entry.mode)\n            } else\n              fs.rmdirSync(entry.absolute)\n          } else\n            unlinkFileSync(entry.absolute)\n          return this[MAKEFS](null, entry)\n        } catch (er) {\n          return this[ONERROR](er, entry)\n        }\n      }\n    } catch (er) {\n      return this[MAKEFS](null, entry)\n    }\n  }\n\n  [FILE] (entry) {\n    const mode = entry.mode & 0o7777 || this.fmode\n\n    const oner = er => {\n      try { fs.closeSync(fd) } catch (_) {}\n      if (er)\n        this[ONERROR](er, entry)\n    }\n\n    let stream\n    let fd\n    try {\n      fd = fs.openSync(entry.absolute, 'w', mode)\n    } catch (er) {\n      return oner(er)\n    }\n    const tx = this.transform ? this.transform(entry) || entry : entry\n    if (tx !== entry) {\n      tx.on('error', er => this[ONERROR](er, entry))\n      entry.pipe(tx)\n    }\n\n    tx.on('data', chunk => {\n      try {\n        fs.writeSync(fd, chunk, 0, chunk.length)\n      } catch (er) {\n        oner(er)\n      }\n    })\n\n    tx.on('end', _ => {\n      let er = null\n      // try both, falling futimes back to utimes\n      // if either fails, handle the first error\n      if (entry.mtime && !this.noMtime) {\n        const atime = entry.atime || new Date()\n        const mtime = entry.mtime\n        try {\n          fs.futimesSync(fd, atime, mtime)\n        } catch (futimeser) {\n          try {\n            fs.utimesSync(entry.absolute, atime, mtime)\n          } catch (utimeser) {\n            er = futimeser\n          }\n        }\n      }\n\n      if (this[DOCHOWN](entry)) {\n        const uid = this[UID](entry)\n        const gid = this[GID](entry)\n\n        try {\n          fs.fchownSync(fd, uid, gid)\n        } catch (fchowner) {\n          try {\n            fs.chownSync(entry.absolute, uid, gid)\n          } catch (chowner) {\n            er = er || fchowner\n          }\n        }\n      }\n\n      oner(er)\n    })\n  }\n\n  [DIRECTORY] (entry) {\n    const mode = entry.mode & 0o7777 || this.dmode\n    const er = this[MKDIR](entry.absolute, mode)\n    if (er)\n      return this[ONERROR](er, entry)\n    if (entry.mtime && !this.noMtime) {\n      try {\n        fs.utimesSync(entry.absolute, entry.atime || new Date(), entry.mtime)\n      } catch (er) {}\n    }\n    if (this[DOCHOWN](entry)) {\n      try {\n        fs.chownSync(entry.absolute, this[UID](entry), this[GID](entry))\n      } catch (er) {}\n    }\n    entry.resume()\n  }\n\n  [MKDIR] (dir, mode) {\n    try {\n      return mkdir.sync(dir, {\n        uid: this.uid,\n        gid: this.gid,\n        processUid: this.processUid,\n        processGid: this.processGid,\n        umask: this.processUmask,\n        preserve: this.preservePaths,\n        unlink: this.unlink,\n        cache: this.dirCache,\n        cwd: this.cwd,\n        mode: mode\n      })\n    } catch (er) {\n      return er\n    }\n  }\n\n  [LINK] (entry, linkpath, link) {\n    try {\n      fs[link + 'Sync'](linkpath, entry.absolute)\n      entry.resume()\n    } catch (er) {\n      return this[ONERROR](er, entry)\n    }\n  }\n}\n\nUnpack.Sync = UnpackSync\nmodule.exports = Unpack\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/unpack.js?");

/***/ }),

/***/ "./node_modules/tar/lib/update.js":
/*!****************************************!*\
  !*** ./node_modules/tar/lib/update.js ***!
  \****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\n// tar -u\n\nconst hlo = __webpack_require__(/*! ./high-level-opt.js */ \"./node_modules/tar/lib/high-level-opt.js\")\nconst r = __webpack_require__(/*! ./replace.js */ \"./node_modules/tar/lib/replace.js\")\n// just call tar.r with the filter and mtimeCache\n\nconst u = module.exports = (opt_, files, cb) => {\n  const opt = hlo(opt_)\n\n  if (!opt.file)\n    throw new TypeError('file is required')\n\n  if (opt.gzip)\n    throw new TypeError('cannot append to compressed archives')\n\n  if (!files || !Array.isArray(files) || !files.length)\n    throw new TypeError('no files or directories specified')\n\n  files = Array.from(files)\n\n  mtimeFilter(opt)\n  return r(opt, files, cb)\n}\n\nconst mtimeFilter = opt => {\n  const filter = opt.filter\n\n  if (!opt.mtimeCache)\n    opt.mtimeCache = new Map()\n\n  opt.filter = filter ? (path, stat) =>\n    filter(path, stat) && !(opt.mtimeCache.get(path) > stat.mtime)\n    : (path, stat) => !(opt.mtimeCache.get(path) > stat.mtime)\n}\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/update.js?");

/***/ }),

/***/ "./node_modules/tar/lib/warn-mixin.js":
/*!********************************************!*\
  !*** ./node_modules/tar/lib/warn-mixin.js ***!
  \********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nmodule.exports = Base => class extends Base {\n  warn (msg, data) {\n    if (!this.strict)\n      this.emit('warn', msg, data)\n    else if (data instanceof Error)\n      this.emit('error', data)\n    else {\n      const er = new Error(msg)\n      er.data = data\n      this.emit('error', er)\n    }\n  }\n}\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/warn-mixin.js?");

/***/ }),

/***/ "./node_modules/tar/lib/winchars.js":
/*!******************************************!*\
  !*** ./node_modules/tar/lib/winchars.js ***!
  \******************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\n\n// When writing files on Windows, translate the characters to their\n// 0xf000 higher-encoded versions.\n\nconst raw = [\n  '|',\n  '<',\n  '>',\n  '?',\n  ':'\n]\n\nconst win = raw.map(char =>\n  String.fromCharCode(0xf000 + char.charCodeAt(0)))\n\nconst toWin = new Map(raw.map((char, i) => [char, win[i]]))\nconst toRaw = new Map(win.map((char, i) => [char, raw[i]]))\n\nmodule.exports = {\n  encode: s => raw.reduce((s, c) => s.split(c).join(toWin.get(c)), s),\n  decode: s => win.reduce((s, c) => s.split(c).join(toRaw.get(c)), s)\n}\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/winchars.js?");

/***/ }),

/***/ "./node_modules/tar/lib/write-entry.js":
/*!*********************************************!*\
  !*** ./node_modules/tar/lib/write-entry.js ***!
  \*********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nconst Buffer = __webpack_require__(/*! ./buffer.js */ \"./node_modules/tar/lib/buffer.js\")\nconst MiniPass = __webpack_require__(/*! minipass */ \"./node_modules/minipass/index.js\")\nconst Pax = __webpack_require__(/*! ./pax.js */ \"./node_modules/tar/lib/pax.js\")\nconst Header = __webpack_require__(/*! ./header.js */ \"./node_modules/tar/lib/header.js\")\nconst ReadEntry = __webpack_require__(/*! ./read-entry.js */ \"./node_modules/tar/lib/read-entry.js\")\nconst fs = __webpack_require__(/*! fs */ \"fs\")\nconst path = __webpack_require__(/*! path */ \"path\")\n\nconst types = __webpack_require__(/*! ./types.js */ \"./node_modules/tar/lib/types.js\")\nconst maxReadSize = 16 * 1024 * 1024\nconst PROCESS = Symbol('process')\nconst FILE = Symbol('file')\nconst DIRECTORY = Symbol('directory')\nconst SYMLINK = Symbol('symlink')\nconst HARDLINK = Symbol('hardlink')\nconst HEADER = Symbol('header')\nconst READ = Symbol('read')\nconst LSTAT = Symbol('lstat')\nconst ONLSTAT = Symbol('onlstat')\nconst ONREAD = Symbol('onread')\nconst ONREADLINK = Symbol('onreadlink')\nconst OPENFILE = Symbol('openfile')\nconst ONOPENFILE = Symbol('onopenfile')\nconst CLOSE = Symbol('close')\nconst MODE = Symbol('mode')\nconst warner = __webpack_require__(/*! ./warn-mixin.js */ \"./node_modules/tar/lib/warn-mixin.js\")\nconst winchars = __webpack_require__(/*! ./winchars.js */ \"./node_modules/tar/lib/winchars.js\")\n\nconst modeFix = __webpack_require__(/*! ./mode-fix.js */ \"./node_modules/tar/lib/mode-fix.js\")\n\nconst WriteEntry = warner(class WriteEntry extends MiniPass {\n  constructor (p, opt) {\n    opt = opt || {}\n    super(opt)\n    if (typeof p !== 'string')\n      throw new TypeError('path is required')\n    this.path = p\n    // suppress atime, ctime, uid, gid, uname, gname\n    this.portable = !!opt.portable\n    // until node has builtin pwnam functions, this'll have to do\n    this.myuid = process.getuid && process.getuid()\n    this.myuser = process.env.USER || ''\n    this.maxReadSize = opt.maxReadSize || maxReadSize\n    this.linkCache = opt.linkCache || new Map()\n    this.statCache = opt.statCache || new Map()\n    this.preservePaths = !!opt.preservePaths\n    this.cwd = opt.cwd || process.cwd()\n    this.strict = !!opt.strict\n    this.noPax = !!opt.noPax\n    this.noMtime = !!opt.noMtime\n    this.mtime = opt.mtime || null\n\n    if (typeof opt.onwarn === 'function')\n      this.on('warn', opt.onwarn)\n\n    if (!this.preservePaths && path.win32.isAbsolute(p)) {\n      // absolutes on posix are also absolutes on win32\n      // so we only need to test this one to get both\n      const parsed = path.win32.parse(p)\n      this.warn('stripping ' + parsed.root + ' from absolute path', p)\n      this.path = p.substr(parsed.root.length)\n    }\n\n    this.win32 = !!opt.win32 || process.platform === 'win32'\n    if (this.win32) {\n      this.path = winchars.decode(this.path.replace(/\\\\/g, '/'))\n      p = p.replace(/\\\\/g, '/')\n    }\n\n    this.absolute = opt.absolute || path.resolve(this.cwd, p)\n\n    if (this.path === '')\n      this.path = './'\n\n    if (this.statCache.has(this.absolute))\n      this[ONLSTAT](this.statCache.get(this.absolute))\n    else\n      this[LSTAT]()\n  }\n\n  [LSTAT] () {\n    fs.lstat(this.absolute, (er, stat) => {\n      if (er)\n        return this.emit('error', er)\n      this[ONLSTAT](stat)\n    })\n  }\n\n  [ONLSTAT] (stat) {\n    this.statCache.set(this.absolute, stat)\n    this.stat = stat\n    if (!stat.isFile())\n      stat.size = 0\n    this.type = getType(stat)\n    this.emit('stat', stat)\n    this[PROCESS]()\n  }\n\n  [PROCESS] () {\n    switch (this.type) {\n      case 'File': return this[FILE]()\n      case 'Directory': return this[DIRECTORY]()\n      case 'SymbolicLink': return this[SYMLINK]()\n      // unsupported types are ignored.\n      default: return this.end()\n    }\n  }\n\n  [MODE] (mode) {\n    return modeFix(mode, this.type === 'Directory')\n  }\n\n  [HEADER] () {\n    if (this.type === 'Directory' && this.portable)\n      this.noMtime = true\n\n    this.header = new Header({\n      path: this.path,\n      linkpath: this.linkpath,\n      // only the permissions and setuid/setgid/sticky bitflags\n      // not the higher-order bits that specify file type\n      mode: this[MODE](this.stat.mode),\n      uid: this.portable ? null : this.stat.uid,\n      gid: this.portable ? null : this.stat.gid,\n      size: this.stat.size,\n      mtime: this.noMtime ? null : this.mtime || this.stat.mtime,\n      type: this.type,\n      uname: this.portable ? null :\n        this.stat.uid === this.myuid ? this.myuser : '',\n      atime: this.portable ? null : this.stat.atime,\n      ctime: this.portable ? null : this.stat.ctime\n    })\n\n    if (this.header.encode() && !this.noPax)\n      this.write(new Pax({\n        atime: this.portable ? null : this.header.atime,\n        ctime: this.portable ? null : this.header.ctime,\n        gid: this.portable ? null : this.header.gid,\n        mtime: this.noMtime ? null : this.mtime || this.header.mtime,\n        path: this.path,\n        linkpath: this.linkpath,\n        size: this.header.size,\n        uid: this.portable ? null : this.header.uid,\n        uname: this.portable ? null : this.header.uname,\n        dev: this.portable ? null : this.stat.dev,\n        ino: this.portable ? null : this.stat.ino,\n        nlink: this.portable ? null : this.stat.nlink\n      }).encode())\n    this.write(this.header.block)\n  }\n\n  [DIRECTORY] () {\n    if (this.path.substr(-1) !== '/')\n      this.path += '/'\n    this.stat.size = 0\n    this[HEADER]()\n    this.end()\n  }\n\n  [SYMLINK] () {\n    fs.readlink(this.absolute, (er, linkpath) => {\n      if (er)\n        return this.emit('error', er)\n      this[ONREADLINK](linkpath)\n    })\n  }\n\n  [ONREADLINK] (linkpath) {\n    this.linkpath = linkpath\n    this[HEADER]()\n    this.end()\n  }\n\n  [HARDLINK] (linkpath) {\n    this.type = 'Link'\n    this.linkpath = path.relative(this.cwd, linkpath)\n    this.stat.size = 0\n    this[HEADER]()\n    this.end()\n  }\n\n  [FILE] () {\n    if (this.stat.nlink > 1) {\n      const linkKey = this.stat.dev + ':' + this.stat.ino\n      if (this.linkCache.has(linkKey)) {\n        const linkpath = this.linkCache.get(linkKey)\n        if (linkpath.indexOf(this.cwd) === 0)\n          return this[HARDLINK](linkpath)\n      }\n      this.linkCache.set(linkKey, this.absolute)\n    }\n\n    this[HEADER]()\n    if (this.stat.size === 0)\n      return this.end()\n\n    this[OPENFILE]()\n  }\n\n  [OPENFILE] () {\n    fs.open(this.absolute, 'r', (er, fd) => {\n      if (er)\n        return this.emit('error', er)\n      this[ONOPENFILE](fd)\n    })\n  }\n\n  [ONOPENFILE] (fd) {\n    const blockLen = 512 * Math.ceil(this.stat.size / 512)\n    const bufLen = Math.min(blockLen, this.maxReadSize)\n    const buf = Buffer.allocUnsafe(bufLen)\n    this[READ](fd, buf, 0, buf.length, 0, this.stat.size, blockLen)\n  }\n\n  [READ] (fd, buf, offset, length, pos, remain, blockRemain) {\n    fs.read(fd, buf, offset, length, pos, (er, bytesRead) => {\n      if (er)\n        return this[CLOSE](fd, _ => this.emit('error', er))\n      this[ONREAD](fd, buf, offset, length, pos, remain, blockRemain, bytesRead)\n    })\n  }\n\n  [CLOSE] (fd, cb) {\n    fs.close(fd, cb)\n  }\n\n  [ONREAD] (fd, buf, offset, length, pos, remain, blockRemain, bytesRead) {\n    if (bytesRead <= 0 && remain > 0) {\n      const er = new Error('encountered unexpected EOF')\n      er.path = this.absolute\n      er.syscall = 'read'\n      er.code = 'EOF'\n      this[CLOSE](fd, _ => _)\n      return this.emit('error', er)\n    }\n\n    if (bytesRead > remain) {\n      const er = new Error('did not encounter expected EOF')\n      er.path = this.absolute\n      er.syscall = 'read'\n      er.code = 'EOF'\n      this[CLOSE](fd, _ => _)\n      return this.emit('error', er)\n    }\n\n    // null out the rest of the buffer, if we could fit the block padding\n    if (bytesRead === remain) {\n      for (let i = bytesRead; i < length && bytesRead < blockRemain; i++) {\n        buf[i + offset] = 0\n        bytesRead ++\n        remain ++\n      }\n    }\n\n    const writeBuf = offset === 0 && bytesRead === buf.length ?\n      buf : buf.slice(offset, offset + bytesRead)\n    remain -= bytesRead\n    blockRemain -= bytesRead\n    pos += bytesRead\n    offset += bytesRead\n\n    this.write(writeBuf)\n\n    if (!remain) {\n      if (blockRemain)\n        this.write(Buffer.alloc(blockRemain))\n      this.end()\n      this[CLOSE](fd, _ => _)\n      return\n    }\n\n    if (offset >= length) {\n      buf = Buffer.allocUnsafe(length)\n      offset = 0\n    }\n    length = buf.length - offset\n    this[READ](fd, buf, offset, length, pos, remain, blockRemain)\n  }\n})\n\nclass WriteEntrySync extends WriteEntry {\n  constructor (path, opt) {\n    super(path, opt)\n  }\n\n  [LSTAT] () {\n    this[ONLSTAT](fs.lstatSync(this.absolute))\n  }\n\n  [SYMLINK] () {\n    this[ONREADLINK](fs.readlinkSync(this.absolute))\n  }\n\n  [OPENFILE] () {\n    this[ONOPENFILE](fs.openSync(this.absolute, 'r'))\n  }\n\n  [READ] (fd, buf, offset, length, pos, remain, blockRemain) {\n    let threw = true\n    try {\n      const bytesRead = fs.readSync(fd, buf, offset, length, pos)\n      this[ONREAD](fd, buf, offset, length, pos, remain, blockRemain, bytesRead)\n      threw = false\n    } finally {\n      if (threw)\n        try { this[CLOSE](fd) } catch (er) {}\n    }\n  }\n\n  [CLOSE] (fd) {\n    fs.closeSync(fd)\n  }\n}\n\nconst WriteEntryTar = warner(class WriteEntryTar extends MiniPass {\n  constructor (readEntry, opt) {\n    opt = opt || {}\n    super(opt)\n    this.preservePaths = !!opt.preservePaths\n    this.portable = !!opt.portable\n    this.strict = !!opt.strict\n    this.noPax = !!opt.noPax\n    this.noMtime = !!opt.noMtime\n\n    this.readEntry = readEntry\n    this.type = readEntry.type\n    if (this.type === 'Directory' && this.portable)\n      this.noMtime = true\n\n    this.path = readEntry.path\n    this.mode = this[MODE](readEntry.mode)\n    this.uid = this.portable ? null : readEntry.uid\n    this.gid = this.portable ? null : readEntry.gid\n    this.uname = this.portable ? null : readEntry.uname\n    this.gname = this.portable ? null : readEntry.gname\n    this.size = readEntry.size\n    this.mtime = this.noMtime ? null : opt.mtime || readEntry.mtime\n    this.atime = this.portable ? null : readEntry.atime\n    this.ctime = this.portable ? null : readEntry.ctime\n    this.linkpath = readEntry.linkpath\n\n    if (typeof opt.onwarn === 'function')\n      this.on('warn', opt.onwarn)\n\n    if (path.isAbsolute(this.path) && !this.preservePaths) {\n      const parsed = path.parse(this.path)\n      this.warn(\n        'stripping ' + parsed.root + ' from absolute path',\n        this.path\n      )\n      this.path = this.path.substr(parsed.root.length)\n    }\n\n    this.remain = readEntry.size\n    this.blockRemain = readEntry.startBlockSize\n\n    this.header = new Header({\n      path: this.path,\n      linkpath: this.linkpath,\n      // only the permissions and setuid/setgid/sticky bitflags\n      // not the higher-order bits that specify file type\n      mode: this.mode,\n      uid: this.portable ? null : this.uid,\n      gid: this.portable ? null : this.gid,\n      size: this.size,\n      mtime: this.noMtime ? null : this.mtime,\n      type: this.type,\n      uname: this.portable ? null : this.uname,\n      atime: this.portable ? null : this.atime,\n      ctime: this.portable ? null : this.ctime\n    })\n\n    if (this.header.encode() && !this.noPax)\n      super.write(new Pax({\n        atime: this.portable ? null : this.atime,\n        ctime: this.portable ? null : this.ctime,\n        gid: this.portable ? null : this.gid,\n        mtime: this.noMtime ? null : this.mtime,\n        path: this.path,\n        linkpath: this.linkpath,\n        size: this.size,\n        uid: this.portable ? null : this.uid,\n        uname: this.portable ? null : this.uname,\n        dev: this.portable ? null : this.readEntry.dev,\n        ino: this.portable ? null : this.readEntry.ino,\n        nlink: this.portable ? null : this.readEntry.nlink\n      }).encode())\n\n    super.write(this.header.block)\n    readEntry.pipe(this)\n  }\n\n  [MODE] (mode) {\n    return modeFix(mode, this.type === 'Directory')\n  }\n\n  write (data) {\n    const writeLen = data.length\n    if (writeLen > this.blockRemain)\n      throw new Error('writing more to entry than is appropriate')\n    this.blockRemain -= writeLen\n    return super.write(data)\n  }\n\n  end () {\n    if (this.blockRemain)\n      this.write(Buffer.alloc(this.blockRemain))\n    return super.end()\n  }\n})\n\nWriteEntry.Sync = WriteEntrySync\nWriteEntry.Tar = WriteEntryTar\n\nconst getType = stat =>\n  stat.isFile() ? 'File'\n  : stat.isDirectory() ? 'Directory'\n  : stat.isSymbolicLink() ? 'SymbolicLink'\n  : 'Unsupported'\n\nmodule.exports = WriteEntry\n\n\n//# sourceURL=webpack:///./node_modules/tar/lib/write-entry.js?");

/***/ }),

/***/ "./node_modules/wide-align/align.js":
/*!******************************************!*\
  !*** ./node_modules/wide-align/align.js ***!
  \******************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar stringWidth = __webpack_require__(/*! string-width */ \"string-width\")\n\nexports.center = alignCenter\nexports.left = alignLeft\nexports.right = alignRight\n\n// lodash's way of generating pad characters.\n\nfunction createPadding (width) {\n  var result = ''\n  var string = ' '\n  var n = width\n  do {\n    if (n % 2) {\n      result += string;\n    }\n    n = Math.floor(n / 2);\n    string += string;\n  } while (n);\n\n  return result;\n}\n\nfunction alignLeft (str, width) {\n  var trimmed = str.trimRight()\n  if (trimmed.length === 0 && str.length >= width) return str\n  var padding = ''\n  var strWidth = stringWidth(trimmed)\n\n  if (strWidth < width) {\n    padding = createPadding(width - strWidth)\n  }\n\n  return trimmed + padding\n}\n\nfunction alignRight (str, width) {\n  var trimmed = str.trimLeft()\n  if (trimmed.length === 0 && str.length >= width) return str\n  var padding = ''\n  var strWidth = stringWidth(trimmed)\n\n  if (strWidth < width) {\n    padding = createPadding(width - strWidth)\n  }\n\n  return padding + trimmed\n}\n\nfunction alignCenter (str, width) {\n  var trimmed = str.trim()\n  if (trimmed.length === 0 && str.length >= width) return str\n  var padLeft = ''\n  var padRight = ''\n  var strWidth = stringWidth(trimmed)\n\n  if (strWidth < width) {\n    var padLeftBy = parseInt((width - strWidth) / 2, 10) \n    padLeft = createPadding(padLeftBy)\n    padRight = createPadding(width - (strWidth + padLeftBy))\n  }\n\n  return padLeft + trimmed + padRight\n}\n\n\n//# sourceURL=webpack:///./node_modules/wide-align/align.js?");

/***/ }),

/***/ "./src/controllers/AuthController.ts":
/*!*******************************************!*\
  !*** ./src/controllers/AuthController.ts ***!
  \*******************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar __importDefault = (this && this.__importDefault) || function (mod) {\n    return (mod && mod.__esModule) ? mod : { \"default\": mod };\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nconst Auth_1 = __importDefault(__webpack_require__(/*! ../utils/Auth */ \"./src/utils/Auth.ts\"));\nconst Database_1 = __webpack_require__(/*! ./Database */ \"./src/controllers/Database.ts\");\nclass AuthController {\n    async register(req, res) {\n        try {\n            const { name, email, password } = req.body;\n            const user = await Database_1.DB.Models.User.findOne({ email });\n            if (user)\n                return res.status(400).json({ message: `Email already exists` });\n            const hashing = await Auth_1.default.hashPassword(password, 12);\n            const newUser = await new Database_1.DB.Models.User({ name, email, password });\n            newUser.save();\n            return res.status(400).json({ message: \"Regiseter success\" });\n        }\n        catch (err) {\n            return res.status(500).json({ error: err.message });\n        }\n    }\n}\nexports.default = new AuthController();\n//# sourceMappingURL=data:application/json;base64,eyJ2ZXJzaW9uIjozLCJmaWxlIjoiQXV0aENvbnRyb2xsZXIuanMiLCJzb3VyY2VSb290IjoiIiwic291cmNlcyI6WyIuLi8uLi8uLi9zcmMvY29udHJvbGxlcnMvQXV0aENvbnRyb2xsZXIudHMiXSwibmFtZXMiOltdLCJtYXBwaW5ncyI6Ijs7Ozs7QUFDQSx5REFBaUM7QUFDakMseUNBQWdDO0FBRWhDLE1BQU0sY0FBYztJQUVmLEtBQUssQ0FBQyxRQUFRLENBQUUsR0FBVyxFQUFFLEdBQVk7UUFDdEMsSUFBSTtZQUNBLE1BQU0sRUFBQyxJQUFJLEVBQUUsS0FBSyxFQUFFLFFBQVEsRUFBQyxHQUFHLEdBQUcsQ0FBQyxJQUFJLENBQUM7WUFDekMsTUFBTSxJQUFJLEdBQUcsTUFBTSxhQUFFLENBQUMsTUFBTSxDQUFDLElBQUksQ0FBQyxPQUFPLENBQUMsRUFBQyxLQUFLLEVBQUMsQ0FBQyxDQUFDO1lBQ25ELElBQUcsSUFBSTtnQkFBRSxPQUFPLEdBQUcsQ0FBQyxNQUFNLENBQUMsR0FBRyxDQUFDLENBQUMsSUFBSSxDQUFDLEVBQUMsT0FBTyxFQUFHLHNCQUFzQixFQUFDLENBQUMsQ0FBQztZQUN6RSxNQUFNLE9BQU8sR0FBRyxNQUFNLGNBQUksQ0FBQyxZQUFZLENBQUMsUUFBUSxFQUFFLEVBQUUsQ0FBQyxDQUFDO1lBQ3RELE1BQU0sT0FBTyxHQUFJLE1BQU0sSUFBSSxhQUFFLENBQUMsTUFBTSxDQUFDLElBQUksQ0FBQyxFQUFDLElBQUksRUFBQyxLQUFLLEVBQUMsUUFBUSxFQUFDLENBQUMsQ0FBQztZQUNqRSxPQUFPLENBQUMsSUFBSSxFQUFFLENBQUM7WUFDZixPQUFPLEdBQUcsQ0FBQyxNQUFNLENBQUMsR0FBRyxDQUFDLENBQUMsSUFBSSxDQUFDLEVBQUUsT0FBTyxFQUFFLG1CQUFtQixFQUFFLENBQUMsQ0FBQztTQUNqRTtRQUFDLE9BQU8sR0FBRyxFQUFFO1lBQ1YsT0FBTyxHQUFHLENBQUMsTUFBTSxDQUFDLEdBQUcsQ0FBQyxDQUFDLElBQUksQ0FBQyxFQUFDLEtBQUssRUFBRyxHQUFHLENBQUMsT0FBTyxFQUFDLENBQUMsQ0FBQztTQUN0RDtJQUNMLENBQUM7Q0FHSjtBQUVELGtCQUFlLElBQUksY0FBYyxFQUFFLENBQUMifQ==\n\n//# sourceURL=webpack:///./src/controllers/AuthController.ts?");

/***/ }),

/***/ "./src/router/AuthRouter.ts":
/*!**********************************!*\
  !*** ./src/router/AuthRouter.ts ***!
  \**********************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar __importDefault = (this && this.__importDefault) || function (mod) {\n    return (mod && mod.__esModule) ? mod : { \"default\": mod };\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nconst BaseRouter_1 = __importDefault(__webpack_require__(/*! ./BaseRouter */ \"./src/router/BaseRouter.ts\"));\nconst AuthController_1 = __importDefault(__webpack_require__(/*! ./../controllers/AuthController */ \"./src/controllers/AuthController.ts\"));\nclass AuthRouter extends BaseRouter_1.default {\n    routes() {\n        this.router.post('/register', AuthController_1.default.register);\n    }\n}\nexports.default = new AuthRouter().router;\n//# sourceMappingURL=data:application/json;base64,eyJ2ZXJzaW9uIjozLCJmaWxlIjoiQXV0aFJvdXRlci5qcyIsInNvdXJjZVJvb3QiOiIiLCJzb3VyY2VzIjpbIi4uLy4uLy4uL3NyYy9yb3V0ZXIvQXV0aFJvdXRlci50cyJdLCJuYW1lcyI6W10sIm1hcHBpbmdzIjoiOzs7OztBQUFBLDhEQUFzQztBQUV0QyxxRkFBNkQ7QUFHN0QsTUFBTSxVQUFXLFNBQVEsb0JBQVU7SUFFeEIsTUFBTTtRQUNULElBQUksQ0FBQyxNQUFNLENBQUMsSUFBSSxDQUFFLFdBQVcsRUFBRSx3QkFBYyxDQUFDLFFBQVEsQ0FBQyxDQUFDO0lBQzVELENBQUM7Q0FFSjtBQUVELGtCQUFlLElBQUksVUFBVSxFQUFFLENBQUMsTUFBTSxDQUFDIn0=\n\n//# sourceURL=webpack:///./src/router/AuthRouter.ts?");

/***/ }),

/***/ "./src/utils/Auth.ts":
/*!***************************!*\
  !*** ./src/utils/Auth.ts ***!
  \***************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
eval("\nvar __importDefault = (this && this.__importDefault) || function (mod) {\n    return (mod && mod.__esModule) ? mod : { \"default\": mod };\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nconst bcrypt_1 = __importDefault(__webpack_require__(/*! bcrypt */ \"./node_modules/bcrypt/bcrypt.js\"));\nclass Auth {\n    static hashPassword(password, rounds) {\n        bcrypt_1.default.hash(password, rounds);\n    }\n    static comparePassword(password, dbHash) {\n        bcrypt_1.default.compare(password, dbHash);\n    }\n}\nexports.default = Auth;\n//# sourceMappingURL=data:application/json;base64,eyJ2ZXJzaW9uIjozLCJmaWxlIjoiQXV0aC5qcyIsInNvdXJjZVJvb3QiOiIiLCJzb3VyY2VzIjpbIi4uLy4uLy4uL3NyYy91dGlscy9BdXRoLnRzIl0sIm5hbWVzIjpbXSwibWFwcGluZ3MiOiI7Ozs7O0FBQUEsb0RBQTRCO0FBRTVCLE1BQXFCLElBQUk7SUFFZCxNQUFNLENBQUMsWUFBWSxDQUFDLFFBQWdCLEVBQUUsTUFBYTtRQUN0RCxnQkFBTSxDQUFDLElBQUksQ0FBQyxRQUFRLEVBQUUsTUFBTSxDQUFDLENBQUM7SUFDbEMsQ0FBQztJQUVNLE1BQU0sQ0FBQyxlQUFlLENBQUMsUUFBZ0IsRUFBQyxNQUFhO1FBQ3hELGdCQUFNLENBQUMsT0FBTyxDQUFDLFFBQVEsRUFBRSxNQUFNLENBQUMsQ0FBQztJQUNyQyxDQUFDO0NBQ0o7QUFURCx1QkFTQyJ9\n\n//# sourceURL=webpack:///./src/utils/Auth.ts?");

/***/ }),

/***/ "aproba":
/*!*************************!*\
  !*** external "aproba" ***!
  \*************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"aproba\");\n\n//# sourceURL=webpack:///external_%22aproba%22?");

/***/ }),

/***/ "assert":
/*!*************************!*\
  !*** external "assert" ***!
  \*************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"assert\");\n\n//# sourceURL=webpack:///external_%22assert%22?");

/***/ }),

/***/ "buffer":
/*!*************************!*\
  !*** external "buffer" ***!
  \*************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"buffer\");\n\n//# sourceURL=webpack:///external_%22buffer%22?");

/***/ }),

/***/ "child_process":
/*!********************************!*\
  !*** external "child_process" ***!
  \********************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"child_process\");\n\n//# sourceURL=webpack:///external_%22child_process%22?");

/***/ }),

/***/ "chownr":
/*!*************************!*\
  !*** external "chownr" ***!
  \*************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"chownr\");\n\n//# sourceURL=webpack:///external_%22chownr%22?");

/***/ }),

/***/ "crypto":
/*!*************************!*\
  !*** external "crypto" ***!
  \*************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"crypto\");\n\n//# sourceURL=webpack:///external_%22crypto%22?");

/***/ }),

/***/ "debug":
/*!************************!*\
  !*** external "debug" ***!
  \************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"debug\");\n\n//# sourceURL=webpack:///external_%22debug%22?");

/***/ }),

/***/ "events":
/*!*************************!*\
  !*** external "events" ***!
  \*************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"events\");\n\n//# sourceURL=webpack:///external_%22events%22?");

/***/ }),

/***/ "fs":
/*!*********************!*\
  !*** external "fs" ***!
  \*********************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"fs\");\n\n//# sourceURL=webpack:///external_%22fs%22?");

/***/ }),

/***/ "http":
/*!***********************!*\
  !*** external "http" ***!
  \***********************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"http\");\n\n//# sourceURL=webpack:///external_%22http%22?");

/***/ }),

/***/ "https":
/*!************************!*\
  !*** external "https" ***!
  \************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"https\");\n\n//# sourceURL=webpack:///external_%22https%22?");

/***/ }),

/***/ "iconv-lite":
/*!*****************************!*\
  !*** external "iconv-lite" ***!
  \*****************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"iconv-lite\");\n\n//# sourceURL=webpack:///external_%22iconv-lite%22?");

/***/ }),

/***/ "ini":
/*!**********************!*\
  !*** external "ini" ***!
  \**********************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"ini\");\n\n//# sourceURL=webpack:///external_%22ini%22?");

/***/ }),

/***/ "minimatch":
/*!****************************!*\
  !*** external "minimatch" ***!
  \****************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"minimatch\");\n\n//# sourceURL=webpack:///external_%22minimatch%22?");

/***/ }),

/***/ "minimist":
/*!***************************!*\
  !*** external "minimist" ***!
  \***************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"minimist\");\n\n//# sourceURL=webpack:///external_%22minimist%22?");

/***/ }),

/***/ "mkdirp":
/*!*************************!*\
  !*** external "mkdirp" ***!
  \*************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"mkdirp\");\n\n//# sourceURL=webpack:///external_%22mkdirp%22?");

/***/ }),

/***/ "object-assign":
/*!********************************!*\
  !*** external "object-assign" ***!
  \********************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"object-assign\");\n\n//# sourceURL=webpack:///external_%22object-assign%22?");

/***/ }),

/***/ "os":
/*!*********************!*\
  !*** external "os" ***!
  \*********************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"os\");\n\n//# sourceURL=webpack:///external_%22os%22?");

/***/ }),

/***/ "path":
/*!***********************!*\
  !*** external "path" ***!
  \***********************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"path\");\n\n//# sourceURL=webpack:///external_%22path%22?");

/***/ }),

/***/ "querystring":
/*!******************************!*\
  !*** external "querystring" ***!
  \******************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"querystring\");\n\n//# sourceURL=webpack:///external_%22querystring%22?");

/***/ }),

/***/ "readable-stream":
/*!**********************************!*\
  !*** external "readable-stream" ***!
  \**********************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"readable-stream\");\n\n//# sourceURL=webpack:///external_%22readable-stream%22?");

/***/ }),

/***/ "rimraf":
/*!*************************!*\
  !*** external "rimraf" ***!
  \*************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"rimraf\");\n\n//# sourceURL=webpack:///external_%22rimraf%22?");

/***/ }),

/***/ "safe-buffer":
/*!******************************!*\
  !*** external "safe-buffer" ***!
  \******************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"safe-buffer\");\n\n//# sourceURL=webpack:///external_%22safe-buffer%22?");

/***/ }),

/***/ "semver":
/*!*************************!*\
  !*** external "semver" ***!
  \*************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"semver\");\n\n//# sourceURL=webpack:///external_%22semver%22?");

/***/ }),

/***/ "set-blocking":
/*!*******************************!*\
  !*** external "set-blocking" ***!
  \*******************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"set-blocking\");\n\n//# sourceURL=webpack:///external_%22set-blocking%22?");

/***/ }),

/***/ "signal-exit":
/*!******************************!*\
  !*** external "signal-exit" ***!
  \******************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"signal-exit\");\n\n//# sourceURL=webpack:///external_%22signal-exit%22?");

/***/ }),

/***/ "stream":
/*!*************************!*\
  !*** external "stream" ***!
  \*************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"stream\");\n\n//# sourceURL=webpack:///external_%22stream%22?");

/***/ }),

/***/ "string-width":
/*!*******************************!*\
  !*** external "string-width" ***!
  \*******************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"string-width\");\n\n//# sourceURL=webpack:///external_%22string-width%22?");

/***/ }),

/***/ "string_decoder":
/*!*********************************!*\
  !*** external "string_decoder" ***!
  \*********************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"string_decoder\");\n\n//# sourceURL=webpack:///external_%22string_decoder%22?");

/***/ }),

/***/ "strip-ansi":
/*!*****************************!*\
  !*** external "strip-ansi" ***!
  \*****************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"strip-ansi\");\n\n//# sourceURL=webpack:///external_%22strip-ansi%22?");

/***/ }),

/***/ "url":
/*!**********************!*\
  !*** external "url" ***!
  \**********************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"url\");\n\n//# sourceURL=webpack:///external_%22url%22?");

/***/ }),

/***/ "util":
/*!***********************!*\
  !*** external "util" ***!
  \***********************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"util\");\n\n//# sourceURL=webpack:///external_%22util%22?");

/***/ }),

/***/ "yallist":
/*!**************************!*\
  !*** external "yallist" ***!
  \**************************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"yallist\");\n\n//# sourceURL=webpack:///external_%22yallist%22?");

/***/ }),

/***/ "zlib":
/*!***********************!*\
  !*** external "zlib" ***!
  \***********************/
/*! no static exports found */
/***/ (function(module, exports) {

eval("module.exports = require(\"zlib\");\n\n//# sourceURL=webpack:///external_%22zlib%22?");

/***/ })

};